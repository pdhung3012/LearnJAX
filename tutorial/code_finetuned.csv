prompt,response
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that computes Hinge Loss for binary classification tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, predictions, targets):
        return torch.mean(torch.clamp(1 - predictions * targets, min=0))

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    return [torch.rand(batch_size, *input_shape), torch.randint(0, 2, (batch_size,)).float() * 2 - 1]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that computes Hinge Loss for binary classification tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        pass

    def __call__(self, predictions, targets):
        return jnp.mean(jnp.clip(1 - predictions * targets, a_min=0))

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    predictions = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, *input_shape))
    targets = jax.random.randint(jax.random.PRNGKey(1), (batch_size,), 0, 2).astype(jnp.float32) * 2 - 1
    return [predictions, targets]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Performs 3D tensor-matrix multiplication.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A, B):
        """"""
        Performs 3D tensor-matrix multiplication.

        Args:
            A (torch.Tensor): Input 3D tensor of shape (N, M, K).
            B (torch.Tensor): Input matrix of shape (K, L).

        Returns:
            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.
        """"""
        return torch.matmul(A, B)

N = 16
M = 1024
K = 2048
L = 768

def get_inputs():
    A = torch.rand(N, M, K)
    B = torch.rand(K, L)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Performs 3D tensor-matrix multiplication.
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs 3D tensor-matrix multiplication.

        Args:
            A (jnp.ndarray): Input 3D tensor of shape (N, M, K).
            B (jnp.ndarray): Input matrix of shape (K, L).

        Returns:
            jnp.ndarray: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.
        """"""
        return jnp.matmul(A, B)

N = 16
M = 1024
K = 2048
L = 768

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (N, M, K))
    B = jax.random.uniform(key2, (K, L))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Performs 4D tensor-matrix multiplication: 
        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]

    Args:
        A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)
        B (torch.Tensor): Input matrix of shape (l, k)

    Returns:
        torch.Tensor: Output 4D tensor of shape (b, i, j, k)
    """"""
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, A, B):
        """"""
        Performs the 4D tensor-matrix multiplication.

        Args:
            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)
            B (torch.Tensor): Input matrix of shape (l, k)

        Returns:
            torch.Tensor: Output 4D tensor of shape (b, i, j, k)
        """"""
        return torch.einsum(""bijl,lk->bijk"", A, B)

# Test code
b = 8
i = 256
j = 512
l = 256
k = 768

def get_inputs():
    A = torch.rand(b, i, j, l)
    B = torch.rand(l, k)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Performs 4D tensor-matrix multiplication: 
        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]

    Args:
        A (jnp.ndarray): Input 4D tensor of shape (b, i, j, l)
        B (jnp.ndarray): Input matrix of shape (l, k)

    Returns:
        jnp.ndarray: Output 4D tensor of shape (b, i, j, k)
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs the 4D tensor-matrix multiplication.

        Args:
            A (jnp.ndarray): Input 4D tensor of shape (b, i, j, l)
            B (jnp.ndarray): Input matrix of shape (l, k)

        Returns:
            jnp.ndarray: Output 4D tensor of shape (b, i, j, k)
        """"""
        return jnp.einsum(""bijl,lk->bijk"", A, B)

# Test code
b = 8
i = 256
j = 512
l = 256
k = 768

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (b, i, j, l))
    B = jax.random.uniform(key2, (l, k))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.
    C = diag(A) * B
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A, B):
        """"""
        Performs the matrix multiplication.

        Args:
            A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).
            B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).

        Returns:
            torch.Tensor: The result of the matrix multiplication. Shape: (N, M).
        """"""
        return torch.diag(A) @ B

M = 4096
N = 4096

def get_inputs():
    A = torch.rand(N)
    B = torch.rand(N, M)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.
    C = diag(A) * B
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs the matrix multiplication.

        Args:
            A (jnp.ndarray): A 1D array representing the diagonal of the diagonal matrix. Shape: (N,).
            B (jnp.ndarray): A 2D array representing the second matrix. Shape: (N, M).

        Returns:
            jnp.ndarray: The result of the matrix multiplication. Shape: (N, M).
        """"""
        # torch.diag(A) @ B is equivalent to (A[:, None] * B)
        return A[:, None] * B

M = 4096
N = 4096

def get_inputs():
    A = jax.random.uniform(jax.random.PRNGKey(0), (N,))
    B = jax.random.uniform(jax.random.PRNGKey(1), (N, M))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A, B):
        """"""
        Performs matrix multiplication of two symmetric matrices.

        Args:
            A (torch.Tensor): Input matrix A, shape (N, N), symmetric.
            B (torch.Tensor): Input matrix B, shape (N, N), symmetric.

        Returns:
            torch.Tensor: Output matrix C, shape (N, N).
        """"""
        return torch.matmul(A, B)

N = 4096

def get_inputs():
    """"""
    Generates a pair of random symmetric matrices for testing.

    Returns:
        list: List containing two symmetric tensors A and B.
    """"""
    A = torch.rand(N, N)
    A = (A + A.T) / 2  # Ensure symmetry
    B = torch.rand(N, N)
    B = (B + B.T) / 2  # Ensure symmetry
    return [A, B]

def get_init_inputs():
    """"""
    No specific initialization inputs needed for this model.

    Returns:
        list: Empty list.
    """"""
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs matrix multiplication of two symmetric matrices.

        Args:
            A (jnp.ndarray): Input matrix A, shape (N, N), symmetric.
            B (jnp.ndarray): Input matrix B, shape (N, N), symmetric.

        Returns:
            jnp.ndarray: Output matrix C, shape (N, N).
        """"""
        return jnp.matmul(A, B)

N = 4096

def get_inputs():
    """"""
    Generates a pair of random symmetric matrices for testing.

    Returns:
        list: List containing two symmetric arrays A and B.
    """"""
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (N, N))
    A = (A + A.T) / 2  # Ensure symmetry
    B = jax.random.uniform(key2, (N, N))
    B = (B + B.T) / 2  # Ensure symmetry
    return [A, B]

def get_init_inputs():
    """"""
    No specific initialization inputs needed for this model.

    Returns:
        list: Empty list.
    """"""
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A, B):
        """"""
        Performs matrix multiplication for upper triangular matrices.

        Args:
            A (torch.Tensor): Upper triangular matrix of shape (N, N).
            B (torch.Tensor): Upper triangular matrix of shape (N, N).

        Returns:
            torch.Tensor: The product of A and B, also an upper triangular matrix of shape (N, N).
        """"""
        return torch.triu(torch.matmul(A, B))

N = 4096

def get_inputs():
    """"""
    Generates upper triangular matrices for testing.

    Returns:
        list: A list containing two upper triangular matrices of shape (N, N).
    """"""
    A = torch.triu(torch.rand(N, N))
    B = torch.triu(torch.rand(N, N))
    return [A, B]

def get_init_inputs():
    """"""
    No specific initialization inputs are needed for this model.

    Returns:
        list: An empty list.
    """"""
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs matrix multiplication for upper triangular matrices.

        Args:
            A (jnp.ndarray): Upper triangular matrix of shape (N, N).
            B (jnp.ndarray): Upper triangular matrix of shape (N, N).

        Returns:
            jnp.ndarray: The product of A and B, also an upper triangular matrix of shape (N, N).
        """"""
        return jnp.triu(jnp.matmul(A, B))

N = 4096

def get_inputs():
    """"""
    Generates upper triangular matrices for testing.

    Returns:
        list: A list containing two upper triangular matrices of shape (N, N).
    """"""
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jnp.triu(jax.random.uniform(key1, (N, N)))
    B = jnp.triu(jax.random.uniform(key2, (N, N)))
    return [A, B]

def get_init_inputs():
    """"""
    No specific initialization inputs are needed for this model.

    Returns:
        list: An empty list.
    """"""
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. 
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A, B):
        """"""
        Performs matrix multiplication of lower triangular matrices A and B.

        Args:
            A (torch.Tensor): Lower triangular matrix of shape (N, N).
            B (torch.Tensor): Lower triangular matrix of shape (N, N).

        Returns:
            torch.Tensor: The result of matrix multiplication C of shape (N, N).
        """"""
        return torch.tril(torch.matmul(A, B))

M = 4096

def get_inputs():
    A = torch.rand(M, M)
    B = torch.rand(M, M)
    A = torch.tril(A)
    B = torch.tril(B)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. 
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs matrix multiplication of lower triangular matrices A and B.

        Args:
            A (jnp.ndarray): Lower triangular matrix of shape (N, N).
            B (jnp.ndarray): Lower triangular matrix of shape (N, N).

        Returns:
            jnp.ndarray: The result of matrix multiplication C of shape (N, N).
        """"""
        return jnp.tril(jnp.matmul(A, B))

M = 4096

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (M, M))
    B = jax.random.uniform(key2, (M, M))
    A = jnp.tril(A)
    B = jnp.tril(B)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return torch.matmul(A.T, B)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = torch.rand(K, M)
    B = torch.rand(K, N)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input array of shape (M, K).
            B: Input array of shape (K, N).

        Returns:
            Output array of shape (M, N).
        """"""
        return jnp.matmul(A.T, B)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (K, M))
    B = jax.random.uniform(key2, (K, N))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return torch.matmul(A, B.T)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(N, K)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (N, K).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return jnp.matmul(A, B.T)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = jax.random.uniform(jax.random.PRNGKey(0), (M, K))
    B = jax.random.uniform(jax.random.PRNGKey(1), (N, K))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return torch.matmul(A.T, B.T)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = torch.rand(K, M)
    B = torch.rand(N, K)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input array of shape (M, K).
            B: Input array of shape (K, N).

        Returns:
            Output array of shape (M, N).
        """"""
        return jnp.matmul(A.T, B.T)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = jax.random.uniform(jax.random.PRNGKey(0), (K, M))
    B = jax.random.uniform(jax.random.PRNGKey(1), (N, K))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single square matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs the matrix multiplication.

        Args:
            A (torch.Tensor): Input matrix A of shape (N, N).
            B (torch.Tensor): Input matrix B of shape (N, N).

        Returns:
            torch.Tensor: Output matrix C of shape (N, N).
        """"""
        return torch.matmul(A, B)

N = 2048 * 2

def get_inputs():
    A = torch.rand(N, N)
    B = torch.rand(N, N)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single square matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs the matrix multiplication.

        Args:
            A (jnp.ndarray): Input matrix A of shape (N, N).
            B (jnp.ndarray): Input matrix B of shape (N, N).

        Returns:
            jnp.ndarray: Output matrix C of shape (N, N).
        """"""
        return jnp.matmul(A, B)

N = 2048 * 2

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (N, N))
    B = jax.random.uniform(key2, (N, N))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a Sigmoid activation.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Sigmoid activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with Sigmoid applied, same shape as input.
        """"""
        return torch.sigmoid(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a Sigmoid activation.
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        """"""
        Applies Sigmoid activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of any shape.

        Returns:
            jnp.ndarray: Output tensor with Sigmoid applied, same shape as input.
        """"""
        return jax.nn.sigmoid(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a Softmax activation.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Softmax activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features).

        Returns:
            torch.Tensor: Output tensor with Softmax applied, same shape as input.
        """"""
        return torch.softmax(x, dim=1)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a Softmax activation.
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        """"""
        Applies Softmax activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, num_features).

        Returns:
            jnp.ndarray: Output tensor with Softmax applied, same shape as input.
        """"""
        return jax.nn.softmax(x, axis=1)

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a LogSoftmax activation.
    """"""
    def __init__(self, dim: int = 1):
        super(Model, self).__init__()
        self.dim = dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies LogSoftmax activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, dim).

        Returns:
            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.
        """"""
        return torch.log_softmax(x, dim=self.dim)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a LogSoftmax activation.
    """"""
    def __init__(self, dim: int = 1):
        self.dim = dim

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies LogSoftmax activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, dim).

        Returns:
            jnp.ndarray: Output tensor with LogSoftmax applied, same shape as input.
        """"""
        return jax.nn.log_softmax(x, axis=self.dim)

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a GELU activation.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies GELU activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with GELU applied, same shape as input.
        """"""
        return torch.nn.functional.gelu(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a GELU activation.
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        """"""
        Applies GELU activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of any shape.

        Returns:
            jnp.ndarray: Output tensor with GELU applied, same shape as input.
        """"""
        return jax.nn.gelu(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a SELU activation.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies SELU activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with SELU applied, same shape as input.
        """"""
        return torch.selu(x)
    
batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a SELU activation.
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        """"""
        Applies SELU activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of any shape.

        Returns:
            jnp.ndarray: Output tensor with SELU applied, same shape as input.
        """"""
        # SELU constants from the original paper
        alpha = 1.6732632423543772
        scale = 1.0507009873554805
        return scale * jnp.where(x > 0, x, alpha * (jnp.exp(x) - 1))

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return torch.matmul(A, B)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(K, N)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B)
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return jnp.matmul(A, B)

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = jax.random.uniform(jax.random.PRNGKey(0), (M, K))
    B = jax.random.uniform(jax.random.PRNGKey(1), (K, N))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a Softsign activation.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Softsign activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with Softsign applied, same shape as input.
        """"""
        return x / (1 + torch.abs(x))

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a Softsign activation.
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        """"""
        Applies Softsign activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of any shape.

        Returns:
            jnp.ndarray: Output tensor with Softsign applied, same shape as input.
        """"""
        return x / (1 + jnp.abs(x))

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    """"""
    Simple model that performs a HardTanh activation.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies HardTanh activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with HardTanh applied, same shape as input.
        """"""
        return F.hardtanh(x, min_val=-1., max_val=1.)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a HardTanh activation.
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        """"""
        Applies HardTanh activation to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of any shape.

        Returns:
            jnp.ndarray: Output tensor with HardTanh applied, same shape as input.
        """"""
        return jnp.clip(x, a_min=-1., a_max=1.)

batch_size = 4096
dim = 393216

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs RMS Normalization.
    """"""
    def __init__(self, num_features: int, eps: float = 1e-5):
        """"""
        Initializes the RMSNorm layer.

        Args:
            num_features (int): Number of features in the input tensor.
            eps (float, optional): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.
        """"""
        super(Model, self).__init__()
        self.num_features = num_features
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies RMS Normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).

        Returns:
            torch.Tensor: Output tensor with RMS Normalization applied, same shape as input.
        """"""
        # Calculate the RMS along the feature dimension
        rms = torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)

        # Normalize the input by dividing by the RMS
        return x / rms

batch_size = 112
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return [features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs RMS Normalization.
    """"""
    num_features: int
    eps: float = 1e-5

    @nn.compact
    def __call__(self, x):
        """"""
        Applies RMS Normalization to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, num_features, *).

        Returns:
            jnp.ndarray: Output tensor with RMS Normalization applied, same shape as input.
        """"""
        # Calculate the RMS along the feature dimension
        rms = jnp.sqrt(jnp.mean(x ** 2, axis=1, keepdims=True) + self.eps)
        # Normalize the input by dividing by the RMS
        return x / rms

batch_size = 112
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, features, dim1, dim2))
    return [x]

def get_init_inputs():
    return [features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs Frobenius norm normalization.
    """"""
    def __init__(self):
        """"""
        Initializes the Frobenius norm normalization layer.
        """"""
        super(Model, self).__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Frobenius norm normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of arbitrary shape.

        Returns:
            torch.Tensor: Output tensor with Frobenius norm normalization applied, same shape as input.
        """"""
        norm = torch.norm(x, p='fro')
        return x / norm

batch_size = 112
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs Frobenius norm normalization.
    """"""
    def __init__(self):
        """"""
        Initializes the Frobenius norm normalization layer.
        """"""
        pass

    def __call__(self, x):
        """"""
        Applies Frobenius norm normalization to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of arbitrary shape.

        Returns:
            jnp.ndarray: Output tensor with Frobenius norm normalization applied, same shape as input.
        """"""
        norm = jnp.linalg.norm(x, ord='fro')
        return x / norm

batch_size = 112
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, features, dim1, dim2))
    return [x]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs L1 normalization.
    """"""
    def __init__(self):
        """"""
        Initializes the L1 normalization layer.
        """"""
        super(Model, self).__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies L1 normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (..., dim, ...).

        Returns:
            torch.Tensor: Output tensor with L1 normalization applied, same shape as input.
        """"""
        return x / torch.mean(torch.abs(x), dim=1, keepdim=True)

batch_size = 32768
# choose dim so total <2^31
dim = 65535

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs L1 normalization.
    """"""
    def __init__(self):
        """"""
        Initializes the L1 normalization layer.
        """"""
        pass

    def __call__(self, x):
        """"""
        Applies L1 normalization to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (..., dim, ...).

        Returns:
            jnp.ndarray: Output tensor with L1 normalization applied, same shape as input.
        """"""
        return x / jnp.mean(jnp.abs(x), axis=1, keepdims=True)

batch_size = 32768
# choose dim so total <2^31
dim = 65535

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs L2 normalization.
    """"""
    def __init__(self):
        """"""
        Initializes the L2Norm layer.

        Args:
            dim (int): Dimension along which to normalize.
        """"""
        super(Model, self).__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies L2 normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (*, dim, *).

        Returns:
            torch.Tensor: Output tensor with L2 normalization applied, same shape as input.
        """"""
        return x / torch.norm(x, p=2, dim=1, keepdim=True)

batch_size = 32768
# choose dim so total <2^31
dim = 65535

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs L2 normalization.
    """"""
    def __init__(self):
        """"""
        Initializes the L2Norm layer.
        """"""
        pass

    def __call__(self, x):
        """"""
        Applies L2 normalization to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (*, dim, *).

        Returns:
            jnp.ndarray: Output tensor with L2 normalization applied, same shape as input.
        """"""
        norm = jnp.linalg.norm(x, ord=2, axis=1, keepdims=True)
        return x / norm

batch_size = 32768
# choose dim so total <2^31
dim = 65535

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim))
    return [x]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs batched matrix multiplication.

        Args:
            A: Input tensor of shape (batch_size, m, k).
            B: Input tensor of shape (batch_size, k, n).

        Returns:
            C: Output tensor of shape (batch_size, m, n).
        """"""
        return torch.bmm(A, B)

batch_size = 128
m = 128 * 4
k = 256 * 4
n = 512 * 4

def get_inputs():
    A = torch.rand(batch_size, m, k)
    B = torch.rand(batch_size, k, n)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs batched matrix multiplication.

        Args:
            A: Input tensor of shape (batch_size, m, k).
            B: Input tensor of shape (batch_size, k, n).

        Returns:
            C: Output tensor of shape (batch_size, m, n).
        """"""
        return jnp.matmul(A, B)

batch_size = 128
m = 128 * 4
k = 256 * 4
n = 512 * 4

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (batch_size, m, k))
    B = jax.random.uniform(key2, (batch_size, k, n))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs Max Pooling 1D.
    """"""
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):
        """"""
        Initializes the Max Pooling 1D layer.

        Args:
            kernel_size (int): Size of the window to take a max over.
            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).
            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.
            dilation (int, optional): Spacing between kernel elements. Defaults to 1.
            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.
        """"""
        super(Model, self).__init__()
        self.maxpool = nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Max Pooling 1D to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features, sequence_length).

        Returns:
            torch.Tensor: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).
        """"""
        return self.maxpool(x)

batch_size = 64
features = 192
sequence_length = 65536

kernel_size = 8
stride      = 1
padding     = 4
dilation    = 3            

return_indices = False

def get_inputs():
    x = torch.rand(batch_size, features, sequence_length)
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation, return_indices]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs Max Pooling 1D.
    """"""
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False):
        """"""
        Initializes the Max Pooling 1D layer.

        Args:
            kernel_size (int): Size of the window to take a max over.
            stride (int, optional): Stride of the window. Defaults to None (same as kernel_size).
            padding (int, optional): Implicit zero padding to be added on both sides. Defaults to 0.
            dilation (int, optional): Spacing between kernel elements. Defaults to 1.
            return_indices (bool, optional): Whether to return the indices of the maximum values. Defaults to False.
        """"""
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.dilation = dilation
        self.return_indices = return_indices

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies Max Pooling 1D to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, num_features, sequence_length).

        Returns:
            jnp.ndarray: Output tensor with Max Pooling 1D applied, shape (batch_size, num_features, output_sequence_length).
        """"""
        return maxpool1d(
            x,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            dilation=self.dilation,
            return_indices=self.return_indices
        )

def maxpool1d(x, kernel_size, stride, padding, dilation, return_indices=False):
    """"""
    MaxPool1d for JAX, emulating PyTorch's behavior.
    Args:
        x: (batch, channels, length)
        kernel_size: int
        stride: int
        padding: int
        dilation: int
        return_indices: bool
    Returns:
        (batch, channels, out_length)
    """"""
    # x: (N, C, L)
    N, C, L = x.shape
    effective_kernel = dilation * (kernel_size - 1) + 1

    # Pad both sides
    if padding > 0:
        x = jnp.pad(x, ((0,0), (0,0), (padding, padding)), mode='constant', constant_values=-jnp.inf)

    L_padded = x.shape[2]
    out_length = (L_padded - effective_kernel) // stride + 1

    # Get all windowed indices
    idxs = jnp.arange(out_length) * stride
    window_idxs = jnp.arange(kernel_size) * dilation
    idxs = idxs[:, None] + window_idxs[None, :]  # (out_length, kernel_size)
    # idxs: (out_length, kernel_size)

    # Gather windows
    def gather_windows(x1d):
        # x1d: (L_padded,)
        return x1d[idxs]  # (out_length, kernel_size)

    # vmap over batch and channel
    x_windows = jax.vmap(
        lambda x1: jax.vmap(gather_windows)(x1)
    )(x)  # (N, C, out_length, kernel_size)

    # Compute max and optionally indices
    max_vals = jnp.max(x_windows, axis=-1)  # (N, C, out_length)
    if return_indices:
        max_idxs = jnp.argmax(x_windows, axis=-1)  # (N, C, out_length)
        return max_vals, max_idxs
    else:
        return max_vals

batch_size = 64
features = 192
sequence_length = 65536

kernel_size = 8
stride      = 1
padding     = 4
dilation    = 3            

return_indices = False

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, features, sequence_length))
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation, return_indices]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs Max Pooling 2D.
    """"""
    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):
        """"""
        Initializes the Max Pooling 2D layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int): Stride of the pooling window.
            padding (int): Padding to be applied before pooling.
            dilation (int): Spacing between kernel elements.
        """"""
        super(Model, self).__init__()
        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Max Pooling 2D to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).

        Returns:
            torch.Tensor: Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).
        """"""
        return self.maxpool(x)

batch_size = 32
channels = 64
height = 512
width = 512
kernel_size = 4
stride = 1
padding = 1
dilation = 1

def get_inputs():
    x = torch.rand(batch_size, channels, height, width)
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

def max_pool2d(x, kernel_size, stride, padding, dilation):
    """"""
    Applies 2D max pooling to the input tensor.

    Args:
        x: Input tensor of shape (batch_size, channels, height, width).
        kernel_size: Size of the pooling window (int).
        stride: Stride of the pooling window (int).
        padding: Padding to be applied before pooling (int).
        dilation: Spacing between kernel elements (int).

    Returns:
        Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).
    """"""
    # JAX expects NHWC, so transpose from NCHW to NHWC
    x_nhwc = jnp.transpose(x, (0, 2, 3, 1))
    window_shape = (kernel_size, kernel_size)
    window_dilation = (dilation, dilation)
    strides = (stride, stride)
    # Padding: convert int to ((before, after), (before, after))
    if isinstance(padding, int):
        pad_hw = ((padding, padding), (padding, padding))
    else:
        pad_hw = padding
    # jax.lax.reduce_window expects padding as a list of (low, high) for each spatial dim
    padding_config = [(pad_hw[0][0], pad_hw[0][1]), (pad_hw[1][0], pad_hw[1][1])]
    # For batch and channel, no padding
    full_padding = [(0, 0), *padding_config, (0, 0)]
    out = jax.lax.reduce_window(
        x_nhwc,
        -jnp.inf,
        jax.lax.max,
        window_shape=(1, *window_shape, 1),
        strides=(1, *strides, 1),
        padding=full_padding,
        window_dilation=(1, *window_dilation, 1),
    )
    # Transpose back to NCHW
    out_nchw = jnp.transpose(out, (0, 3, 1, 2))
    return out_nchw

class Model:
    """"""
    Simple model that performs Max Pooling 2D.
    """"""
    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):
        """"""
        Initializes the Max Pooling 2D layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int): Stride of the pooling window.
            padding (int): Padding to be applied before pooling.
            dilation (int): Spacing between kernel elements.
        """"""
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation

    def __call__(self, x):
        """"""
        Applies Max Pooling 2D to the input tensor.

        Args:
            x: Input tensor of shape (batch_size, channels, height, width).

        Returns:
            Output tensor after Max Pooling 2D, shape (batch_size, channels, pooled_height, pooled_width).
        """"""
        return max_pool2d(
            x,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            dilation=self.dilation,
        )

batch_size = 32
channels = 64
height = 512
width = 512
kernel_size = 4
stride = 1
padding = 1
dilation = 1

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, channels, height, width))
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs Max Pooling 3D.
    """"""
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):
        """"""
        Initializes the Max Pooling 3D layer.

        Args:
            kernel_size (int): Size of the kernel for the max pooling operation.
            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.
            padding (int, optional): Padding applied to the input tensor. Defaults to 0.
            dilation (int, optional): Spacing between kernel elements. Defaults to 1.
            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.
            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.
        """"""
        super(Model, self).__init__()
        self.maxpool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Max Pooling 3D to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).

        Returns:
            torch.Tensor: Output tensor with Max Pooling 3D applied.
        """"""
        return self.maxpool(x)

batch_size = 16
channels = 32
dim1 = 128
dim2 = 128
dim3 = 128
kernel_size = 3
stride = 2
padding = 1
dilation = 3

def get_inputs():
    x = torch.rand(batch_size, channels, dim1, dim2, dim3)
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs Max Pooling 3D.
    """"""
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0, dilation: int = 1, return_indices: bool = False, ceil_mode: bool = False):
        """"""
        Initializes the Max Pooling 3D layer.

        Args:
            kernel_size (int): Size of the kernel for the max pooling operation.
            stride (int, optional): Stride of the pooling operation. Defaults to None, which means stride is equal to kernel_size.
            padding (int, optional): Padding applied to the input tensor. Defaults to 0.
            dilation (int, optional): Spacing between kernel elements. Defaults to 1.
            return_indices (bool, optional): Whether to return indices of the maximum values. Defaults to False.
            ceil_mode (bool, optional): When True, the output size is ceil(input_size / stride) instead of floor. Defaults to False.
        """"""
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.dilation = dilation
        self.return_indices = return_indices
        self.ceil_mode = ceil_mode

    def forward(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies Max Pooling 3D to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, channels, dim1, dim2, dim3).

        Returns:
            jnp.ndarray: Output tensor with Max Pooling 3D applied.
        """"""
        # x: (N, C, D, H, W)
        N, C, D, H, W = x.shape
        k = self.kernel_size
        s = self.stride
        p = self.padding
        d = self.dilation

        # Calculate effective kernel size with dilation
        eff_k = d * (k - 1) + 1

        # Pad input
        pad = [(0,0), (0,0)]
        for dim in [D, H, W]:
            pad.append((p, p))
        x_padded = jnp.pad(x, pad, mode='constant', constant_values=-jnp.inf)

        # Compute output shape
        def get_out_size(in_size):
            if self.ceil_mode:
                return int(jnp.ceil((in_size + 2 * p - eff_k) / s)) + 1
            else:
                return int(jnp.floor((in_size + 2 * p - eff_k) / s)) + 1

        out_D = get_out_size(D)
        out_H = get_out_size(H)
        out_W = get_out_size(W)

        # Stride tricks for 3D pooling with dilation
        # We want to extract sliding windows of shape (k, k, k) with stride s and dilation d
        # We'll use jax.lax.reduce_window

        window_shape = (1, 1, eff_k, eff_k, eff_k)
        strides = (1, 1, s, s, s)
        # jax.lax.reduce_window does not support dilation directly, so we need to use window_dilation
        window_dimensions = (1, 1, k, k, k)
        window_strides = (1, 1, s, s, s)
        base_dilation = (1, 1, 1, 1, 1)
        window_dilation = (1, 1, d, d, d)

        out = jax.lax.reduce_window(
            x_padded,
            -jnp.inf,
            jax.lax.max,
            window_dimensions,
            window_strides,
            'VALID',
            base_dilation=base_dilation,
            window_dilation=window_dilation
        )

        # Crop to output shape in case of over-padding due to ceil_mode
        out = out[:, :, :out_D, :out_H, :out_W]

        if self.return_indices:
            # Not supported in JAX, so just return the output
            return out, None
        return out

batch_size = 16
channels = 32
dim1 = 128
dim2 = 128
dim3 = 128
kernel_size = 3
stride = 2
padding = 1
dilation = 3

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, channels, dim1, dim2, dim3))
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding, dilation]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs 1D Average Pooling.
    """"""
    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):
        """"""
        Initializes the 1D Average Pooling layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int, optional): Stride of the pooling operation. Defaults to 1.
            padding (int, optional): Padding applied to the input tensor. Defaults to 0.
        """"""
        super(Model, self).__init__()
        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=padding)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies 1D Average Pooling to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, input_length).

        Returns:
            torch.Tensor: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).
        """"""
        return self.avg_pool(x)

batch_size = 64
in_channels = 128
input_length = 65536
kernel_size = 8
stride = 1
padding = 4

def get_inputs():
    x = torch.rand(batch_size, in_channels, input_length)
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs 1D Average Pooling.
    """"""
    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):
        """"""
        Initializes the 1D Average Pooling layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int, optional): Stride of the pooling operation. Defaults to 1.
            padding (int, optional): Padding applied to the input tensor. Defaults to 0.
        """"""
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies 1D Average Pooling to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, in_channels, input_length).

        Returns:
            jnp.ndarray: Output tensor with 1D Average Pooling applied, shape (batch_size, in_channels, output_length).
        """"""
        # x: (batch, channels, length)
        batch_size, in_channels, input_length = x.shape

        # Pad along the last dimension (length)
        if self.padding > 0:
            pad_width = [(0, 0), (0, 0), (self.padding, self.padding)]
            x = jnp.pad(x, pad_width, mode='constant', constant_values=0)

        # Calculate output length
        L_in = x.shape[2]
        kernel_size = self.kernel_size
        stride = self.stride
        output_length = (L_in - kernel_size) // stride + 1

        # Use jax.lax.reduce_window to perform average pooling
        window_shape = (1, 1, kernel_size)
        window_strides = (1, 1, stride)
        pooled = jax.lax.reduce_window(
            x,
            0.0,
            jax.lax.add,
            window_shape,
            window_strides,
            padding='VALID'
        )
        pooled = pooled / kernel_size
        return pooled

batch_size = 64
in_channels = 128
input_length = 65536
kernel_size = 8
stride = 1
padding = 4

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, in_channels, input_length))
    return [x]

def get_init_inputs():
    return [kernel_size, stride, padding]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs 2D Average Pooling.
    """"""
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):
        """"""
        Initializes the Average Pooling layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).
            padding (int, optional): Padding applied to the input tensor. Defaults to 0.
        """"""
        super(Model, self).__init__()
        self.avg_pool = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies 2D Average Pooling to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).

        Returns:
            torch.Tensor: Output tensor with Average Pooling applied.
        """"""
        return self.avg_pool(x)

batch_size = 16
channels = 64
height = 2048
width = 2048
kernel_size = 11

def get_inputs():
    x = torch.rand(batch_size, channels, height, width)
    return [x]

def get_init_inputs():
    return [kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs 2D Average Pooling.
    """"""
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):
        """"""
        Initializes the Average Pooling layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int, optional): Stride of the pooling operation. Defaults to None (same as kernel_size).
            padding (int, optional): Padding applied to the input tensor. Defaults to 0.
        """"""
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies 2D Average Pooling to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, channels, height, width).

        Returns:
            jnp.ndarray: Output tensor with Average Pooling applied.
        """"""
        # x: (N, C, H, W)
        N, C, H, W = x.shape
        kH, kW = self.kernel_size, self.kernel_size
        sH, sW = self.stride, self.stride
        pH, pW = self.padding, self.padding

        # Pad input if needed
        if pH > 0 or pW > 0:
            # Pad only height and width dimensions
            pad_width = ((0,0), (0,0), (pH,pH), (pW,pW))
            x = jnp.pad(x, pad_width, mode='constant', constant_values=0)

        # Calculate output dimensions
        out_H = (x.shape[2] - kH) // sH + 1
        out_W = (x.shape[3] - kW) // sW + 1

        # Use jax.lax.reduce_window to perform average pooling
        window_shape = (1, 1, kH, kW)
        strides = (1, 1, sH, sW)
        pooled = jax.lax.reduce_window(
            x,
            0.0,
            jax.lax.add,
            window_shape,
            strides,
            padding='VALID'
        )
        # Divide by window size to get average
        pooled = pooled / (kH * kW)
        return pooled

batch_size = 16
channels = 64
height = 2048
width = 2048
kernel_size = 11

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, channels, height, width))
    return [x]

def get_init_inputs():
    return [kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs sum reduction over a specified dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): Dimension to reduce over.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies sum reduction over the specified dimension.

        Args:
            x (torch.Tensor): Input tensor of shape (..., dim, ...).

        Returns:
            torch.Tensor: Output tensor after sum reduction, shape (..., 1, ...).
        """"""
        return torch.sum(x, dim=self.dim, keepdim=True)

batch_size = 128
dim1 = 4096
dim2 = 4095
reduce_dim = 1

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [reduce_dim]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs sum reduction over a specified dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): Dimension to reduce over.
        """"""
        self.dim = dim

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies sum reduction over the specified dimension.

        Args:
            x (jnp.ndarray): Input tensor of shape (..., dim, ...).

        Returns:
            jnp.ndarray: Output tensor after sum reduction, shape (..., 1, ...).
        """"""
        return jnp.sum(x, axis=self.dim, keepdims=True)

batch_size = 128
dim1 = 4096
dim2 = 4095
reduce_dim = 1

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim1, dim2))
    return [x]

def get_init_inputs():
    return [reduce_dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs mean reduction over a specific dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Reduces the input tensor along the specified dimension by taking the mean.

        Args:
            x (torch.Tensor): Input tensor of arbitrary shape.

        Returns:
            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.
        """"""
        return torch.mean(x, dim=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [1]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs mean reduction over a specific dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """"""
        self.dim = dim

    def __call__(self, x):
        """"""
        Reduces the input tensor along the specified dimension by taking the mean.

        Args:
            x (jnp.ndarray): Input tensor of arbitrary shape.

        Returns:
            jnp.ndarray: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.
        """"""
        return jnp.mean(x, axis=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim1, dim2))
    return [x]

def get_init_inputs():
    return [1]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs Max reduction over a specific dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies Max reduction over the specified dimension to the input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor after Max reduction over the specified dimension.
        """"""
        return torch.max(x, dim=self.dim)[0]

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [1] # Example, change to desired dimension
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs Max reduction over a specific dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """"""
        self.dim = dim

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies Max reduction over the specified dimension to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor.

        Returns:
            jnp.ndarray: Output tensor after Max reduction over the specified dimension.
        """"""
        return jnp.max(x, axis=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim1, dim2))
    return [x]

def get_init_inputs():
    return [1]  # Example, change to desired dimension"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs matrix-vector multiplication (C = A * B).
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix-vector multiplication.

        Args:
            A: Input matrix of shape (M, K).
            B: Input vector of shape (K, 1).

        Returns:
            Output vector of shape (M, 1).
        """"""
        return torch.matmul(A, B)

M = 256 * 8 # 2048
K = 131072 * 8 # 1048576

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(K, 1)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs matrix-vector multiplication (C = A * B).
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix-vector multiplication.

        Args:
            A: Input matrix of shape (M, K).
            B: Input vector of shape (K, 1).

        Returns:
            Output vector of shape (M, 1).
        """"""
        return jnp.matmul(A, B)

M = 256 * 8  # 2048
K = 131072 * 8  # 1048576

def get_inputs():
    key_A, key_B = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key_A, (M, K))
    B = jax.random.uniform(key_B, (K, 1))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs Argmax over a specified dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to perform argmax.

        Args:
            dim (int): The dimension to perform argmax over.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies argmax over the specified dimension to the input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor with argmax applied, with the specified dimension removed.
        """"""
        return torch.argmax(x, dim=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [1]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs Argmax over a specified dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to perform argmax.

        Args:
            dim (int): The dimension to perform argmax over.
        """"""
        self.dim = dim

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies argmax over the specified dimension to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor.

        Returns:
            jnp.ndarray: Output tensor with argmax applied, with the specified dimension removed.
        """"""
        return jnp.argmax(x, axis=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim1, dim2))
    return [x]

def get_init_inputs():
    return [1]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that finds the index of the minimum value along a specified dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to perform argmin on.

        Args:
            dim (int): Dimension along which to find the minimum value.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Finds the index of the minimum value along the specified dimension.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Tensor containing the indices of the minimum values along the specified dimension.
        """"""
        return torch.argmin(x, dim=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095
dim = 1

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [dim]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that finds the index of the minimum value along a specified dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to perform argmin on.

        Args:
            dim (int): Dimension along which to find the minimum value.
        """"""
        self.dim = dim

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Finds the index of the minimum value along the specified dimension.

        Args:
            x (jnp.ndarray): Input tensor.

        Returns:
            jnp.ndarray: Tensor containing the indices of the minimum values along the specified dimension.
        """"""
        return jnp.argmin(x, axis=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095
dim = 1

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim1, dim2))
    return [x]

def get_init_inputs():
    return [dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs min reduction over a specific dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        Applies min reduction over the specified dimension to the input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor after min reduction over the specified dimension.
        """"""
        return torch.min(x, dim=self.dim)[0]

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [1] # Example, change to desired dimension
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs min reduction over a specific dimension.
    """"""
    def __init__(self, dim: int):
        """"""
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """"""
        self.dim = dim

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        """"""
        Applies min reduction over the specified dimension to the input tensor.

        Args:
            x (jnp.ndarray): Input tensor.

        Returns:
            jnp.ndarray: Output tensor after min reduction over the specified dimension.
        """"""
        return jnp.min(x, axis=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, dim1, dim2))
    return [x]

def get_init_inputs():
    return [1]  # Example, change to desired dimension"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix-scalar multiplication (C = A * s)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:
        """"""
        Performs matrix-scalar multiplication.

        Args:
            A: Input matrix of shape (M, N)
            s: Scalar value

        Returns:
            C: Resulting matrix of shape (M, N)
        """"""
        return A * s

M = 16384 * 4
N = 4096 * 4

def get_inputs():
    A = torch.rand(M, N)
    s = 3.14
    return [A, s]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a matrix-scalar multiplication (C = A * s)
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, s: float) -> jnp.ndarray:
        """"""
        Performs matrix-scalar multiplication.

        Args:
            A: Input matrix of shape (M, N)
            s: Scalar value

        Returns:
            C: Resulting matrix of shape (M, N)
        """"""
        return A * s

M = 16384 * 4
N = 4096 * 4

def get_inputs():
    key = jax.random.PRNGKey(0)
    A = jax.random.uniform(key, (M, N))
    s = 3.14
    return [A, s]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication of A and B.

        Args:
            A: Input tensor of shape (M, K)
            B: Input tensor of shape (K, N)

        Returns:
            Output tensor of shape (M, N)
        """"""
        return torch.matmul(A, B)

M = 256
N = 256
K = 131072 * 4

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(K, N)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

M = 256
N = 256
K = 131072 * 4

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication of A and B.

        Args:
            A: Input tensor of shape (M, K)
            B: Input tensor of shape (K, N)

        Returns:
            Output tensor of shape (M, N)
        """"""
        return jnp.matmul(A, B)

def get_inputs():
    key_A, key_B = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key_A, (M, K))
    B = jax.random.uniform(key_B, (K, N))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return torch.matmul(A, B)

M = 16384 * 2
N = 16384 * 2
K = 32 * 2

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(K, N)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication.

        Args:
            A: Input tensor of shape (M, K).
            B: Input tensor of shape (K, N).

        Returns:
            Output tensor of shape (M, N).
        """"""
        return jnp.matmul(A, B)

M = 16384 * 2
N = 16384 * 2
K = 32 * 2

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (M, K))
    B = jax.random.uniform(key2, (K, N))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py

class Model(nn.Module):
    """"""
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x):
        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))

batch_size = 8192
dim = 8192

def get_inputs():
    return [torch.rand(batch_size, dim)]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import math
import jax
import jax.numpy as jnp

# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py

class Model:
    """"""
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """"""
    def __init__(self):
        pass

    def __call__(self, x):
        return 0.5 * x * (1.0 + jnp.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * jnp.power(x, 3.0))))

batch_size = 8192
dim = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, dim)]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.

    Parameters:
        dim (int): The dimension along which to perform the scan operation.
    """"""

    def __init__(self, dim):
        """"""
        Initialize the Scan model.

        Args:
            dim (int): The dimension along which to perform the cumulative sum.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x):
        """"""
        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape), where `*input_shape` 
                              can vary depending on the use case.

        Returns:
            torch.Tensor: Tensor of the same shape as `x` after applying cumulative sum along `dim`.
        """"""
        return torch.cumsum(x, dim=self.dim)

# Define input dimensions and parameters
batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    """"""
    Generates random inputs for testing the Scan model.

    Returns:
        list: A list containing a single randomly generated tensor with shape 
              (batch_size, *input_shape).
    """"""
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    """"""
    Returns the initialization parameters for the Scan model.

    Returns:
        list: A list containing the `dim` parameter for model initialization.
    """"""
    return [dim]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A simple model that performs a cumulative sum (prefix sum) operation along a specified dimension.

    Parameters:
        dim (int): The dimension along which to perform the scan operation.
    """"""

    def __init__(self, dim):
        """"""
        Initialize the Scan model.

        Args:
            dim (int): The dimension along which to perform the cumulative sum.
        """"""
        self.dim = dim

    def __call__(self, x):
        """"""
        Forward pass for the Scan model, computing the cumulative sum along the specified dimension.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, *input_shape), where `*input_shape` 
                             can vary depending on the use case.

        Returns:
            jnp.ndarray: Tensor of the same shape as `x` after applying cumulative sum along `dim`.
        """"""
        return jnp.cumsum(x, axis=self.dim)

# Define input dimensions and parameters
batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    """"""
    Generates random inputs for testing the Scan model.

    Returns:
        list: A list containing a single randomly generated tensor with shape 
              (batch_size, *input_shape).
    """"""
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, shape=(batch_size, *input_shape))
    return [x]

def get_init_inputs():
    """"""
    Returns the initialization parameters for the Scan model.

    Returns:
        list: A list containing the `dim` parameter for model initialization.
    """"""
    return [dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """"""
        Performs matrix multiplication of A and B.

        Args:
            A: Input tensor with shape (M, K).
            B: Input tensor with shape (K, N).

        Returns:
            C: Output tensor with shape (M, N).
        """"""
        return torch.matmul(A, B)

M = 8205
K = 2949
N = 5921

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(K, N)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

M = 8205
K = 2949
N = 5921

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes
    """"""
    def __init__(self):
        pass

    def __call__(self, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
        """"""
        Performs matrix multiplication of A and B.

        Args:
            A: Input tensor with shape (M, K).
            B: Input tensor with shape (K, N).

        Returns:
            C: Output tensor with shape (M, N).
        """"""
        return jnp.matmul(A, B)

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (M, K))
    B = jax.random.uniform(key2, (K, N))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a cumulative product operation along a specified dimension.

    Parameters:
        dim (int): The dimension along which to perform the cumulative product operation.
    """"""

    def __init__(self, dim):
        """"""
        Initialize the CumulativeProductModel.

        Args:
            dim (int): The dimension along which to perform the cumulative product.
        """"""
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x):
        """"""
        Forward pass, computing the cumulative product along the specified dimension.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).

        Returns:
            torch.Tensor: Tensor of the same shape as `x` after applying cumulative product along `dim`.
        """"""
        return torch.cumprod(x, dim=self.dim)

# Define input dimensions and parameters
batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [dim]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that performs a cumulative product operation along a specified dimension.

    Parameters:
        dim (int): The dimension along which to perform the cumulative product operation.
    """"""

    def __init__(self, dim):
        """"""
        Initialize the CumulativeProductModel.

        Args:
            dim (int): The dimension along which to perform the cumulative product.
        """"""
        self.dim = dim

    def __call__(self, x):
        """"""
        Forward pass, computing the cumulative product along the specified dimension.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, *input_shape).

        Returns:
            jnp.ndarray: Tensor of the same shape as `x` after applying cumulative product along `dim`.
        """"""
        return jnp.cumprod(x, axis=self.dim)

# Define input dimensions and parameters
batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    return [jnp.random.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a reverse cumulative sum operation along a specified dimension.

    Parameters:
        dim (int): The dimension along which to perform the reverse cumulative sum.
    """"""

    def __init__(self, dim):
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x):
        return torch.cumsum(x.flip(self.dim), dim=self.dim).flip(self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [dim]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that performs a reverse cumulative sum operation along a specified dimension.

    Parameters:
        dim (int): The dimension along which to perform the reverse cumulative sum.
    """"""

    def __init__(self, dim):
        self.dim = dim

    def __call__(self, x):
        # Reverse along the specified dimension
        x_reversed = jnp.flip(x, axis=self.dim)
        # Cumulative sum along the specified dimension
        cumsum = jnp.cumsum(x_reversed, axis=self.dim)
        # Reverse back along the same dimension
        return jnp.flip(cumsum, axis=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, shape=(batch_size, *input_shape))
    return [x]

def get_init_inputs():
    return [dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs an exclusive cumulative sum (does not include the current element).

    Parameters:
        dim (int): The dimension along which to perform the exclusive cumulative sum.
    """"""

    def __init__(self, dim):
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x):
        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]
        return torch.cumsum(exclusive_cumsum, dim=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [dim]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that performs an exclusive cumulative sum (does not include the current element).

    Parameters:
        dim (int): The dimension along which to perform the exclusive cumulative sum.
    """"""

    def __init__(self, dim):
        self.dim = dim

    def __call__(self, x):
        # Create a zeros slice matching the shape of x, but only the first index along self.dim
        zeros_shape = list(x.shape)
        zeros_shape[self.dim] = 1
        zeros = jnp.zeros(zeros_shape, dtype=x.dtype)
        # Concatenate zeros at the start along self.dim
        exclusive_cumsum = jnp.concatenate([zeros, x], axis=self.dim)
        # Remove the last element along self.dim
        exclusive_cumsum = jnp.take(exclusive_cumsum, indices=range(x.shape[self.dim]), axis=self.dim)
        return jnp.cumsum(exclusive_cumsum, axis=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, shape=(batch_size, *input_shape))
    return [x]

def get_init_inputs():
    return [dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.

    Parameters:
        dim (int): The dimension along which to perform the masked cumulative sum.
    """"""

    def __init__(self, dim):
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x, mask):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).
            mask (torch.Tensor): Boolean mask of the same shape as x.

        Returns:
            torch.Tensor: Cumulative sum of elements where mask is True.
        """"""
        return torch.cumsum(x * mask, dim=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    x = torch.rand(batch_size, *input_shape)
    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask
    return [x, mask]

def get_init_inputs():
    return [dim]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.

    Parameters:
        dim (int): The dimension along which to perform the masked cumulative sum.
    """"""

    def __init__(self, dim):
        self.dim = dim

    def __call__(self, x, mask):
        """"""
        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, *input_shape).
            mask (jnp.ndarray): Boolean mask of the same shape as x.

        Returns:
            jnp.ndarray: Cumulative sum of elements where mask is True.
        """"""
        # mask is boolean, so x * mask will zero out elements where mask is False
        return jnp.cumsum(x * mask, axis=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    x = jax.random.uniform(key1, shape=(batch_size, *input_shape))
    mask = jax.random.randint(key2, shape=x.shape, minval=0, maxval=2).astype(bool)
    return [x, mask]

def get_init_inputs():
    return [dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that computes the Mean Squared Error loss for regression tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, predictions, targets):
        return torch.mean((predictions - targets) ** 2)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    scale = torch.rand(())
    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return []

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that computes the Mean Squared Error loss for regression tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        pass

    def __call__(self, predictions, targets):
        return jnp.mean((predictions - targets) ** 2)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    key1, key2, key3 = jax.random.split(jax.random.PRNGKey(0), 3)
    scale = jax.random.uniform(key1, ())
    preds = jax.random.uniform(key2, (batch_size, *input_shape)) * scale
    targets = jax.random.uniform(key3, (batch_size, *input_shape))
    return [preds, targets]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that computes Cross Entropy Loss for multi-class classification tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, predictions, targets):
        return torch.nn.functional.cross_entropy(predictions, targets)

batch_size = 32768
num_classes = 4096
input_shape = (num_classes,)
dim = 1

def get_inputs():
    return [torch.rand(batch_size, *input_shape), torch.randint(0, num_classes, (batch_size,))]

def get_init_inputs():
    return []

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that computes Cross Entropy Loss for multi-class classification tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        pass

    def __call__(self, predictions, targets):
        # predictions: [batch_size, num_classes], logits
        # targets: [batch_size], integer class labels
        log_probs = jax.nn.log_softmax(predictions, axis=1)
        nll = -jnp.take_along_axis(log_probs, targets[:, None], axis=1).squeeze(axis=1)
        return nll.mean()

batch_size = 32768
num_classes = 4096
input_shape = (num_classes,)
dim = 1

def get_inputs():
    rng = jax.random.PRNGKey(0)
    rng_pred, rng_target = jax.random.split(rng)
    predictions = jax.random.uniform(rng_pred, (batch_size, *input_shape))
    targets = jax.random.randint(rng_target, (batch_size,), 0, num_classes)
    return [predictions, targets]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that computes Smooth L1 (Huber) Loss for regression tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, predictions, targets):
        return torch.nn.functional.smooth_l1_loss(predictions, targets)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    scale = torch.rand(())
    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return []

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that computes Smooth L1 (Huber) Loss for regression tasks.

    Parameters:
        None
    """"""
    def __init__(self):
        pass

    def __call__(self, predictions, targets):
        # Default beta=1.0 as in PyTorch
        beta = 1.0
        diff = predictions - targets
        abs_diff = jnp.abs(diff)
        loss = jnp.where(abs_diff < beta,
                         0.5 * (diff ** 2) / beta,
                         abs_diff - 0.5 * beta)
        # PyTorch's smooth_l1_loss default reduction is 'mean'
        return jnp.mean(loss)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    key1, key2, key3 = jax.random.split(jax.random.PRNGKey(0), 3)
    scale = jax.random.uniform(key1, ())
    x = jax.random.uniform(key2, (batch_size, *input_shape)) * scale
    y = jax.random.uniform(key3, (batch_size, *input_shape))
    return [x, y]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)
        return out

batch_size = 32
num_heads = 32
sequence_length = 512
embedding_dimension = 1024

def get_inputs():
    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    return [Q, K, V]

def get_init_inputs():
    return []

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    def __init__(self):
        pass

    def __call__(self, Q: jnp.ndarray, K: jnp.ndarray, V: jnp.ndarray) -> jnp.ndarray:
        # Q, K, V: (batch, heads, seq, dim)
        # Compute attention scores
        d = Q.shape[-1]
        attn_scores = jnp.einsum('bhqd,bhkd->bhqk', Q, K) / jnp.sqrt(d)
        attn_weights = jax.nn.softmax(attn_scores, axis=-1)
        out = jnp.einsum('bhqk,bhkd->bhqd', attn_weights, V)
        return out

batch_size = 32
num_heads = 32
sequence_length = 512
embedding_dimension = 1024

def get_inputs():
    key = jax.random.PRNGKey(0)
    Q = jax.random.uniform(
        key, 
        (batch_size, num_heads, sequence_length, embedding_dimension), 
        dtype=jnp.float16
    )
    key, subkey = jax.random.split(key)
    K = jax.random.uniform(
        subkey, 
        (batch_size, num_heads, sequence_length, embedding_dimension), 
        dtype=jnp.float16
    )
    key, subkey = jax.random.split(key)
    V = jax.random.uniform(
        subkey, 
        (batch_size, num_heads, sequence_length, embedding_dimension), 
        dtype=jnp.float16
    )
    return [Q, K, V]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that computes Kullback-Leibler Divergence for comparing two distributions.

    Parameters:
        None
    """"""
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, predictions, targets):
        return torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')

batch_size = 8192 * 2
input_shape = (8192 * 2,)
dim = 1

def get_inputs():
    scale = torch.rand(())
    return [(torch.rand(batch_size, *input_shape)*scale).softmax(dim=-1), torch.rand(batch_size, *input_shape).softmax(dim=-1)]

def get_init_inputs():
    return []

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that computes Kullback-Leibler Divergence for comparing two distributions.

    Parameters:
        None
    """"""
    def __init__(self):
        pass

    def __call__(self, predictions, targets):
        # jax.scipy.special.kl_div is elementwise, so we implement KLDiv as in PyTorch:
        # torch.nn.functional.kl_div(torch.log(predictions), targets, reduction='batchmean')
        # KL(P || Q) = sum_i targets_i * (log(targets_i) - log(predictions_i))
        # PyTorch's kl_div(input, target) expects input = log Q, target = P
        # So: kl_div(log(predictions), targets) = sum targets * (log(targets) - log(predictions))
        # reduction='batchmean' divides by batch_size
        # predictions: (batch_size, input_shape)
        # targets: (batch_size, input_shape)
        # We'll sum over all elements and divide by batch_size

        # Add small epsilon for numerical stability
        eps = 1e-8
        predictions = jnp.clip(predictions, eps, 1.0)
        targets = jnp.clip(targets, eps, 1.0)
        log_predictions = jnp.log(predictions)
        log_targets = jnp.log(targets)
        kl = targets * (log_targets - log_predictions)
        kl_sum = jnp.sum(kl)
        batch_size = predictions.shape[0]
        return kl_sum / batch_size

batch_size = 8192 * 2
input_shape = (8192 * 2,)
dim = 1

def get_inputs():
    key1, key2, key3 = jax.random.split(jax.random.PRNGKey(0), 3)
    scale = jax.random.uniform(key1, ())
    preds = jax.random.uniform(key2, (batch_size, *input_shape)) * scale
    preds = jax.nn.softmax(preds, axis=-1)
    targets = jax.random.uniform(key3, (batch_size, *input_shape))
    targets = jax.nn.softmax(targets, axis=-1)
    return [preds, targets]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that computes Triplet Margin Loss for metric learning tasks.

    Parameters:
        margin (float): The margin between the positive and negative samples.
    """"""
    def __init__(self, margin=1.0):
        super(Model, self).__init__()
        self.loss_fn = torch.nn.TripletMarginLoss(margin=margin)

    def forward(self, anchor, positive, negative):
        return self.loss_fn(anchor, positive, negative)

batch_size = 32768
input_shape = (8192,)
dim = 1

def get_inputs():
    scale = torch.rand(())
    return [torch.rand(batch_size, *input_shape)*scale, torch.rand(batch_size, *input_shape), torch.rand(batch_size, *input_shape)]
    
def get_init_inputs():
    return [1.0]  # Default margin

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    A model that computes Triplet Margin Loss for metric learning tasks.

    Parameters:
        margin (float): The margin between the positive and negative samples.
    """"""
    def __init__(self, margin=1.0):
        self.margin = margin

    def __call__(self, anchor, positive, negative):
        # Triplet Margin Loss: max(||anchor - positive|| - ||anchor - negative|| + margin, 0)
        # Default: p=2 (Euclidean)
        ap_dist = jnp.linalg.norm(anchor - positive, ord=2, axis=1)
        an_dist = jnp.linalg.norm(anchor - negative, ord=2, axis=1)
        loss = jnp.maximum(ap_dist - an_dist + self.margin, 0.0)
        return jnp.mean(loss)

batch_size = 32768
input_shape = (8192,)
dim = 1

def get_inputs():
    key = jax.random.PRNGKey(0)
    scale_key, a_key, p_key, n_key = jax.random.split(key, 4)
    scale = jax.random.uniform(scale_key, ())
    anchor = jax.random.uniform(a_key, (batch_size, *input_shape)) * scale
    positive = jax.random.uniform(p_key, (batch_size, *input_shape))
    negative = jax.random.uniform(n_key, (batch_size, *input_shape))
    return [anchor, positive, negative]

def get_init_inputs():
    return [1.0]  # Default margin"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)
    """"""
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A, B):
        """"""
        Performs the matrix multiplication.

        Args:
            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.
            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.

        Returns:
            torch.Tensor: Output matrix of shape (M, N) or (N, M)
        """"""
        return torch.matmul(A, B)

M = 16384 * 2
N = 16 * 2

def get_inputs():
    A = torch.rand(M, N)
    B = torch.rand(N, M)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    """"""
    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)
    """"""
    def __init__(self):
        pass

    def __call__(self, A, B):
        """"""
        Performs the matrix multiplication.

        Args:
            A (jnp.ndarray): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.
            B (jnp.ndarray): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.

        Returns:
            jnp.ndarray: Output matrix of shape (M, N) or (N, M)
        """"""
        return jnp.matmul(A, B)

M = 16384 * 2
N = 16 * 2

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    A = jax.random.uniform(key1, (M, N))
    B = jax.random.uniform(key2, (N, M))
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a transposed 3D convolution, clamps the output to a minimum value, 
    and then divides the result by a constant.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, min_value, divisor):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.min_value = min_value
        self.divisor = divisor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.clamp(x, min=self.min_value)
        x = x / self.divisor
        return x

batch_size = 16
in_channels = 64
out_channels = 128
depth, height, width = 24, 48, 48
kernel_size = 3
stride = 2
padding = 1
min_value = -1.0
divisor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, min_value, divisor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a transposed 3D convolution, clamps the output to a minimum value, 
    and then divides the result by a constant.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    min_value: float
    divisor: float

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute axes
        # Input x is (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)] * 3,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
            use_bias=True,
        )
        x = conv_transpose(x)
        x = jnp.clip(x, a_min=self.min_value)
        x = x / self.divisor
        # Back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 16
in_channels = 64
out_channels = 128
depth, height, width = 24, 48, 48
kernel_size = 3
stride = 2
padding = 1
min_value = -1.0
divisor = 2.0

def get_inputs():
    # JAX expects float32 by default
    x = jax.random.uniform(
        jax.random.PRNGKey(0),
        (batch_size, in_channels, depth, height, width),
        dtype=jnp.float32
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, min_value, divisor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, followed by max pooling, hardtanh activation, mean operation, and tanh activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=maxpool_stride)
        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.maxpool(x)
        x = self.hardtanh(x)
        x = torch.mean(x, dim=(2, 3), keepdim=True)
        x = torch.tanh(x)
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 256  
kernel_size  = 3
stride = 1
padding = 1
maxpool_kernel_size = 2
maxpool_stride = 2
hardtanh_min = -1
hardtanh_max = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, followed by max pooling, hardtanh activation, mean operation, and tanh activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    maxpool_kernel_size: int
    maxpool_stride: int
    hardtanh_min: float
    hardtanh_max: float

    @nn.compact
    def __call__(self, x):
        # ConvTranspose2d in Flax: use nn.ConvTranspose
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            use_bias=True,
        )(x)
        # MaxPool2d
        x = nn.max_pool(
            x,
            window_shape=(self.maxpool_kernel_size, self.maxpool_kernel_size),
            strides=(self.maxpool_stride, self.maxpool_stride),
            padding='VALID'
        )
        # Hardtanh
        x = jnp.clip(x, self.hardtanh_min, self.hardtanh_max)
        # Mean over H and W (axes 2, 3), keepdim=True
        x = jnp.mean(x, axis=(2, 3), keepdims=True)
        # Tanh activation
        x = jnp.tanh(x)
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 256  
kernel_size  = 3
stride = 1
padding = 1
maxpool_kernel_size = 2
maxpool_stride = 2
hardtanh_min = -1
hardtanh_max = 1

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, batch normalization, tanh activation, max pooling, and group normalization.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, num_groups):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.batch_norm = nn.BatchNorm2d(out_channels)
        self.tanh = nn.Tanh()
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = self.tanh(x)
        x = self.max_pool(x)
        x = self.group_norm(x)
        return x

batch_size = 512
in_channels  = 64  
out_channels = 128  
height = width = 2048  
kernel_size  = 5
stride       = 1  
padding      = 1
groups       = 8
num_groups   = 8
height, width = 32, 32

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, num_groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, batch normalization, tanh activation, max pooling, and group normalization.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    groups: int
    num_groups: int

    @nn.compact
    def __call__(self, x, train=True):
        # ConvTranspose2d in PyTorch: (N, C_in, H, W) -> (N, C_out, H_out, W_out)
        # Flax ConvTranspose: (N, H, W, C_in) -> (N, H_out, W_out, C_out)
        # So we need to transpose input and output accordingly

        # Move channels to last dimension
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C)

        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            feature_group_count=1,  # PyTorch's groups param is not supported for ConvTranspose2d in Flax
            use_bias=True,
        )(x)

        x = nn.BatchNorm(use_running_average=not train, axis=-1, momentum=0.9, epsilon=1e-5)(x)
        x = jnp.tanh(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')
        x = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-5, axis=-1)(x)

        # Move channels back to (N, C, H, W)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 512
in_channels  = 64  
out_channels = 128  
height = width = 32
kernel_size  = 5
stride       = 1  
padding      = 1
groups       = 8
num_groups   = 8

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width), minval=0.0, maxval=1.0)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, num_groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a Gemm, multiplies the result, and applies LeakyReLU.
    """"""
    def __init__(self, in_features, out_features, multiplier, negative_slope):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.multiplier = multiplier
        self.leaky_relu = nn.LeakyReLU(negative_slope)

    def forward(self, x):
        x = self.gemm(x)
        x = x * self.multiplier
        x = self.leaky_relu(x)
        return x

batch_size = 1024
in_features  = 8192  
out_features = 8192
multiplier = 2.0
negative_slope = 0.1

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, multiplier, negative_slope]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a Gemm, multiplies the result, and applies LeakyReLU.
    """"""
    in_features: int
    out_features: int
    multiplier: float
    negative_slope: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = x * self.multiplier
        x = jnp.where(x >= 0, x, self.negative_slope * x)
        return x

batch_size = 1024
in_features  = 8192  
out_features = 8192
multiplier = 2.0
negative_slope = 0.1

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_features))
    return [x]

def get_init_inputs():
    return [in_features, out_features, multiplier, negative_slope]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a series of operations:
    1. Transposed 3D convolution
    2. Mean pooling (across depth)
    3. Addition
    4. Softmax (across channels)
    5. Tanh activation
    6. Scaling
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))  # Broadcastable bias over channels
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)                            # (B, C, D, H, W)
        x = x.mean(dim=2, keepdim=True)                       # Mean pool over depth dim (D)
        x = x + self.bias                                     # Bias add per channel
        x = torch.softmax(x, dim=1)                           # Softmax over channels
        x = torch.tanh(x)                                     # Nonlinearity
        x = x * self.scaling_factor                           # Scaling
        return x

# === Test config ===
batch_size = 16
in_channels  = 16  
out_channels = 64  
depth = 32; height = width = 128  
kernel_size  = 3
stride       = 1  
padding = 1
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scaling_factor]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a series of operations:
    1. Transposed 3D convolution
    2. Mean pooling (across depth)
    3. Addition
    4. Softmax (across channels)
    5. Tanh activation
    6. Scaling
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    scaling_factor: float

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute
        # Input x is (B, C, D, H, W) -> (B, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            feature_group_count=1,
            use_bias=False,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
        )
        x = conv_transpose(x)  # (B, D, H, W, C_out)
        # Mean pool over depth (axis=1)
        x = jnp.mean(x, axis=1, keepdims=True)  # (B, 1, H, W, C_out)
        # Bias: shape (1, 1, 1, 1, C_out), broadcastable
        bias = self.param('bias', nn.initializers.normal(), (1, 1, 1, 1, self.out_channels))
        x = x + bias
        # Softmax over channels (last axis)
        x = jax.nn.softmax(x, axis=-1)
        # Tanh activation
        x = jnp.tanh(x)
        # Scaling
        x = x * self.scaling_factor
        # Return to (B, C, D, H, W) for compatibility if needed
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

# === Test config ===
batch_size = 16
in_channels  = 16  
out_channels = 64  
depth = 32
height = width = 128  
kernel_size  = 3
stride       = 1  
padding = 1
scaling_factor = 2.0

def get_inputs():
    # Return a list with a single input array, shape (B, C, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, depth, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, division, summation, and scaling.
    """"""
    def __init__(self, input_size, hidden_size, scaling_factor):
        super(Model, self).__init__()
        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, input_size).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, hidden_size).
        """"""
        x = torch.matmul(x, self.weight.T)  # Gemm
        x = x / 2  # Divide
        x = torch.sum(x, dim=1, keepdim=True) # Sum
        x = x * self.scaling_factor  # Scaling
        return x


batch_size   = 1024  
input_size   = 8192  
hidden_size  = 8192 
scaling_factor = 1.5

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, division, summation, and scaling.
    """"""
    input_size: int
    hidden_size: int
    scaling_factor: float

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, input_size).
        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, hidden_size).
        """"""
        weight = self.param(
            ""weight"",
            jax.nn.initializers.normal(),
            (self.hidden_size, self.input_size)
        )
        x = jnp.matmul(x, weight.T)  # Gemm
        x = x / 2  # Divide
        x = jnp.sum(x, axis=1, keepdims=True)  # Sum
        x = x * self.scaling_factor  # Scaling
        return x

batch_size   = 1024  
input_size   = 8192  
hidden_size  = 8192 
scaling_factor = 1.5

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A 3D convolutional transpose layer followed by Batch Normalization and subtraction.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)
        self.batch_norm = nn.BatchNorm3d(out_channels)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = x - torch.mean(x, dim=(2, 3, 4), keepdim=True)  # Subtract mean along spatial dimensions
        return x

batch_size = 16
in_channels = 16
out_channels = 32
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A 3D convolutional transpose layer followed by Batch Normalization and subtraction.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    bias: bool = True

    @nn.compact
    def __call__(self, x, train=True):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute axes
        # PyTorch: (N, C, D, H, W)
        # Flax:    (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=self.bias,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC')
        )(x)
        x = nn.BatchNorm(use_running_average=not train, axis=-1, momentum=0.9, epsilon=1e-5)(x)
        # Subtract mean along spatial dimensions (D, H, W)
        mean = jnp.mean(x, axis=(1, 2, 3), keepdims=True)
        x = x - mean
        # Convert back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 16
in_channels = 16
out_channels = 32
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1

def get_inputs():
    # Return a list with a single input array, matching PyTorch's shape: (N, C, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, depth, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, applies Mish activation, adds a value, 
    applies Hardtanh activation, and scales the output.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.add_value = add_value
        self.scale = scale

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.nn.functional.mish(x) # Mish activation
        x = x + self.add_value
        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh activation
        x = x * self.scale # Scaling
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 128  
kernel_size  = 3
stride       = 2  
padding      = 1
output_padding = 1
add_value = 0.5
scale = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, applies Mish activation, adds a value, 
    applies Hardtanh activation, and scales the output.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    add_value: float
    scale: float

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose uses (batch, height, width, channels) by default
        # Input is (batch, channels, height, width), so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # BCHW -> BHWC

        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            kernel_dilation=(1, 1),
            use_bias=True,
            name=""conv_transpose""
        )
        # Flax ConvTranspose does not have output_padding, so we manually pad if needed
        x = conv_transpose(x)
        # If output_padding > 0, pad the output accordingly
        if self.output_padding > 0:
            pad_h = (0, self.output_padding)
            pad_w = (0, self.output_padding)
            x = jnp.pad(x, ((0,0), pad_h, pad_w, (0,0)))

        # Mish activation: x * tanh(softplus(x))
        x = x * jnp.tanh(jax.nn.softplus(x))
        x = x + self.add_value
        # Hardtanh: clamp between -1 and 1
        x = jnp.clip(x, a_min=-1.0, a_max=1.0)
        x = x * self.scale

        # Transpose back to (batch, channels, height, width)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 128  
kernel_size  = 3
stride       = 2  
padding      = 1
output_padding = 1
add_value = 0.5
scale = 2

def get_inputs():
    # Return a list with a single input array, shape (batch, channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width), minval=0.0, maxval=1.0)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, divide_by):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.instance_norm = nn.InstanceNorm2d(out_channels)
        self.divide_by = divide_by

    def forward(self, x):
        x = self.conv(x)
        x = self.instance_norm(x)
        x = x / self.divide_by
        return x

batch_size = 128
in_channels  = 64  
out_channels = 128  
height = width = 128  
kernel_size = 3
divide_by = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divide_by]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies Instance Normalization, and divides by a constant.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    divide_by: float

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=True
        )(x)
        x = nn.InstanceNorm()(x)
        x = x / self.divide_by
        return x

batch_size = 128
in_channels  = 64  
out_channels = 128  
height = width = 128  
kernel_size = 3
divide_by = 2.0

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divide_by]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a sequence of operations:
        - Matrix multiplication
        - Summation
        - Max
        - Average pooling
        - LogSumExp
        - LogSumExp
    """"""
    def __init__(self, in_features, out_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, 1).
        """"""
        x = self.linear(x)  # (batch_size, out_features)
        x = torch.sum(x, dim=1, keepdim=True) # (batch_size, 1)
        x = torch.max(x, dim=1, keepdim=True)[0] # (batch_size, 1)
        x = torch.mean(x, dim=1, keepdim=True) # (batch_size, 1)
        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)
        x = torch.logsumexp(x, dim=1, keepdim=True) # (batch_size, 1)
        return x

batch_size = 1024
in_features  = 8192  
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a sequence of operations:
        - Matrix multiplication
        - Summation
        - Max
        - Average pooling
        - LogSumExp
        - LogSumExp
    """"""
    in_features: int
    out_features: int

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, in_features).
        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, 1).
        """"""
        x = nn.Dense(self.out_features)(x)  # (batch_size, out_features)
        x = jnp.sum(x, axis=1, keepdims=True)  # (batch_size, 1)
        x = jnp.max(x, axis=1, keepdims=True)  # (batch_size, 1)
        x = jnp.mean(x, axis=1, keepdims=True)  # (batch_size, 1)
        x = jax.scipy.special.logsumexp(x, axis=1, keepdims=True)  # (batch_size, 1)
        x = jax.scipy.special.logsumexp(x, axis=1, keepdims=True)  # (batch_size, 1)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, applies GELU, and normalizes with GroupNorm.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)
        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.nn.functional.gelu(x)
        x = self.group_norm(x)
        return x

batch_size   = 128  
in_channels  = 64  
out_channels = 64  
height = width = 256  
kernel_size  = 3
stride       = 1
groups = 8
num_groups = 8

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, groups, num_groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, applies GELU, and normalizes with GroupNorm.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    groups: int
    num_groups: int

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose uses (batch, height, width, channels), so we need to transpose
        # Input: (N, C, H, W) -> (N, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 1))
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            use_bias=True,
            feature_group_count=1,  # PyTorch's groups param is not directly supported for ConvTranspose2d in Flax
            padding='SAME'
        )(x)
        x = nn.gelu(x)
        x = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-5)(x)
        # Output: (N, H, W, C) -> (N, C, H, W)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size   = 128  
in_channels  = 64  
out_channels = 64  
height = width = 256  
kernel_size  = 3
stride       = 1
groups = 8
num_groups = 8

def get_inputs():
    # Return a list with a single input array, shape (batch, channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width), dtype=jnp.float32)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, groups, num_groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies ReLU, and adds a bias term.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 

    def forward(self, x):
        x = self.conv(x)
        x = torch.relu(x)
        x = x + self.bias
        return x

batch_size = 128
in_channels  = 64  
out_channels = 128  
height = width = 128
kernel_size = 3
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies ReLU, and adds a bias term.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=False,
            padding='VALID'
        )(x)
        x = jax.nn.relu(x)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias
        return x

batch_size = 128
in_channels  = 64  
out_channels = 128  
height = width = 128
kernel_size = 3
bias_shape = (out_channels, 1, 1)

def get_inputs():
    # Flax expects NHWC by default, but we use NCHW to match PyTorch, so set data_format in Conv if needed.
    # Here, we use NCHW to match the original.
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, followed by a sum, 
    a residual add, a multiplication, and another residual add.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        original_x = x.clone().detach()
        x = x + self.bias
        x = x + original_x
        x = x * original_x
        x = x + original_x
        return x

batch_size = 16
in_channels = 32
out_channels = 64
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, followed by a sum, 
    a residual add, a multiplication, and another residual add.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute
        # Input x is (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=False,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
        )
        x = conv_transpose(x)
        # output_padding is not directly supported in Flax, so we manually pad if needed
        if self.output_padding != 0:
            pad = self.output_padding
            x = jnp.pad(x, ((0,0), (0,pad), (0,pad), (0,pad), (0,0)))
        original_x = jnp.array(x)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        # bias shape: (out_channels, 1, 1, 1), need to broadcast to (N, D, H, W, C)
        # Flax output: (N, D, H, W, C), bias: (C, 1, 1, 1)
        # So reshape bias to (1, 1, 1, 1, C) and broadcast
        bias_broadcast = bias.reshape((1, 1, 1, 1, -1))
        x = x + bias_broadcast
        x = x + original_x
        x = x * original_x
        x = x + original_x
        # Return to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 16
in_channels = 32
out_channels = 64
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, depth, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, depth, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, adds a bias term, scales, applies sigmoid, and performs group normalization.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = x + self.bias
        x = x * self.scale
        x = torch.sigmoid(x)
        x = self.group_norm(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 32
height = width = 256
kernel_size = 3
num_groups = 8
bias_shape = (out_channels, 1, 1)
scale_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, adds a bias term, scales, applies sigmoid, and performs group normalization.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    num_groups: int
    bias_shape: tuple
    scale_shape: tuple

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=False,
            padding='SAME'
        )(x)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        scale = self.param('scale', nn.initializers.normal(), self.scale_shape)
        x = x + bias
        x = x * scale
        x = jax.nn.sigmoid(x)
        x = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-5)(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 32
height = width = 256
kernel_size = 3
num_groups = 8
bias_shape = (out_channels, 1, 1)
scale_shape = (out_channels, 1, 1)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, scales the result, adds a residual connection, clamps the output,
    applies LogSumExp, and finally applies the Mish activation function.
    """"""
    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):
        super(Model, self).__init__()
        self.matmul = nn.Linear(input_size, hidden_size)
        self.scale_factor = scale_factor
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, input_size).

        Returns:
            Output tensor of shape (batch_size, hidden_size).
        """"""
        x = self.matmul(x)
        x = x * self.scale_factor
        x = x + x
        x = torch.clamp(x, self.clamp_min, self.clamp_max)
        x = torch.logsumexp(x, dim=1, keepdim=True)
        x = x * torch.nn.functional.mish(x)  # Mish activation
        return x

batch_size = 1024
input_size = 8192
hidden_size = 8192
scale_factor = 2.0
clamp_min = -10.0
clamp_max = 10.0

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scale_factor, clamp_min, clamp_max]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, scales the result, adds a residual connection, clamps the output,
    applies LogSumExp, and finally applies the Mish activation function.
    """"""
    input_size: int
    hidden_size: int
    scale_factor: float
    clamp_min: float
    clamp_max: float

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, input_size).

        Returns:
            Output tensor of shape (batch_size, hidden_size).
        """"""
        x = nn.Dense(self.hidden_size)(x)
        x = x * self.scale_factor
        x = x + x
        x = jnp.clip(x, self.clamp_min, self.clamp_max)
        x = jax.nn.logsumexp(x, axis=1, keepdims=True)
        x = x * mish(x)
        return x

def mish(x):
    return x * jnp.tanh(jax.nn.softplus(x))

batch_size = 1024
input_size = 8192
hidden_size = 8192
scale_factor = 2.0
clamp_min = -10.0
clamp_max = 10.0

def get_inputs():
    return [jnp.random.rand(batch_size, input_size).astype(jnp.float32)]

def get_init_inputs():
    return [input_size, hidden_size, scale_factor, clamp_min, clamp_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Group Normalization, computes the mean
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, num_groups):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, 1).
        """"""
        x = self.conv(x)
        x = self.group_norm(x)
        x = x.mean(dim=[1, 2, 3, 4]) # Compute mean across all dimensions except batch
        return x

batch_size = 128
in_channels = 3
out_channels = 24
D, H, W = 24, 32, 32
kernel_size = 3
num_groups = 8

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Group Normalization, computes the mean
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    num_groups: int

    @nn.compact
    def __call__(self, x):
        # x: (batch_size, in_channels, D, H, W)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = nn.GroupNorm(
            num_groups=self.num_groups,
            epsilon=1e-5,
            use_bias=True,
            use_scale=True
        )(x)
        # Compute mean across all dimensions except batch
        x = jnp.mean(x, axis=(1, 2, 3, 4))
        return x

batch_size = 128
in_channels = 3
out_channels = 24
D, H, W = 24, 32, 32
kernel_size = 3
num_groups = 8

def get_inputs():
    # Returns a list with a single input array of shape (batch_size, in_channels, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, D, H, W), minval=0.0, maxval=1.0)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a 3D convolution, applies minimum operation along a specific dimension, 
    and then applies softmax.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, dim):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.dim = dim

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W)
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, H, W)
        """"""
        x = self.conv(x)
        x = torch.min(x, dim=self.dim)[0]  # Apply minimum along the specified dimension
        x = torch.softmax(x, dim=1)  # Apply softmax along the channel dimension
        return x

batch_size = 128
in_channels = 3
out_channels = 24  # Increased output channels
D, H, W = 24, 32, 32  # Increased depth
kernel_size = 3
dim = 2  # Dimension along which to apply minimum operation (e.g., depth)

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, dim]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a 3D convolution, applies minimum operation along a specific dimension, 
    and then applies softmax.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    dim: int

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, in_channels, D, H, W)
        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, out_channels, H, W)
        """"""
        # Flax Conv3D expects (batch, D, H, W, C), so we need to permute axes
        # PyTorch: (N, C, D, H, W) -> Flax: (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='VALID',
            use_bias=True,
        )(x)
        # Output is (N, D', H', W', out_channels)
        # Transpose back to (N, out_channels, D', H', W') for min/softmax logic
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        # Apply minimum along the specified dimension
        x = jnp.min(x, axis=self.dim)
        # x shape: (N, out_channels, H', W') if dim=2 (D')
        # Apply softmax along channel dimension (axis=1)
        x = jax.nn.softmax(x, axis=1)
        return x

batch_size = 128
in_channels = 3
out_channels = 24
D, H, W = 24, 32, 32
kernel_size = 3
dim = 2  # Dimension along which to apply minimum operation (e.g., depth)

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_channels, D, H, W)
    return [jnp.random.rand(batch_size, in_channels, D, H, W).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, applies minimum operation, Tanh, and another Tanh.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

    def forward(self, x):
        x = self.conv(x)
        x = torch.min(x, dim=1, keepdim=True)[0] # Apply minimum operation along the channel dimension
        x = torch.tanh(x)
        x = torch.tanh(x)
        return x

batch_size = 128
in_channels = 16
out_channels = 64
height = width = 256
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, applies minimum operation, Tanh, and another Tanh.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int

    @nn.compact
    def __call__(self, x):
        # Flax Conv expects (N, H, W, C), so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        # After conv: (N, H, W, out_channels)
        # Min along channel dimension (axis=-1)
        x = jnp.min(x, axis=-1, keepdims=True)  # (N, H, W, 1)
        x = jnp.tanh(x)
        x = jnp.tanh(x)
        # Transpose back to (N, 1, H, W)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 128
in_channels = 16
out_channels = 64
height = width = 256
kernel_size = 3

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, adds an input tensor, and applies HardSwish activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x, add_input):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).
            add_input (torch.Tensor): Input tensor to be added after transposed convolution, of shape (batch_size, out_channels, D, H, W).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W) after HardSwish activation.
        """"""
        x = self.conv_transpose(x)
        x = x + add_input
        x = x * torch.nn.functional.hardswish(x)
        return x


batch_size = 128
in_channels = 32
out_channels = 64
D, H, W = 16, 16, 16
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W), torch.rand(batch_size, out_channels, D*stride, H*stride, W*stride)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, adds an input tensor, and applies HardSwish activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x, add_input):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute axes
        # PyTorch: (N, C, D, H, W)
        # JAX/Flax: (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        add_input = jnp.transpose(add_input, (0, 2, 3, 4, 1))

        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)] * 3,
            use_bias=False,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
            transpose_kernel=True,
        )
        x = conv_transpose(x)

        # Output padding: manually crop if needed
        # Flax ConvTranspose does not have output_padding, so we crop to match add_input shape
        if x.shape != add_input.shape:
            # Crop x to match add_input shape
            crop_d = x.shape[1] - add_input.shape[1]
            crop_h = x.shape[2] - add_input.shape[2]
            crop_w = x.shape[3] - add_input.shape[3]
            x = x[:, 
                  (crop_d // 2):(x.shape[1] - (crop_d - crop_d // 2)),
                  (crop_h // 2):(x.shape[2] - (crop_h - crop_h // 2)),
                  (crop_w // 2):(x.shape[3] - (crop_w - crop_w // 2)),
                  :
            ]

        # Add bias
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        # bias_shape: (out_channels, 1, 1, 1, 1) in PyTorch (C,1,1,1,1)
        # In JAX: (C,1,1,1,1) but we need to broadcast to (N, D, H, W, C)
        # So we reshape to (1,1,1,1,C) and add
        bias = jnp.reshape(bias, (1, 1, 1, 1, self.out_channels))
        x = x + bias

        x = x + add_input

        # HardSwish: x * relu6(x + 3) / 6
        def hardswish(x):
            return x * jnp.clip(x + 3, 0, 6) / 6

        x = x * hardswish(x)

        # Transpose back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 128
in_channels = 32
out_channels = 64
D, H, W = 16, 16, 16
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1, 1, 1)

def get_inputs():
    # JAX uses numpy arrays, not torch tensors
    x = jnp.random.rand(batch_size, in_channels, D, H, W)
    add_input = jnp.random.rand(batch_size, out_channels, D*stride, H*stride, W*stride)
    return [x, add_input]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    """"""
    Model that performs:
    1. Conv3D
    2. HardSwish activation
    3. GroupNorm  
    4. Mean pooling across spatial dimensions
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        x = self.conv(x)                             # (B, C, D, H, W)
        x = F.hardswish(x)                           # Nonlinear activation
        x = self.group_norm(x)                       # Normalization over channels
        x = torch.mean(x, dim=[2, 3, 4])             # Mean over spatial dims  (B, C)
        return x

# === Test config ===
batch_size = 1024
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class GroupNorm3D(nn.Module):
    num_groups: int
    num_channels: int
    eps: float = 1e-5

    @nn.compact
    def __call__(self, x):
        # x: (B, C, D, H, W)
        B, C, D, H, W = x.shape
        assert C == self.num_channels
        assert C % self.num_groups == 0, ""num_channels must be divisible by num_groups""
        G = self.num_groups
        x = x.reshape(B, G, C // G, D, H, W)
        mean = jnp.mean(x, axis=(2, 3, 4, 5), keepdims=True)
        var = jnp.var(x, axis=(2, 3, 4, 5), keepdims=True)
        x = (x - mean) / jnp.sqrt(var + self.eps)
        x = x.reshape(B, C, D, H, W)
        gamma = self.param('gamma', nn.initializers.ones, (1, C, 1, 1, 1))
        beta = self.param('beta', nn.initializers.zeros, (1, C, 1, 1, 1))
        return x * gamma + beta

def hardswish(x):
    return x * jnp.clip(x + 3, 0, 6) / 6

class Model(nn.Module):
    """"""
    Model that performs:
    1. Conv3D
    2. HardSwish activation
    3. GroupNorm  
    4. Mean pooling across spatial dimensions
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    num_groups: int = 4
    bias: bool = True

    @nn.compact
    def __call__(self, x):
        # x: (B, C, D, H, W)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            use_bias=self.bias,
            padding='VALID'
        )(x)
        x = hardswish(x)
        x = GroupNorm3D(self.num_groups, self.out_channels)(x)
        x = jnp.mean(x, axis=(2, 3, 4))  # Mean over D, H, W -> (B, C)
        return x

# === Test config ===
batch_size = 1024
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 4

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, depth, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a batch matrix multiplication, instance normalization, summation, residual addition, and multiplication.
    """"""
    def __init__(self, in_features, out_features, eps=1e-5, momentum=0.1):
        super(Model, self).__init__()
        self.bmm = nn.Linear(in_features, out_features)
        self.instance_norm = nn.InstanceNorm2d(out_features, eps=eps, momentum=momentum)

    def forward(self, x, y):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
            y (torch.Tensor): Input tensor of shape (batch_size, out_features).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.bmm(x)
        x = self.instance_norm(x.unsqueeze(1).unsqueeze(1)).squeeze(1).squeeze(1)
        x = x + y
        x = x * y
        return x

batch_size = 1024  # Increased batch size
in_features = 8192  # Increased input features
out_features = 8192  # Increased output features

def get_inputs():
    return [torch.rand(batch_size, in_features), torch.rand(batch_size, out_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class InstanceNorm2d(nn.Module):
    num_features: int
    eps: float = 1e-5

    @nn.compact
    def __call__(self, x):
        # x: (batch, 1, 1, num_features)
        mean = jnp.mean(x, axis=(1, 2), keepdims=True)
        var = jnp.var(x, axis=(1, 2), keepdims=True)
        x_norm = (x - mean) / jnp.sqrt(var + self.eps)
        gamma = self.param('gamma', nn.initializers.ones, (1, 1, 1, self.num_features))
        beta = self.param('beta', nn.initializers.zeros, (1, 1, 1, self.num_features))
        return x_norm * gamma + beta

class Model(nn.Module):
    in_features: int
    out_features: int
    eps: float = 1e-5

    def setup(self):
        self.bmm = nn.Dense(self.out_features)
        self.instance_norm = InstanceNorm2d(self.out_features, eps=self.eps)

    def __call__(self, x, y):
        # x: (batch_size, in_features)
        # y: (batch_size, out_features)
        x = self.bmm(x)  # (batch_size, out_features)
        x = x[:, None, None, :]  # (batch_size, 1, 1, out_features)
        x = self.instance_norm(x)  # (batch_size, 1, 1, out_features)
        x = x.squeeze(axis=(1, 2))  # (batch_size, out_features)
        x = x + y
        x = x * y
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    key1, key2 = jax.random.split(jax.random.PRNGKey(0))
    x = jax.random.uniform(key1, (batch_size, in_features))
    y = jax.random.uniform(key2, (batch_size, out_features))
    return [x, y]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.
    """"""
    def __init__(self, in_features, out_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)

    def forward(self, x):
        x = self.linear(x)
        x = torch.nn.functional.mish(x)
        x = torch.nn.functional.mish(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

def mish(x):
    return x * jnp.tanh(jax.nn.softplus(x))

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies Mish, and applies Mish again.
    """"""
    in_features: int
    out_features: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = mish(x)
        x = mish(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, adds a bias term, clamps, scales, clamps, and divides.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x + self.bias
        x = torch.clamp(x, min=0.0, max=1.0)
        x = x * self.scaling_factor
        x = torch.clamp(x, min=0.0, max=1.0)
        x = x / self.scaling_factor
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 128 
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, adds a bias term, clamps, scales, clamps, and divides.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    bias_shape: tuple
    scaling_factor: float

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose uses (batch, height, width, channels) by default
        # Input is (batch, in_channels, height, width) in PyTorch, so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # to (batch, height, width, channels)
        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            kernel_init=nn.initializers.lecun_normal(),
            use_bias=False,
        )
        x = conv_transpose(x)
        # Output shape: (batch, new_height, new_width, out_channels)
        # Add bias: shape (out_channels, 1, 1) in PyTorch, so (1, 1, 1, out_channels) in Flax
        bias = self.param('bias', nn.initializers.normal(), (1, 1, 1, self.out_channels))
        x = x + bias
        x = jnp.clip(x, a_min=0.0, a_max=1.0)
        x = x * self.scaling_factor
        x = jnp.clip(x, a_min=0.0, a_max=1.0)
        x = x / self.scaling_factor
        # Transpose back to (batch, channels, height, width)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 128 
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

def get_inputs():
    # Return a list with a single JAX array of shape (batch, in_channels, height, width)
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a GEMM, applies Group Normalization, and then HardTanh.
    """"""
    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.gemm(x)
        x = self.group_norm(x)
        x = self.hardtanh(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 16
hardtanh_min = -2.0
hardtanh_max = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class GroupNorm(nn.Module):
    num_groups: int
    num_channels: int
    eps: float = 1e-5
    use_bias: bool = True
    use_scale: bool = True

    @nn.compact
    def __call__(self, x):
        # x: (batch, features)
        # Reshape to (batch, num_groups, group_size)
        assert x.shape[-1] == self.num_channels
        group_size = self.num_channels // self.num_groups
        assert self.num_channels % self.num_groups == 0
        orig_shape = x.shape
        x = x.reshape(x.shape[0], self.num_groups, group_size)
        mean = jnp.mean(x, axis=(2,), keepdims=True)
        var = jnp.var(x, axis=(2,), keepdims=True)
        x = (x - mean) / jnp.sqrt(var + self.eps)
        x = x.reshape(orig_shape)
        if self.use_scale:
            gamma = self.param('gamma', nn.initializers.ones, (self.num_channels,))
            x = x * gamma
        if self.use_bias:
            beta = self.param('beta', nn.initializers.zeros, (self.num_channels,))
            x = x + beta
        return x

class Model(nn.Module):
    in_features: int
    out_features: int
    num_groups: int
    hardtanh_min: float
    hardtanh_max: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = GroupNorm(self.num_groups, self.out_features)(x)
        x = jnp.clip(x, self.hardtanh_min, self.hardtanh_max)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 16
hardtanh_min = -2.0
hardtanh_max = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, takes the minimum with a constant, adds a bias term, and multiplies by a scaling factor.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.constant_value = constant_value
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv(x)
        x = torch.min(x, torch.tensor(self.constant_value))
        x = x + self.bias
        x = x * self.scaling_factor
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
constant_value = 0.5
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, takes the minimum with a constant, adds a bias term, and multiplies by a scaling factor.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    constant_value: float
    bias_shape: tuple
    scaling_factor: float

    @nn.compact
    def __call__(self, x):
        conv = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=False,
        )
        x = conv(x)
        x = jnp.minimum(x, self.constant_value)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias
        x = x * self.scaling_factor
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
constant_value = 0.5
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, scales the output, and then applies a minimum operation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).
        """"""
        x = self.conv(x)
        x = x * self.scale_factor
        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum along channel dimension
        return x

batch_size = 64
in_channels = 64
out_channels = 128
height = width = 256
kernel_size = 3
scale_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scale_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, scales the output, and then applies a minimum operation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    scale_factor: float

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, in_channels, height, width).
        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, out_channels, height, width).
        """"""
        # Flax Conv expects (N, H, W, C), so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        x = x * self.scale_factor
        # Minimum along channel dimension (axis=-1), keepdims=True
        x = jnp.min(x, axis=-1, keepdims=True)  # (N, H, W, 1)
        # Transpose back to (N, C, H, W)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 64
in_channels = 64
out_channels = 128
height = width = 256
kernel_size = 3
scale_factor = 2.0

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scale_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a GEMM (general matrix multiplication), applies scaling, 
    and then batch normalization.
    """"""
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)

    def forward(self, x):
        x = self.gemm(x)
        x = x * self.scale
        x = self.bn(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
scale_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, scale_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.core import freeze, unfreeze
import numpy as np

class Model(nn.Module):
    """"""
    Simple model that performs a GEMM (general matrix multiplication), applies scaling, 
    and then batch normalization.
    """"""
    in_features: int
    out_features: int
    scale_shape: tuple
    eps: float = 1e-5
    momentum: float = 0.1

    @nn.compact
    def __call__(self, x, train=True):
        x = nn.Dense(self.out_features)(x)
        scale = self.param('scale', nn.initializers.normal(), self.scale_shape)
        x = x * scale
        x = nn.BatchNorm(use_running_average=not train, 
                         momentum=self.momentum, 
                         epsilon=self.eps,
                         use_bias=True, 
                         use_scale=True)(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
scale_shape = (out_features,)

def get_inputs():
    return [jnp.array(np.random.rand(batch_size, in_features), dtype=jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, scale_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, layer normalization, GELU activation, and scaling.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True, eps=1e-5, scaling_factor=1.0):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)
        self.layer_norm = nn.LayerNorm(out_channels, eps=eps)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').
        """"""
        x = self.conv_transpose(x)
        x = self.layer_norm(x)
        x = torch.nn.functional.gelu(x)
        x = x * self.scaling_factor
        return x

batch_size = 32
in_channels = 32
out_channels = 64
D, H, W = 16, 32, 32
kernel_size = 4
stride = 2
padding = 1
bias = True
eps = 1e-5
scaling_factor = 1.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias, eps, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, layer normalization, GELU activation, and scaling.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    bias: bool = True
    eps: float = 1e-5
    scaling_factor: float = 1.0

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute
        x = jnp.transpose(x, (0, 2, 3, 4, 1))  # (N, D, H, W, C)
        conv = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)] * 3,
            use_bias=self.bias,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
        )
        x = conv(x)
        # LayerNorm over channel axis (-1)
        x = nn.LayerNorm(axis=-1, epsilon=self.eps)(x)
        x = nn.gelu(x)
        x = x * self.scaling_factor
        # Transpose back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 32
in_channels = 32
out_channels = 64
D, H, W = 16, 32, 32
kernel_size = 4
stride = 2
padding = 1
bias = True
eps = 1e-5
scaling_factor = 1.0

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, D, H, W))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias, eps, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a convolution transpose, minimum operation, sum operation, GELU activation and addition.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum operation along channel dimension
        x = torch.sum(x, dim=2, keepdim=True)  # Sum operation along height dimension
        x = torch.nn.functional.gelu(x)  # GELU activation
        x = x + self.bias
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a convolution transpose, minimum operation, sum operation, GELU activation and addition.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose2D expects (N, H, W, C), so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C)
        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            kernel_init=nn.initializers.lecun_normal(),
            use_bias=False,
        )
        x = conv_transpose(x)
        # Output shape: (N, H_out, W_out, out_channels)
        # Mimic output_padding by cropping or padding as needed
        if self.output_padding != 0:
            # Add output_padding to H and W
            pad = self.output_padding
            x = jnp.pad(x, ((0,0), (0,pad), (0,pad), (0,0)))
        # Now, min along channel dimension (last axis)
        x = jnp.min(x, axis=-1, keepdims=True)  # (N, H_out, W_out, 1)
        # Sum along height dimension (axis=1)
        x = jnp.sum(x, axis=1, keepdims=True)  # (N, 1, W_out, 1)
        # GELU activation
        x = nn.gelu(x)
        # Add bias (broadcasted)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias
        # Transpose back to (N, C, H, W) for consistency with PyTorch
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (1, 1, 1)

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, applies Swish activation, sums with a bias term, and normalizes with GroupNorm.
    """"""
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_features)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.matmul(x)
        x = torch.sigmoid(x) * x  # Swish activation
        x = x + self.bias
        x = self.group_norm(x)
        return x

batch_size = 32768
in_features = 1024
out_features = 4096
num_groups = 64
bias_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, applies Swish activation, sums with a bias term, and normalizes with GroupNorm.
    """"""
    in_features: int
    out_features: int
    num_groups: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # Linear layer
        x = nn.Dense(self.out_features)(x)
        # Swish activation: x * sigmoid(x)
        x = x * jax.nn.sigmoid(x)
        # Bias addition
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias
        # GroupNorm expects (batch, ..., features)
        # Flax's GroupNorm normalizes over the last axis by default
        x = nn.GroupNorm(num_groups=self.num_groups)(x)
        return x

batch_size = 32768
in_features = 1024
out_features = 4096
num_groups = 64
bias_shape = (out_features,)

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs average pooling, 3D transposed convolution, clamping,
    spatial softmax, and multiplication by a learnable scale.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):
        super(Model, self).__init__()
        self.avg_pool = nn.AvgPool3d(pool_kernel_size)
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth, height, width).
        """"""
        x = self.avg_pool(x)
        x = self.conv_transpose(x)
        x = torch.clamp(x, self.clamp_min, self.clamp_max)
        b, c, d, h, w = x.shape
        x = x.view(b, c, -1)                     # flatten spatial dims
        x = torch.softmax(x, dim=2)
        x = x.view(b, c, d, h, w)
        x = x * self.scale
        return x

batch_size = 32
in_channels = 32
out_channels = 64
depth, height, width = 32, 64, 64
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
pool_kernel_size = 2
clamp_min = 0.0
clamp_max = 1.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs average pooling, 3D transposed convolution, clamping,
    spatial softmax, and multiplication by a learnable scale.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    pool_kernel_size: int
    clamp_min: float
    clamp_max: float

    @nn.compact
    def __call__(self, x):
        # Average Pooling 3D
        x = nn.avg_pool(
            x,
            window_shape=(self.pool_kernel_size, self.pool_kernel_size, self.pool_kernel_size),
            strides=(self.pool_kernel_size, self.pool_kernel_size, self.pool_kernel_size),
            padding='VALID'
        )
        # ConvTranspose3D
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            kernel_init=nn.initializers.lecun_normal(),
            use_bias=True
        )(x)
        # Output padding for ConvTranspose3D
        # Flax does not support output_padding directly, so we manually pad if needed
        if self.output_padding > 0:
            pad = self.output_padding
            x = jnp.pad(
                x,
                ((0, 0), (0, 0), (0, pad), (0, pad), (0, pad)),
                mode='constant'
            )
            # Crop to match PyTorch's output shape if needed
            # (No cropping here, as we only add output_padding)
        # Clamp
        x = jnp.clip(x, self.clamp_min, self.clamp_max)
        # Spatial softmax
        b, c, d, h, w = x.shape
        x_flat = x.reshape((b, c, -1))
        x_flat = jax.nn.softmax(x_flat, axis=2)
        x = x_flat.reshape((b, c, d, h, w))
        # Learnable scale
        scale = self.param('scale', nn.initializers.ones, (1, self.out_channels, 1, 1, 1))
        x = x * scale
        return x

batch_size = 32
in_channels = 32
out_channels = 64
depth, height, width = 32, 64, 64
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
pool_kernel_size = 2
clamp_min = 0.0
clamp_max = 1.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.
    """"""
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)

    def forward(self, x):
        x = self.gemm(x)
        x = x * self.scale
        x = self.bn(x)
        return x

batch_size = 16384
in_features = 4096
out_features = 4096
scale_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, scale_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.core import freeze, unfreeze
import numpy as np

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, scales the result, and applies batch normalization.
    """"""
    in_features: int
    out_features: int
    scale_shape: tuple
    eps: float = 1e-5
    momentum: float = 0.1

    @nn.compact
    def __call__(self, x, train=True):
        x = nn.Dense(self.out_features)(x)
        scale = self.param('scale', nn.initializers.normal(), self.scale_shape)
        x = x * scale
        x = nn.BatchNorm(use_running_average=not train, 
                         momentum=self.momentum, 
                         epsilon=self.eps,
                         use_bias=True, 
                         use_scale=True)(x)
        return x

batch_size = 16384
in_features = 4096
out_features = 4096
scale_shape = (out_features,)

def get_inputs():
    # Return a list with a single JAX array of shape (batch_size, in_features)
    return [jnp.array(np.random.rand(batch_size, in_features), dtype=jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, scale_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, followed by a sum, layer normalization, average pooling, and GELU activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.sum_weight = nn.Parameter(torch.tensor(sum_weight))
        self.norm = nn.LayerNorm(norm_shape)
        self.avg_pool = nn.AvgPool3d(kernel_size=pool_kernel_size)
        self.gelu = nn.GELU()

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x + self.sum_weight
        x = self.norm(x)
        x = self.avg_pool(x)
        x = self.gelu(x)
        return x

batch_size = 32
in_channels = 32
out_channels = 64
depth, height, width = 16, 32, 32
kernel_size = (3, 3, 3)
stride = (2, 2, 2)
padding = (1, 1, 1)
output_padding = (1, 1, 1)
sum_weight = 1.0
norm_shape = (out_channels,)
pool_kernel_size = (2, 2, 2)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class ConvTranspose3D(nn.Module):
    features: int
    kernel_size: tuple
    strides: tuple = (1, 1, 1)
    padding: tuple = (0, 0, 0)
    output_padding: tuple = (0, 0, 0)
    use_bias: bool = True

    @nn.compact
    def __call__(self, x):
        # x: (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        kernel = self.param(
            'kernel',
            nn.initializers.lecun_normal(),
            self.kernel_size + (self.features, x.shape[-1])
        )
        bias = self.param(
            'bias',
            nn.initializers.zeros,
            (self.features,)
        ) if self.use_bias else None

        # Calculate output spatial shape
        d, h, w = x.shape[1:4]
        sd, sh, sw = self.strides
        pd, ph, pw = self.padding
        opd, oph, opw = self.output_padding
        kd, kh, kw = self.kernel_size

        out_d = (d - 1) * sd - 2 * pd + kd + opd
        out_h = (h - 1) * sh - 2 * ph + kh + oph
        out_w = (w - 1) * sw - 2 * pw + kw + opw
        output_shape = (x.shape[0], out_d, out_h, out_w, self.features)

        x = jax.lax.conv_transpose(
            x,
            kernel,
            strides=self.strides,
            padding=self.padding,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
            transpose_kernel=True,
            output_shape=output_shape
        )
        if self.use_bias:
            x = x + bias
        # Back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

class LayerNorm(nn.Module):
    norm_shape: tuple

    @nn.compact
    def __call__(self, x):
        # x: (N, C, D, H, W)
        # norm_shape is (C,)
        return nn.LayerNorm(axis=1, epsilon=1e-5, use_bias=True, use_scale=True)(x)

class AvgPool3D(nn.Module):
    kernel_size: tuple

    @nn.compact
    def __call__(self, x):
        # x: (N, C, D, H, W)
        kD, kH, kW = self.kernel_size
        # Pool over D, H, W (axes 2, 3, 4)
        return nn.avg_pool(
            x,
            window_shape=(1, 1, kD, kH, kW),
            strides=(1, 1, kD, kH, kW),
            padding='VALID'
        )

class Model(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: tuple
    stride: tuple
    padding: tuple
    output_padding: tuple
    sum_weight: float
    norm_shape: tuple
    pool_kernel_size: tuple

    def setup(self):
        self.conv_transpose = ConvTranspose3D(
            features=self.out_channels,
            kernel_size=self.kernel_size,
            strides=self.stride,
            padding=self.padding,
            output_padding=self.output_padding
        )
        self.sum_weight_param = self.param('sum_weight', lambda key: jnp.array(self.sum_weight, dtype=jnp.float32))
        self.norm = LayerNorm(norm_shape=self.norm_shape)
        self.avg_pool = AvgPool3D(kernel_size=self.pool_kernel_size)
        self.gelu = nn.gelu

    def __call__(self, x):
        x = self.conv_transpose(x)
        x = x + self.sum_weight_param
        x = self.norm(x)
        x = self.avg_pool(x)
        x = self.gelu(x)
        return x

batch_size = 32
in_channels = 32
out_channels = 64
depth, height, width = 16, 32, 32
kernel_size = (3, 3, 3)
stride = (2, 2, 2)
padding = (1, 1, 1)
output_padding = (1, 1, 1)
sum_weight = 1.0
norm_shape = (out_channels,)
pool_kernel_size = (2, 2, 2)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, scaling, and residual addition.

    Args:
        in_features (int): Number of input features.
        out_features (int): Number of output features.
        scaling_factor (float): Scaling factor to apply after matrix multiplication.
    """"""
    def __init__(self, in_features, out_features, scaling_factor):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        """"""
        Forward pass of the model.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.matmul(x)
        original_x = x.clone().detach()
        x = x * self.scaling_factor
        x = x + original_x
        return x

batch_size = 16384
in_features = 4096
out_features = 4096
scaling_factor = 0.5

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, scaling, and residual addition.

    Args:
        in_features (int): Number of input features.
        out_features (int): Number of output features.
        scaling_factor (float): Scaling factor to apply after matrix multiplication.
    """"""
    in_features: int
    out_features: int
    scaling_factor: float

    def setup(self):
        self.matmul = nn.Dense(self.out_features, use_bias=True)

    def __call__(self, x):
        """"""
        Forward pass of the model.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, in_features).

        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.matmul(x)
        original_x = jnp.array(x)  # clone/detach not needed in JAX; arrays are immutable
        x = x * self.scaling_factor
        x = x + original_x
        return x

batch_size = 16384
in_features = 4096
out_features = 4096
scaling_factor = 0.5

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, BatchNorm, GELU, and ReLU in sequence.
    """"""
    def __init__(self, in_features, out_features):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.batch_norm = nn.BatchNorm1d(out_features)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.gemm(x)
        x = self.batch_norm(x)
        x = torch.nn.functional.gelu(x)
        x = torch.relu(x)
        return x

batch_size = 16384
in_features = 4096
out_features = 4096

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, BatchNorm, GELU, and ReLU in sequence.
    """"""
    in_features: int
    out_features: int

    @nn.compact
    def __call__(self, x, *, train=True):
        x = nn.Dense(self.out_features)(x)
        x = nn.BatchNorm(use_running_average=not train, axis=-1, momentum=0.1, epsilon=1e-5)(x)
        x = nn.gelu(x)
        x = jnp.maximum(x, 0)
        return x

batch_size = 16384
in_features = 4096
out_features = 4096

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, global average pooling, adds a bias, applies log-sum-exp, sum, and multiplication.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.mean(x, dim=(2, 3), keepdim=True)  # Global average pooling
        x = x + self.bias
        x = torch.logsumexp(x, dim=1, keepdim=True)  # Log-sum-exp
        x = torch.sum(x, dim=(2, 3))  # Sum
        x = x * 10.0  # Multiplication
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height = width = 512
kernel_size = 3
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, global average pooling, adds a bias, applies log-sum-exp, sum, and multiplication.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # ConvTranspose2d in PyTorch: (N, C_in, H, W) -> (N, C_out, H_out, W_out)
        # Flax ConvTranspose: (N, H, W, C_in) -> (N, H_out, W_out, C_out)
        # So we need to transpose input and output accordingly
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C_in)
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=False,
            name='conv_transpose'
        )(x)
        x = jnp.transpose(x, (0, 3, 1, 2))  # (N, C_out, H_out, W_out)

        # Global average pooling over H and W
        x = jnp.mean(x, axis=(2, 3), keepdims=True)

        # Add bias (broadcasted)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias

        # Log-sum-exp over channel dimension
        x = jax.scipy.special.logsumexp(x, axis=1, keepdims=True)

        # Sum over H and W
        x = jnp.sum(x, axis=(2, 3))

        # Multiply by 10.0
        x = x * 10.0
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height = width = 512
kernel_size = 3
bias_shape = (out_channels, 1, 1)

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, max pooling, log sum exp, and ReLU activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_channels, depth, height, width)
        Returns:
            Output tensor of shape (batch_size, out_channels, depth', height', width')
        """"""
        x = self.conv(x)
        x = self.max_pool(x)
        x = torch.logsumexp(x, dim=1, keepdim=True)
        x = torch.relu(x)
        return x

batch_size = 4
in_channels = 32
out_channels = 64
depth, height, width = 32, 128, 128
kernel_size = 3
stride = 1
padding = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, max pooling, log sum exp, and ReLU activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int

    @nn.compact
    def __call__(self, x):
        # Flax Conv3D expects (batch, D, H, W, C), so we need to transpose
        # Input: (batch, in_channels, D, H, W) -> (batch, D, H, W, in_channels)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)] * 3,
            use_bias=True,
        )(x)
        # MaxPool3D: kernel_size=2, stride=2
        x = nn.max_pool(
            x,
            window_shape=(2, 2, 2),
            strides=(2, 2, 2),
            padding=""VALID""
        )
        # logsumexp over channel axis (which is now -1)
        x = jax.scipy.special.logsumexp(x, axis=-1, keepdims=True)
        x = nn.relu(x)
        # Output: (batch, D', H', W', 1) -> (batch, 1, D', H', W')
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 4
in_channels = 32
out_channels = 64
depth, height, width = 32, 128, 128
kernel_size = 3
stride = 1
padding = 1

def get_inputs():
    # Return a list with a single input array of shape (batch, in_channels, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, depth, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, multiplies by a scalar, applies global average pooling, 
    another global average pooling
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.multiplier = multiplier

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x * self.multiplier
        x = torch.mean(x, dim=[2, 3], keepdim=True)  # First global average pooling
        x = torch.mean(x, dim=[2, 3], keepdim=True)  # Second global average pooling
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
multiplier = 0.5

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, multiplies by a scalar, applies global average pooling, 
    another global average pooling
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    multiplier: float

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose uses 'SAME'/'VALID' padding, so we need to manually pad if necessary
        # PyTorch's padding is the number of zeros added to both sides, so we mimic it with jnp.pad
        # For 2D, padding is (pad_h, pad_w)
        pad_h = self.padding
        pad_w = self.padding
        if pad_h > 0 or pad_w > 0:
            x = jnp.pad(x, ((0,0), (0,0), (pad_h, pad_h), (pad_w, pad_w)))
        conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding='VALID',  # since we already padded
            use_bias=True,
        )
        x = conv_transpose(x)
        # Output padding: PyTorch adds extra size after transposed conv if needed
        # In JAX, we can pad the output if output_padding > 0
        if self.output_padding > 0:
            x = jnp.pad(
                x,
                ((0,0), (0,0), (0, self.output_padding), (0, self.output_padding))
            )
        x = x * self.multiplier
        x = jnp.mean(x, axis=(2, 3), keepdims=True)  # First global average pooling
        x = jnp.mean(x, axis=(2, 3), keepdims=True)  # Second global average pooling
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
multiplier = 0.5

def get_inputs():
    # JAX expects numpy arrays, not torch tensors
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication (Gemm), applies Sigmoid,
    another Gemm, and computes LogSumExp over features.
    """"""
    def __init__(self, input_size, hidden_size, output_size):
        super(Model, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.linear1(x)
        x = torch.sigmoid(x)
        x = self.linear2(x)
        x = torch.logsumexp(x, dim=1)  # compute LogSumExp over features per sample
        return x

batch_size = 16384
input_size = 2048
hidden_size = 4096
output_size = 1024

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, output_size]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication (Gemm), applies Sigmoid,
    another Gemm, and computes LogSumExp over features.
    """"""
    input_size: int
    hidden_size: int
    output_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.hidden_size)(x)
        x = jax.nn.sigmoid(x)
        x = nn.Dense(self.output_size)(x)
        x = jax.scipy.special.logsumexp(x, axis=1)  # compute LogSumExp over features per sample
        return x

batch_size = 16384
input_size = 2048
hidden_size = 4096
output_size = 1024

def get_inputs():
    return [jnp.random.rand(batch_size, input_size).astype(jnp.float32)]

def get_init_inputs():
    return [input_size, hidden_size, output_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, subtraction, tanh activation, subtraction and average pooling.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract1_value = subtract1_value
        self.subtract2_value = subtract2_value
        self.avgpool = nn.AvgPool2d(kernel_size_pool)

    def forward(self, x):
        x = self.conv(x)
        x = x - self.subtract1_value
        x = torch.tanh(x)
        x = x - self.subtract2_value
        x = self.avgpool(x)
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
subtract1_value = 0.5
subtract2_value = 0.2
kernel_size_pool = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, subtraction, tanh activation, subtraction and average pooling.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    subtract1_value: float
    subtract2_value: float
    kernel_size_pool: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = x - self.subtract1_value
        x = jnp.tanh(x)
        x = x - self.subtract2_value
        x = nn.avg_pool(
            x,
            window_shape=(self.kernel_size_pool, self.kernel_size_pool),
            strides=(self.kernel_size_pool, self.kernel_size_pool),
            padding='VALID'
        )
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
subtract1_value = 0.5
subtract2_value = 0.2
kernel_size_pool = 2

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Mish activation, and then applies Tanh activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, D', H', W').
        """"""
        x = self.conv(x)
        x = torch.nn.functional.mish(x)
        x = torch.tanh(x)
        return x

batch_size = 16
in_channels = 32
out_channels = 64
D, H, W = 32, 64, 64
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Mish activation, and then applies Tanh activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int = 1
    padding: int = 0

    @nn.compact
    def __call__(self, x):
        # Flax Conv3D expects (batch, D, H, W, C), so we need to transpose
        # Input x: (batch, in_channels, D, H, W)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))  # (batch, D, H, W, C)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)] * 3,
            use_bias=True,
        )(x)
        # Mish activation: x * tanh(softplus(x))
        x = x * jnp.tanh(jax.nn.softplus(x))
        x = jnp.tanh(x)
        # Transpose back to (batch, out_channels, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 16
in_channels = 32
out_channels = 64
D, H, W = 32, 64, 64
kernel_size = 3

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_channels, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, D, H, W))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, scales the output, applies tanh, multiplies by a scaling factor, and applies sigmoid.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))
        self.bias = nn.Parameter(torch.randn(bias_shape)) 

    def forward(self, x):
        x = self.conv(x)
        x = x * self.scaling_factor 
        x = torch.tanh(x)
        x = x * self.bias
        x = torch.sigmoid(x)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
scaling_factor = 2
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, scales the output, applies tanh, multiplies by a scaling factor, and applies sigmoid.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    scaling_factor: float
    bias_shape: tuple

    def setup(self):
        self.conv = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=False
        )
        self.scaling_factor_param = self.param(
            'scaling_factor_param',
            nn.initializers.normal(),
            self.bias_shape
        )
        self.bias = self.param(
            'bias',
            nn.initializers.normal(),
            self.bias_shape
        )

    def __call__(self, x):
        x = self.conv(x)
        x = x * self.scaling_factor_param
        x = jnp.tanh(x)
        x = x * self.bias
        x = jax.nn.sigmoid(x)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
scaling_factor = 2
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, applies Softmax and Sigmoid.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, bias=bias)
        self.softmax = nn.Softmax(dim=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).
        """"""
        x = self.conv_transpose(x)
        x = self.softmax(x)
        x = self.sigmoid(x)
        return x

batch_size = 16
in_channels = 32
out_channels = 64
D, H, W = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, applies Softmax and Sigmoid.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    bias: bool = True

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute axes
        # PyTorch: (N, C, D, H, W) -> Flax: (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        conv = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=self.bias,
            # Flax does not have output_padding, so we handle it below
        )
        x = conv(x)
        # Handle output_padding manually if needed
        if self.output_padding > 0:
            pad = self.output_padding
            # Pad at the end of D, H, W
            x = jnp.pad(
                x,
                ((0, 0), (0, pad), (0, pad), (0, pad), (0, 0)),
                mode='constant'
            )
            # Crop to match PyTorch's output shape if needed
            # (PyTorch's ConvTranspose3d with output_padding can be tricky, but this is a close approximation)
        # Softmax over channel axis (axis=-1 after transpose)
        x = nn.softmax(x, axis=-1)
        x = nn.sigmoid(x)
        # Back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 16
in_channels = 32
out_channels = 64
D, H, W = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, D, H, W).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies Mish, and another Mish.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

    def forward(self, x):
        x = self.conv(x)
        x = torch.nn.functional.mish(x)
        x = torch.nn.functional.mish(x)
        return x

batch_size   = 64  
in_channels  = 64  
out_channels = 128  
height = width = 256
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies Mish, and another Mish.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = mish(x)
        x = mish(x)
        return x

def mish(x):
    return x * jnp.tanh(jax.nn.softplus(x))

batch_size   = 64  
in_channels  = 64  
out_channels = 128  
height = width = 256
kernel_size = 3

def get_inputs():
    # Flax expects NHWC by default, but we keep NCHW to match PyTorch logic.
    # We'll transpose to NHWC before passing to the model.
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, in_channels, height, width))
    # Transpose to NHWC for Flax Conv2D
    x = jnp.transpose(x, (0, 2, 3, 1))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, scaling, average pooling, bias addition, and scaling.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.scale1 = nn.Parameter(torch.tensor(scale1))
        self.avg_pool = nn.AvgPool3d(kernel_size=2)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale2 = nn.Parameter(torch.tensor(scale2))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x * self.scale1
        x = self.avg_pool(x)
        x = x + self.bias
        x = x * self.scale2
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
scale1 = 0.5
scale2 = 1.0
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Tuple, Any

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, scaling, average pooling, bias addition, and scaling.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    scale1: float
    scale2: float
    bias_shape: Tuple[int, int, int, int]

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute axes
        # Input x: (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=False,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC')
        )(x)
        scale1 = self.param('scale1', lambda rng, shape: jnp.array(self.scale1), ())
        x = x * scale1
        # Average pooling
        x = nn.avg_pool(
            x,
            window_shape=(2, 2, 2),
            strides=(2, 2, 2),
            padding='VALID'
        )
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        # bias shape: (out_channels, 1, 1, 1), need to broadcast to (N, D, H, W, C)
        # Flax output shape: (N, D, H, W, C)
        # So reshape bias to (1, 1, 1, 1, out_channels) and add
        bias = jnp.reshape(bias, (1, 1, 1, 1, self.out_channels))
        x = x + bias
        scale2 = self.param('scale2', lambda rng, shape: jnp.array(self.scale2), ())
        x = x * scale2
        # Back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
scale1 = 0.5
scale2 = 1.0
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_channels, depth, height, width)
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.
    """"""
    def __init__(self, in_features, out_features, bias=True):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.subtract = nn.Parameter(torch.randn(out_features))

    def forward(self, x):
        original_x = x.clone().detach()
        # Gemm
        x = self.gemm(x)

        # Subtract
        x = x - self.subtract

        # GlobalAvgPool
        x = torch.mean(x, dim=1, keepdim=True)

        # LogSumExp
        x = torch.logsumexp(x, dim=1, keepdim=True)

        # GELU
        x = torch.nn.functional.gelu(x)

        # ResidualAdd
        x = x + original_x

        return x

batch_size = 2048
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.
    """"""
    in_features: int
    out_features: int
    bias: bool = True

    def setup(self):
        self.gemm = nn.Dense(self.out_features, use_bias=self.bias)
        self.subtract = self.param('subtract', jax.nn.initializers.normal(), (self.out_features,))

    def __call__(self, x):
        original_x = jnp.array(x)  # clone/detach equivalent

        # Gemm
        x = self.gemm(x)

        # Subtract
        x = x - self.subtract

        # GlobalAvgPool (mean over dim=1, keepdim=True)
        x = jnp.mean(x, axis=1, keepdims=True)

        # LogSumExp (over dim=1, keepdim=True)
        x = jax.scipy.special.logsumexp(x, axis=1, keepdims=True)

        # GELU
        x = jax.nn.gelu(x)

        # ResidualAdd
        # original_x shape: (batch_size, in_features)
        # x shape: (batch_size, 1, out_features) after above ops
        # But after mean and logsumexp, x is (batch_size, 1, out_features) only if keepdims=True and axis=1 on 3D input.
        # In this code, after gemm, x is (batch_size, out_features)
        # After mean over axis=1, x is (batch_size, 1)
        # After logsumexp over axis=1, x is (batch_size, 1)
        # After gelu, x is (batch_size, 1)
        # original_x is (batch_size, in_features)
        # To add, we need to broadcast or slice accordingly.
        # We'll broadcast x to match original_x shape if possible.
        x = x + original_x

        return x

batch_size = 2048
in_features = 8192
out_features = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies activation, and then applies Batch Normalization.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)

    def forward(self, x):
        x = self.conv(x)
        x = torch.multiply(torch.tanh(torch.nn.functional.softplus(x)), x)
        x = self.bn(x)
        return x

batch_size = 64
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies activation, and then applies Batch Normalization.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    eps: float = 1e-5
    momentum: float = 0.1

    @nn.compact
    def __call__(self, x, train=True):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=True
        )(x)
        # Activation: multiply(tanh(softplus(x)), x)
        x = jnp.tanh(nn.softplus(x)) * x
        x = nn.BatchNorm(
            use_running_average=not train,
            momentum=self.momentum,
            epsilon=self.eps
        )(x)
        return x

batch_size = 64
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3

def get_inputs():
    # Flax expects NHWC by default, but Conv2d in PyTorch uses NCHW.
    # We'll use NCHW for input, and set Conv to use channels_first.
    # But Flax's Conv expects NHWC, so we need to transpose.
    x = jax.random.uniform(
        jax.random.PRNGKey(0),
        (batch_size, in_channels, height, width),
        dtype=jnp.float32
    )
    # Transpose to NHWC for Flax
    x = jnp.transpose(x, (0, 2, 3, 1))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, scaling, hardtanh, and GELU activation.
    """"""
    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scaling_factor = scaling_factor
        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)
        self.gelu = nn.GELU()

    def forward(self, x):
        x = self.gemm(x)
        x = x * self.scaling_factor
        x = self.hardtanh(x)
        x = self.gelu(x)
        return x

batch_size = 2048
in_features = 8192
out_features = 8192
scaling_factor = 0.5
hardtanh_min = -2
hardtanh_max = 2

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, scaling, hardtanh, and GELU activation.
    """"""
    in_features: int
    out_features: int
    scaling_factor: float
    hardtanh_min: float
    hardtanh_max: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = x * self.scaling_factor
        x = jnp.clip(x, self.hardtanh_min, self.hardtanh_max)
        x = nn.gelu(x)
        return x

batch_size = 2048
in_features = 8192
out_features = 8192
scaling_factor = 0.5
hardtanh_min = -2
hardtanh_max = 2

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, multiplies by a learnable scalar, applies LeakyReLU, and then GELU.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.multiplier = nn.Parameter(torch.randn(multiplier_shape)) 
        self.leaky_relu = nn.LeakyReLU()

    def forward(self, x):
        x = self.conv(x)
        x = x * self.multiplier
        x = self.leaky_relu(x)
        x = torch.nn.functional.gelu(x)
        return x

batch_size = 64
in_channels = 64
out_channels = 64
height, width = 256, 256
kernel_size = 3
multiplier_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, multiplier_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, multiplies by a learnable scalar, applies LeakyReLU, and then GELU.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    multiplier_shape: tuple

    def setup(self):
        self.conv = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=True
        )
        self.multiplier = self.param(
            'multiplier',
            nn.initializers.normal(),
            self.multiplier_shape
        )
        self.leaky_relu = nn.leaky_relu

    def __call__(self, x):
        x = self.conv(x)
        x = x * self.multiplier
        x = self.leaky_relu(x)
        x = nn.gelu(x)
        return x

batch_size = 64
in_channels = 64
out_channels = 64
height, width = 256, 256
kernel_size = 3
multiplier_shape = (out_channels, 1, 1)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, multiplier_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs matrix multiplication, max pooling, sum, and scaling.
    """"""
    def __init__(self, in_features, out_features, kernel_size, scale_factor):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.max_pool = nn.MaxPool1d(kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.matmul(x)
        x = self.max_pool(x.unsqueeze(1)).squeeze(1)
        x = torch.sum(x, dim=1)
        x = x * self.scale_factor
        return x

batch_size = 128
in_features = 32768
out_features = 32768
kernel_size = 2
scale_factor = 0.5

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, kernel_size, scale_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs matrix multiplication, max pooling, sum, and scaling.
    """"""
    in_features: int
    out_features: int
    kernel_size: int
    scale_factor: float

    @nn.compact
    def __call__(self, x):
        # x: (batch_size, in_features)
        x = nn.Dense(self.out_features)(x)  # (batch_size, out_features)
        # Reshape for 1D max pooling: (batch_size, 1, out_features)
        x = x[:, None, :]
        # Apply 1D max pooling along the last dimension
        x = nn.max_pool(
            x,
            window_shape=(1, self.kernel_size),
            strides=(1, self.kernel_size),
            padding='VALID'
        )
        # Remove the singleton dimension: (batch_size, pooled_features)
        x = jnp.squeeze(x, axis=1)
        # Sum along features
        x = jnp.sum(x, axis=1)
        # Scale
        x = x * self.scale_factor
        return x

batch_size = 128
in_features = 32768
out_features = 32768
kernel_size = 2
scale_factor = 0.5

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_features))
    return [x]

def get_init_inputs():
    return [in_features, out_features, kernel_size, scale_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies sigmoid, and sums the result.
    """"""
    def __init__(self, input_size, hidden_size):
        super(Model, self).__init__()
        self.linear = nn.Linear(input_size, hidden_size)

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, input_size).

        Returns:
            Output tensor of shape (batch_size, 1).
        """"""
        x = self.linear(x)
        x = torch.sigmoid(x)
        x = torch.sum(x, dim=1, keepdim=True)
        return x

batch_size = 128
input_size = 32768
hidden_size = 32768

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies sigmoid, and sums the result.
    """"""
    input_size: int
    hidden_size: int

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, input_size).

        Returns:
            Output tensor of shape (batch_size, 1).
        """"""
        x = nn.Dense(self.hidden_size)(x)
        x = jax.nn.sigmoid(x)
        x = jnp.sum(x, axis=1, keepdims=True)
        return x

batch_size = 128
input_size = 32768
hidden_size = 32768

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies ReLU, and applies HardSwish activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

    def forward(self, x):
        x = self.conv(x)
        x = torch.relu(x)
        x = x * torch.clamp((x + 3) / 6, 0, 1)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies ReLU, and applies HardSwish activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        x = jax.nn.relu(x)
        x = x * jnp.clip((x + 3) / 6, 0, 1)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, LogSumExp, HardSwish, subtraction, clamp operations.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.bias = nn.Parameter(torch.randn(1, 1, 1, 1)) 

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.logsumexp(x, dim=1, keepdim=True)
        x = x * torch.sigmoid(x + 3) / 6
        x = x - self.bias
        x = torch.clamp(x, min=-1, max=1)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
bias_shape = (1, 1, 1, 1)  

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class ConvTranspose3d(nn.Module):
    features: int
    kernel_size: int
    strides: int
    padding: int

    @nn.compact
    def __call__(self, x):
        # x: (batch, in_channels, D, H, W)
        # Flax expects (batch, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        kernel_shape = (self.kernel_size, self.kernel_size, self.kernel_size)
        x = nn.ConvTranspose(
            features=self.features,
            kernel_size=kernel_shape,
            strides=(self.strides, self.strides, self.strides),
            padding=[(self.padding, self.padding)] * 3,
            feature_group_count=1,
            use_bias=True,
            dtype=jnp.float32,
            name=""conv_transpose3d""
        )(x)
        # Back to (batch, channels, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

class Model(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    bias_shape: tuple

    def setup(self):
        self.conv_transpose = ConvTranspose3d(
            features=self.out_channels,
            kernel_size=self.kernel_size,
            strides=self.stride,
            padding=self.padding
        )
        self.bias = self.param('bias', nn.initializers.normal(), self.bias_shape)

    def __call__(self, x):
        x = self.conv_transpose(x)
        x = jax.scipy.special.logsumexp(x, axis=1, keepdims=True)
        x = x * jax.nn.sigmoid(x + 3) / 6
        x = x - self.bias
        x = jnp.clip(x, a_min=-1, a_max=1)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
bias_shape = (1, 1, 1, 1)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.
    """"""
    def __init__(self, in_features, out_features, scaling_factor):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.matmul(x)
        x = x * torch.sigmoid(x)  # Swish activation
        x = x * self.scaling_factor
        return x

batch_size = 128
in_features = 32768
out_features = 32768
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies Swish activation, and scales the result.
    """"""
    in_features: int
    out_features: int
    scaling_factor: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = x * jax.nn.sigmoid(x)  # Swish activation
        x = x * self.scaling_factor
        return x

batch_size = 128
in_features = 32768
out_features = 32768
scaling_factor = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, subtracts a bias term, and applies tanh activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x - self.bias
        x = torch.tanh(x)
        return x

batch_size = 32
in_channels  = 64  
out_channels = 64  
height = width = 256 
kernel_size = 4
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, subtracts a bias term, and applies tanh activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    bias_shape: tuple
    stride: int = 2
    padding: int = 1
    output_padding: int = 1

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose uses (batch, height, width, channels)
        # Input x is expected to be (batch, in_channels, height, width), so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C)
        conv = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            use_bias=False,
            name=""conv_transpose""
        )
        x = conv(x)
        # Add output_padding by slicing or padding if needed
        if self.output_padding > 0:
            pad_h = self.output_padding
            pad_w = self.output_padding
            x = jnp.pad(x, ((0,0), (0,pad_h), (0,pad_w), (0,0)))
            # Optionally, you may want to slice to exact shape if needed

        bias = self.param('bias', jax.nn.initializers.normal(), self.bias_shape)
        # bias shape: (out_channels, 1, 1)
        # x shape: (batch, H, W, out_channels)
        # Reshape bias for broadcasting
        bias_broadcast = bias.reshape((1, 1, 1, self.out_channels))
        x = x - bias_broadcast
        x = jnp.tanh(x)
        # Transpose back to (batch, channels, height, width)
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 32
in_channels  = 64  
out_channels = 64  
height = width = 256 
kernel_size = 4
bias_shape = (out_channels, 1, 1)

def get_inputs():
    # Return a list with a single input array, shape (batch, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, applies Swish activation, 
    group normalization, and then HardSwish activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)
        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.sigmoid(x) * x  # Swish activation
        x = self.group_norm(x)
        x = torch.nn.functional.hardswish(x)  # HardSwish activation
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
groups = 4
eps = 1e-5

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, applies Swish activation, 
    group normalization, and then HardSwish activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    groups: int
    eps: float
    bias: bool = True

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to permute
        # PyTorch: (N, C, D, H, W) -> Flax: (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=self.bias,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC')
        )(x)
        # Swish activation: sigmoid(x) * x
        x = jax.nn.sigmoid(x) * x
        # GroupNorm expects (N, ..., C) so NDHWC is fine
        x = nn.GroupNorm(num_groups=self.groups, epsilon=self.eps)(x)
        # HardSwish: x * relu6(x + 3) / 6
        x = x * jnp.clip(x + 3, 0, 6) / 6
        # Back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
groups = 4
eps = 1e-5

def get_inputs():
    # PyTorch: (N, C, D, H, W)
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed 3D convolution, applies ReLU, and then applies group normalization.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=bias)
        self.relu = nn.ReLU()
        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, D, H, W).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, D, H, W).
        """"""
        x = self.conv_transpose(x)
        x = self.relu(x)
        x = self.group_norm(x)
        return x

batch_size = 16
in_channels = 64
out_channels = 128
D, H, W = 32, 32, 32
kernel_size = 3
groups = 8
bias = False

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups, bias]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class GroupNorm3D(nn.Module):
    num_groups: int
    num_channels: int
    eps: float = 1e-5
    use_bias: bool = True
    use_scale: bool = True

    @nn.compact
    def __call__(self, x):
        # x: (batch, channels, D, H, W)
        B, C, D, H, W = x.shape
        assert C == self.num_channels, ""Input channels must match num_channels""
        G = self.num_groups
        assert C % G == 0, ""num_channels must be divisible by num_groups""
        x = x.reshape(B, G, C // G, D, H, W)
        mean = jnp.mean(x, axis=(2,3,4,5), keepdims=True)
        var = jnp.var(x, axis=(2,3,4,5), keepdims=True)
        x = (x - mean) / jnp.sqrt(var + self.eps)
        x = x.reshape(B, C, D, H, W)
        if self.use_scale:
            gamma = self.param('gamma', nn.initializers.ones, (1, C, 1, 1, 1))
            x = x * gamma
        if self.use_bias:
            beta = self.param('beta', nn.initializers.zeros, (1, C, 1, 1, 1))
            x = x + beta
        return x

class ConvTranspose3d(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    bias: bool = False

    @nn.compact
    def __call__(self, x):
        # x: (batch, in_channels, D, H, W)
        # Flax ConvTranspose expects (batch, D, H, W, in_channels)
        x = jnp.transpose(x, (0,2,3,4,1))
        conv = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1,1,1),
            padding='SAME',
            use_bias=self.bias,
            feature_group_count=1,
            dtype=x.dtype,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC')
        )
        x = conv(x)
        # Back to (batch, out_channels, D, H, W)
        x = jnp.transpose(x, (0,4,1,2,3))
        return x

class Model(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    groups: int
    bias: bool = False

    @nn.compact
    def __call__(self, x):
        x = ConvTranspose3d(
            in_channels=self.in_channels,
            out_channels=self.out_channels,
            kernel_size=self.kernel_size,
            bias=self.bias
        )(x)
        x = nn.relu(x)
        x = GroupNorm3D(
            num_groups=self.groups,
            num_channels=self.out_channels
        )(x)
        return x

batch_size = 16
in_channels = 64
out_channels = 128
D, H, W = 32, 32, 32
kernel_size = 3
groups = 8
bias = False

def get_inputs():
    # Return a list with a single input array of shape (batch, in_channels, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, D, H, W))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups, bias]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.
    """"""
    def __init__(self, input_size, hidden_size, num_groups, eps=1e-5, negative_slope=0.01):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, hidden_size)
        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=hidden_size, eps=eps)
        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)

    def forward(self, x):
        """"""
        Performs the forward pass of the model.

        Args:
            x: Input tensor of shape (batch_size, input_size).

        Returns:
            Output tensor of shape (batch_size, hidden_size).
        """"""
        x = self.fc(x)
        x = self.gn(x)
        x = self.leaky_relu(x)
        x = x + x
        return x


batch_size = 1024
input_size = 8192
hidden_size = 8192
num_groups = 512

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, num_groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, group normalization, leaky ReLU activation, and element-wise sum.
    """"""
    input_size: int
    hidden_size: int
    num_groups: int
    eps: float = 1e-5
    negative_slope: float = 0.01

    @nn.compact
    def __call__(self, x):
        # Linear layer
        x = nn.Dense(self.hidden_size)(x)
        # GroupNorm expects (batch, ..., channels), so we add a dummy spatial dim
        # to match (batch, 1, channels), then remove it after normalization
        x = x[:, None, :]
        x = nn.GroupNorm(num_groups=self.num_groups, epsilon=self.eps)(x)
        x = x[:, 0, :]
        # LeakyReLU
        x = jnp.where(x > 0, x, self.negative_slope * x)
        # Element-wise sum (x + x)
        x = x + x
        return x

batch_size = 1024
input_size = 8192
hidden_size = 8192
num_groups = 512

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, num_groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies ReLU, and divides by a constant.
    """"""
    def __init__(self, in_features, out_features, divisor):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.divisor = divisor

    def forward(self, x):
        x = self.linear(x)
        x = torch.relu(x)
        x = x / self.divisor
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
divisor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, divisor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies ReLU, and divides by a constant.
    """"""
    in_features: int
    out_features: int
    divisor: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = jax.nn.relu(x)
        x = x / self.divisor
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
divisor = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, divisor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication (Gemm), followed by LogSumExp, LeakyReLU, 
    LeakyReLU, GELU, and GELU activations.
    """"""
    def __init__(self, in_features, out_features, bias=True):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features, bias=bias)

    def forward(self, x):
        # Gemm
        x = self.linear(x)
        # LogSumExp
        x = torch.logsumexp(x, dim=1, keepdim=True)
        # LeakyReLU
        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)
        # LeakyReLU
        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)
        # GELU
        x = torch.nn.functional.gelu(x)
        # GELU
        x = torch.nn.functional.gelu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication (Gemm), followed by LogSumExp, LeakyReLU, 
    LeakyReLU, GELU, and GELU activations.
    """"""
    in_features: int
    out_features: int
    bias: bool = True

    @nn.compact
    def __call__(self, x):
        # Gemm
        x = nn.Dense(self.out_features, use_bias=self.bias)(x)
        # LogSumExp
        x = jax.nn.logsumexp(x, axis=1, keepdims=True)
        # LeakyReLU
        x = jax.nn.leaky_relu(x, negative_slope=0.01)
        # LeakyReLU
        x = jax.nn.leaky_relu(x, negative_slope=0.01)
        # GELU
        x = jax.nn.gelu(x)
        # GELU
        x = jax.nn.gelu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    This model performs a convolution, average pooling, applies sigmoid, and sums the result.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.avg_pool = nn.AvgPool2d(pool_kernel_size)

    def forward(self, x):
        x = self.conv(x)
        x = self.avg_pool(x)
        x = torch.sigmoid(x)
        x = torch.sum(x, dim=[1,2,3]) # Sum over all spatial dimensions
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 384, 384
kernel_size = 3
pool_kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, pool_kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    This model performs a convolution, average pooling, applies sigmoid, and sums the result.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    pool_kernel_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = nn.avg_pool(
            x,
            window_shape=(self.pool_kernel_size, self.pool_kernel_size),
            strides=(self.pool_kernel_size, self.pool_kernel_size),
            padding='VALID'
        )
        x = jax.nn.sigmoid(x)
        x = jnp.sum(x, axis=(1, 2, 3))  # Sum over all spatial dimensions
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 384, 384
kernel_size = 3
pool_kernel_size = 4

def get_inputs():
    # Flax expects NHWC by default, but Conv2d in PyTorch uses NCHW.
    # We'll keep NCHW for compatibility, and set data_format='NCHW' in Conv if needed.
    # But Flax's nn.Conv expects NHWC, so we need to transpose.
    x = jax.random.uniform(
        jax.random.PRNGKey(0),
        (batch_size, in_channels, height, width),
        minval=0.0,
        maxval=1.0
    )
    # Transpose to NHWC for Flax
    x = jnp.transpose(x, (0, 2, 3, 1))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, pool_kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs matrix multiplication, applies dropout, and then applies softmax.
    """"""
    def __init__(self, in_features, out_features, dropout_p):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.matmul(x)
        x = self.dropout(x)
        x = torch.softmax(x, dim=1)  # Softmax over features
        return x

batch_size = 128
in_features = 16384
out_features = 16384
dropout_p = 0.2

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, dropout_p]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Any, Callable

class Model(nn.Module):
    """"""
    A model that performs matrix multiplication, applies dropout, and then applies softmax.
    """"""
    in_features: int
    out_features: int
    dropout_p: float

    @nn.compact
    def __call__(self, x, *, train: bool, rng=None):
        x = nn.Dense(self.out_features)(x)
        x = nn.Dropout(rate=self.dropout_p, deterministic=not train)(x, deterministic=not train, rng=rng)
        x = nn.softmax(x, axis=1)  # Softmax over features
        return x

batch_size = 128
in_features = 16384
out_features = 16384
dropout_p = 0.2

def get_inputs():
    # Returns a list with a single JAX array of shape (batch_size, in_features)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_features))
    return [x]

def get_init_inputs():
    # Returns a list of model init arguments
    return [in_features, out_features, dropout_p]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies GELU, and then performs global average pooling.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_channels, height, width)
        Returns:
            Output tensor of shape (batch_size, out_channels)
        """"""
        x = self.conv(x)
        x = torch.nn.functional.gelu(x)
        x = torch.nn.functional.adaptive_avg_pool2d(x, 1)
        x = x.squeeze(-1).squeeze(-1)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies GELU, and then performs global average pooling.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_channels, height, width)
        Returns:
            Output tensor of shape (batch_size, out_channels)
        """"""
        # Flax Conv expects (N, H, W, C), so we need to transpose
        x = jnp.transpose(x, (0, 2, 3, 1))  # (N, H, W, C)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        x = nn.gelu(x)
        # Global average pooling over spatial dims (H, W)
        x = jnp.mean(x, axis=(1, 2))  # (N, out_channels)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies minimum, and subtracts a constant.
    """"""
    def __init__(self, in_features, out_features, constant):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.constant = nn.Parameter(torch.tensor(constant))

    def forward(self, x):
        x = self.linear(x)
        x = torch.min(x, self.constant)
        x = x - self.constant
        return x

batch_size = 128
in_features = 16384
out_features = 16384
constant = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, constant]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies minimum, and subtracts a constant.
    """"""
    in_features: int
    out_features: int
    constant: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        const = self.constant
        x = jnp.minimum(x, const)
        x = x - const
        return x

batch_size = 128
in_features = 16384
out_features = 16384
constant = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, constant]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, applies HardSwish, and then ReLU.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).
        """"""
        x = self.conv(x)
        x = torch.nn.functional.hardswish(x)
        x = torch.relu(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, applies HardSwish, and then ReLU.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        # HardSwish: x * relu6(x + 3) / 6
        x = x * jnp.clip(x + 3, 0, 6) / 6
        x = jnp.maximum(x, 0)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3

def get_inputs():
    # Flax expects NHWC by default, but we keep NCHW for parity with PyTorch
    # So we will use data_format='NCHW' in Conv if needed, but Flax Conv expects NHWC
    # Let's transpose input to NHWC for Flax
    x = jax.random.uniform(
        jax.random.PRNGKey(0),
        (batch_size, in_channels, height, width),
        minval=0.0,
        maxval=1.0
    )
    # Transpose to NHWC for Flax Conv
    x = jnp.transpose(x, (0, 2, 3, 1))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Softmax, and performs two max pooling operations.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.pool1 = nn.MaxPool3d(pool_kernel_size)
        self.pool2 = nn.MaxPool3d(pool_kernel_size)

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_channels, depth, height, width)
        Returns:
            Output tensor of shape (batch_size, out_channels, depth', height', width') where depth', height', width' are the dimensions after pooling.
        """"""
        x = self.conv(x)
        x = torch.softmax(x, dim=1)
        x = self.pool1(x)
        x = self.pool2(x)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
pool_kernel_size = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, pool_kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Softmax, and performs two max pooling operations.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    pool_kernel_size: int

    @nn.compact
    def __call__(self, x):
        # Conv3D in Flax expects (N, D, H, W, C), so we need to transpose
        # Input: (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        # Softmax along channel axis (last axis after transpose)
        x = nn.softmax(x, axis=-1)
        # MaxPool3D: pool over (D, H, W)
        x = nn.max_pool(
            x,
            window_shape=(self.pool_kernel_size, self.pool_kernel_size, self.pool_kernel_size),
            strides=(self.pool_kernel_size, self.pool_kernel_size, self.pool_kernel_size),
            padding='SAME'
        )
        x = nn.max_pool(
            x,
            window_shape=(self.pool_kernel_size, self.pool_kernel_size, self.pool_kernel_size),
            strides=(self.pool_kernel_size, self.pool_kernel_size, self.pool_kernel_size),
            padding='SAME'
        )
        # Transpose back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
pool_kernel_size = 2

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_channels, depth, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, depth, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, pool_kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model implementing the pattern ""Gemm_Sigmoid_Scaling_ResidualAdd"".
    """"""
    def __init__(self, input_size, hidden_size, scaling_factor):
        super(Model, self).__init__()
        self.gemm = nn.Linear(input_size, hidden_size)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        """"""
        Forward pass of the model.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, input_size).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, hidden_size).
        """"""
        x = self.gemm(x)
        original_x = x
        x = torch.sigmoid(x)
        x = x * self.scaling_factor
        x = x + original_x
        return x

batch_size = 1024
input_size = 8192
hidden_size = 8192
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model implementing the pattern ""Gemm_Sigmoid_Scaling_ResidualAdd"".
    """"""
    input_size: int
    hidden_size: int
    scaling_factor: float

    @nn.compact
    def __call__(self, x):
        """"""
        Forward pass of the model.

        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, input_size).

        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, hidden_size).
        """"""
        x = nn.Dense(self.hidden_size)(x)
        original_x = x
        x = jax.nn.sigmoid(x)
        x = x * self.scaling_factor
        x = x + original_x
        return x

batch_size = 1024
input_size = 8192
hidden_size = 8192
scaling_factor = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, divisor):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.divisor = divisor

    def forward(self, x):
        x = self.conv(x)
        x = x / self.divisor
        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
divisor = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divisor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, divides by a constant, and applies LeakyReLU.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    divisor: float

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = x / self.divisor
        x = jax.nn.leaky_relu(x, negative_slope=0.01)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
divisor = 2

def get_inputs():
    # Flax expects NHWC by default, but we use NCHW to match PyTorch, so set data_format in Conv if needed.
    # Here, we use NCHW to match the original.
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divisor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a 3D transposed convolution, followed by batch normalization, 
    two average pooling layers.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.batch_norm = nn.BatchNorm3d(out_channels)
        self.avg_pool1 = nn.AvgPool3d(kernel_size=2)
        self.avg_pool2 = nn.AvgPool3d(kernel_size=2)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = self.avg_pool1(x)
        x = self.avg_pool2(x)
        return x


batch_size = 64
in_channels = 3
out_channels = 16
depth, height, width = 32, 32, 32
kernel_size = 3
stride = 2
padding = 1
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a 3D transposed convolution, followed by batch normalization, 
    two average pooling layers.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x, train=True):
        # Flax ConvTranspose3D expects (N, D, H, W, C), so we need to transpose
        # PyTorch uses (N, C, D, H, W)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=True,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC')
        )(x)
        x = nn.BatchNorm(use_running_average=not train, axis=-1)(x)
        # Average pooling in Flax expects (N, D, H, W, C)
        x = nn.avg_pool(x, window_shape=(2, 2, 2), strides=(2, 2, 2), padding='VALID')
        x = nn.avg_pool(x, window_shape=(2, 2, 2), strides=(2, 2, 2), padding='VALID')
        # Transpose back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 64
in_channels = 3
out_channels = 16
depth, height, width = 32, 32, 32
kernel_size = 3
stride = 2
padding = 1
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies Batch Normalization, and scales the output.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = x * self.scaling_factor
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a convolution, applies Batch Normalization, and scales the output.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    scaling_factor: float

    @nn.compact
    def __call__(self, x, *, train=True):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=True,
            padding='SAME'
        )(x)
        x = nn.BatchNorm(use_running_average=not train, axis=1)(x)
        x = x * self.scaling_factor
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
scaling_factor = 2.0

def get_inputs():
    # Flax expects NCHW by default for Conv/BatchNorm if axis=1
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.
    """"""
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.gemm(x)
        x = self.group_norm(x)
        x = torch.min(x, dim=1, keepdim=True)[0] 
        x = x + self.bias
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 512
bias_shape = (1, out_features, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, Group Normalization, Minimum operation, and Bias addition.
    """"""
    in_features: int
    out_features: int
    num_groups: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # GEMM (Linear)
        x = nn.Dense(self.out_features)(x)
        # GroupNorm expects (N, ..., C), so add a dummy spatial dimension to match PyTorch's (N, C, H, W)
        x = x[:, None, :]  # (N, 1, C)
        # Flax GroupNorm expects features as the last dim, so transpose to (N, 1, C)
        x = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-5, use_bias=True, use_scale=True)(x)
        # Min over the ""spatial"" dimension (axis=1), keepdim=True
        x = jnp.min(x, axis=1, keepdims=True)  # (N, 1, C)
        # Add bias: bias shape is (1, out_features, 1, 1), need to broadcast to (N, 1, C)
        # We'll define bias as a parameter
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        # Reshape bias to (1, 1, C) to match (N, 1, C)
        bias_reshaped = jnp.reshape(bias, (1, 1, self.out_features))
        x = x + bias_reshaped
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 512
bias_shape = (1, out_features, 1, 1)

def get_inputs():
    # Return a list with a single input array of shape (batch_size, in_features)
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, adds a bias term, and applies ReLU.
    """"""
    def __init__(self, in_features, out_features, bias_shape):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=False)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor with shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor with shape (batch_size, out_features).
        """"""
        x = self.gemm(x)
        x = x + self.bias
        x = torch.relu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bias_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, adds a bias term, and applies ReLU.
    """"""
    in_features: int
    out_features: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # Linear layer without bias
        x = nn.Dense(self.out_features, use_bias=False)(x)
        # Custom bias parameter
        bias = self.param('bias', jax.nn.initializers.normal(), self.bias_shape)
        x = x + bias
        x = jax.nn.relu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bias_shape = (out_features,)

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, scales the output, applies batch normalization, 
    and then performs global average pooling. 
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor
        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x * self.scale_factor
        x = self.batch_norm(x)
        x = self.global_avg_pool(x)
        return x

batch_size = 16
in_channels = 64
out_channels = 128
depth, height, width = 16, 32, 32
kernel_size = 5
scale_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scale_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, scales the output, applies batch normalization, 
    and then performs global average pooling. 
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    scale_factor: float
    eps: float = 1e-5
    momentum: float = 0.1

    @nn.compact
    def __call__(self, x, train=True):
        # ConvTranspose3d in PyTorch uses (N, C, D, H, W)
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=True,
            dimension_numbers=('NCDHW', 'OIDHW', 'NCDHW')
        )(x)
        x = x * self.scale_factor
        x = nn.BatchNorm(
            use_running_average=not train,
            momentum=self.momentum,
            epsilon=self.eps,
            axis=1,  # channel axis for NCDHW
        )(x)
        # Global average pooling over D, H, W (axes 2, 3, 4)
        x = jnp.mean(x, axis=(2, 3, 4), keepdims=True)
        return x

batch_size = 16
in_channels = 64
out_channels = 128
depth, height, width = 16, 32, 32
kernel_size = 5
scale_factor = 2.0

def get_inputs():
    # Return a list with a single input array, matching PyTorch's shape (N, C, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key, (batch_size, in_channels, depth, height, width), minval=0.0, maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scale_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, followed by two max pooling layers and a sum operation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.max_pool1 = nn.MaxPool3d(kernel_size=2)
        self.max_pool2 = nn.MaxPool3d(kernel_size=3)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.max_pool1(x)
        x = self.max_pool2(x)
        x = torch.sum(x, dim=1, keepdim=True) 
        return x

batch_size = 16
in_channels = 32
out_channels = 64
depth, height, width = 32, 32, 32
kernel_size = 5
stride = 2
padding = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D transposed convolution, followed by two max pooling layers and a sum operation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int

    @nn.compact
    def __call__(self, x):
        # ConvTranspose3d in PyTorch: (N, C_in, D, H, W) -> (N, C_out, D_out, H_out, W_out)
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=True,
            dimension_numbers=('NCDHW', 'OIDHW', 'NCDHW')
        )(x)
        # MaxPool3d with kernel_size=2
        x = nn.max_pool(
            x,
            window_shape=(1, 2, 2, 2, 1),
            strides=(1, 2, 2, 2, 1),
            padding='VALID'
        )
        # MaxPool3d with kernel_size=3
        x = nn.max_pool(
            x,
            window_shape=(1, 3, 3, 3, 1),
            strides=(1, 3, 3, 3, 1),
            padding='VALID'
        )
        # Sum over channel dimension (axis=1), keepdim=True
        x = jnp.sum(x, axis=1, keepdims=True)
        return x

batch_size = 16
in_channels = 32
out_channels = 64
depth, height, width = 32, 32, 32
kernel_size = 5
stride = 2
padding = 2

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A 3D convolutional layer followed by multiplication, instance normalization, clamping, multiplication, and a max operation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))
        self.instance_norm = nn.InstanceNorm3d(out_channels)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.conv(x)
        x = x * self.multiplier
        x = self.instance_norm(x)
        x = torch.clamp(x, self.clamp_min, self.clamp_max)
        x = x * self.multiplier
        x = torch.max(x, dim=1)[0]
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
multiplier_shape = (out_channels, 1, 1, 1)
clamp_min = -1.0
clamp_max = 1.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A 3D convolutional layer followed by multiplication, instance normalization, clamping, multiplication, and a max operation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    multiplier_shape: tuple
    clamp_min: float
    clamp_max: float

    @nn.compact
    def __call__(self, x):
        # Conv3D: Flax expects (N, D, H, W, C), PyTorch uses (N, C, D, H, W)
        # So we need to transpose before and after
        x = jnp.transpose(x, (0, 2, 3, 4, 1))  # (N, D, H, W, C)
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=True,
        )(x)
        # multiplier shape: (out_channels, 1, 1, 1)
        # Flax param: (out_channels, 1, 1, 1)
        multiplier = self.param('multiplier', nn.initializers.normal(), self.multiplier_shape)
        # Broadcast multiplier to (1, D, H, W, out_channels)
        multiplier_broadcast = multiplier.reshape((1,) + self.multiplier_shape)
        x = x * multiplier_broadcast
        # InstanceNorm3d: normalize over (D, H, W) for each sample and channel
        # Flax nn.InstanceNorm expects (N, ..., C)
        x = nn.InstanceNorm(axis=(1,2,3), use_scale=True, use_bias=True)(x)
        x = jnp.clip(x, self.clamp_min, self.clamp_max)
        x = x * multiplier_broadcast
        # Max over channel dimension (last axis)
        x = jnp.max(x, axis=-1)
        # Transpose back to (N, D, H, W)
        x = jnp.transpose(x, (0, 1, 2, 3))
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
multiplier_shape = (out_channels, 1, 1, 1)
clamp_min = -1.0
clamp_max = 1.0

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, depth, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, depth, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies ReLU, LeakyReLU, GELU, Sigmoid activations, and bias in sequence.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 

    def forward(self, x):
        x = self.conv(x)
        x = torch.relu(x)
        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)
        x = torch.nn.functional.gelu(x)
        x = torch.sigmoid(x)
        x = x + self.bias
        return x

batch_size = 64
in_channels = 8
out_channels = 32
depth, height, width = 32, 64, 64
kernel_size = 3
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies ReLU, LeakyReLU, GELU, Sigmoid activations, and bias in sequence.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    bias_shape: tuple

    @nn.compact
    def __call__(self, x):
        # Flax Conv3D expects (N, D, H, W, C), so we need to transpose
        # PyTorch: (N, C, D, H, W) -> Flax: (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            use_bias=False,
            padding='SAME'
        )(x)
        x = nn.relu(x)
        x = nn.leaky_relu(x, negative_slope=0.01)
        x = nn.gelu(x)
        x = nn.sigmoid(x)
        # Add learnable bias
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        # bias shape: (out_channels, 1, 1, 1)
        # x shape: (N, D, H, W, out_channels)
        # Reshape bias to (1, 1, 1, 1, out_channels) for broadcasting
        bias = jnp.reshape(bias, (1, 1, 1, 1, self.out_channels))
        x = x + bias
        # Transpose back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

batch_size = 64
in_channels = 8
out_channels = 32
depth, height, width = 32, 64, 64
kernel_size = 3
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    # Return a list with a single input array, matching PyTorch's shape: (N, C, D, H, W)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, depth, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, followed by a max operation, subtraction, and GELU activation.
    """"""
    def __init__(self, in_features, out_features, max_dim):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.max_dim = max_dim

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_features)

        Returns:
            Output tensor of shape (batch_size, out_features)
        """"""
        x = self.gemm(x)
        x = torch.max(x, dim=self.max_dim, keepdim=True).values
        x = x - x.mean(dim=1, keepdim=True)
        x = torch.nn.functional.gelu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
max_dim = 1

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, max_dim]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, followed by a max operation, subtraction, and GELU activation.
    """"""
    in_features: int
    out_features: int
    max_dim: int

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_features)

        Returns:
            Output tensor of shape (batch_size, out_features)
        """"""
        x = nn.Dense(self.out_features)(x)
        x = jnp.max(x, axis=self.max_dim, keepdims=True)
        x = x - jnp.mean(x, axis=1, keepdims=True)
        x = nn.gelu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
max_dim = 1

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, max_dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a gemm, swish, divide, clamp, tanh, and clamp operations.
    """"""
    def __init__(self, in_features, out_features, bias=True):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.gemm(x)
        x = x * torch.sigmoid(x)  # Swish activation
        x = x / 2.0
        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1
        x = torch.tanh(x)  # Tanh activation
        x = torch.clamp(x, min=-1.0, max=1.0)  # Clamp between -1 and 1
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a gemm, swish, divide, clamp, tanh, and clamp operations.
    """"""
    in_features: int
    out_features: int
    bias: bool = True

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features, use_bias=self.bias)(x)
        x = x * jax.nn.sigmoid(x)  # Swish activation
        x = x / 2.0
        x = jnp.clip(x, a_min=-1.0, a_max=1.0)  # Clamp between -1 and 1
        x = jnp.tanh(x)  # Tanh activation
        x = jnp.clip(x, a_min=-1.0, a_max=1.0)  # Clamp between -1 and 1
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)

    def forward(self, x):
        # Convolution
        x = self.conv(x)
        # Tanh activation
        x = torch.tanh(x)
        # Scaling
        x = x * self.scaling_factor
        # Bias addition
        x = x + self.bias
        # Max-pooling
        x = self.max_pool(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
scaling_factor = 2.0
bias_shape = (out_channels, 1, 1)
pool_kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    scaling_factor: float
    bias_shape: tuple
    pool_kernel_size: int

    @nn.compact
    def __call__(self, x):
        # Convolution
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=False
        )(x)
        # Tanh activation
        x = jnp.tanh(x)
        # Scaling
        x = x * self.scaling_factor
        # Bias addition (broadcasted)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias
        # Max-pooling
        x = nn.max_pool(
            x,
            window_shape=(self.pool_kernel_size, self.pool_kernel_size),
            strides=(self.pool_kernel_size, self.pool_kernel_size),
            padding='VALID'
        )
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
scaling_factor = 2.0
bias_shape = (out_channels, 1, 1)
pool_kernel_size = 4

def get_inputs():
    # Flax expects NHWC by default, but Conv2d in PyTorch uses NCHW.
    # We'll use NCHW for compatibility, and set Conv to use NCHW.
    # However, Flax's nn.Conv expects NHWC, so we need to transpose.
    # To keep the logic close to PyTorch, we generate NCHW and transpose in the model.
    x = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, in_channels, height, width))
    # Transpose to NHWC for Flax Conv
    x = jnp.transpose(x, (0, 2, 3, 1))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.norm = nn.GroupNorm(groups, out_channels)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = torch.min(x, torch.tensor(min_value, device=x.device))
        x = torch.clamp(x, min=min_value, max=max_value)
        x = self.dropout(x)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
groups = 8
min_value = 0.0
max_value = 1.0
dropout_p = 0.2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Any, Callable, Sequence

class GroupNorm(nn.Module):
    num_groups: int
    num_channels: int
    eps: float = 1e-5
    affine: bool = True

    @nn.compact
    def __call__(self, x):
        # x: (N, C, D, H, W)
        N, C, D, H, W = x.shape
        G = self.num_groups
        assert C % G == 0, ""num_channels must be divisible by num_groups""
        x = x.reshape((N, G, C // G, D, H, W))
        mean = jnp.mean(x, axis=(2, 3, 4, 5), keepdims=True)
        var = jnp.var(x, axis=(2, 3, 4, 5), keepdims=True)
        x = (x - mean) / jnp.sqrt(var + self.eps)
        x = x.reshape((N, C, D, H, W))
        if self.affine:
            gamma = self.param('gamma', nn.initializers.ones, (1, C, 1, 1, 1))
            beta = self.param('beta', nn.initializers.zeros, (1, C, 1, 1, 1))
            x = x * gamma + beta
        return x

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    groups: int
    min_value: float
    max_value: float
    dropout_p: float

    @nn.compact
    def __call__(self, x, *, deterministic: bool = False):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = GroupNorm(self.groups, self.out_channels)(x)
        x = jnp.minimum(x, self.min_value)
        x = jnp.clip(x, a_min=self.min_value, a_max=self.max_value)
        x = nn.Dropout(rate=self.dropout_p)(x, deterministic=deterministic)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
groups = 8
min_value = 0.0
max_value = 1.0
dropout_p = 0.2

def get_inputs():
    # Flax expects (N, C, D, H, W) by default for Conv3D
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.
    """"""
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.gemm(x)
        x = self.bn(x)
        x = self.scale * x
        x = self.softmax(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bn_eps = 1e-5
bn_momentum = 0.1
scale_shape = (1,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication (Gemm), Batch Normalization, scaling, and Softmax.
    """"""
    in_features: int
    out_features: int
    bn_eps: float = 1e-5
    bn_momentum: float = 0.1
    scale_shape: tuple = (1,)

    @nn.compact
    def __call__(self, x, train: bool = True):
        x = nn.Dense(self.out_features)(x)
        x = nn.BatchNorm(use_running_average=not train, 
                         momentum=1.0 - self.bn_momentum, 
                         epsilon=self.bn_eps)(x)
        scale = self.param('scale', nn.initializers.ones, self.scale_shape)
        x = scale * x
        x = nn.softmax(x, axis=1)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bn_eps = 1e-5
bn_momentum = 0.1
scale_shape = (1,)

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, bn_eps, bn_momentum, scale_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs convolution, group normalization, scaling, max pooling, and clamping.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        """"""
        Args:
            x: Input tensor of shape (batch_size, in_channels, height, width).
        Returns:
            Output tensor of shape (batch_size, out_channels, height', width').
        """"""
        x = self.conv(x)
        x = self.group_norm(x)
        x = x * self.scale
        x = self.maxpool(x)
        x = torch.clamp(x, self.clamp_min, self.clamp_max)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128 
kernel_size = 3
num_groups = 16
scale_shape = (out_channels, 1, 1)
maxpool_kernel_size = 4
clamp_min = 0.0
clamp_max = 1.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs convolution, group normalization, scaling, max pooling, and clamping.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    num_groups: int
    scale_shape: tuple
    maxpool_kernel_size: int
    clamp_min: float
    clamp_max: float

    @nn.compact
    def __call__(self, x):
        # Conv2D: (batch, in_channels, height, width) -> (batch, out_channels, height', width')
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=True,
            padding='SAME'
        )(x)
        # GroupNorm: num_groups, axis=1 (channel axis)
        x = nn.GroupNorm(
            num_groups=self.num_groups,
            axis=1,
            epsilon=1e-5,
            use_bias=True,
            use_scale=True
        )(x)
        # Learnable scale parameter (broadcasted)
        scale = self.param('scale', nn.initializers.ones, self.scale_shape)
        x = x * scale
        # MaxPool2D: kernel_size
        x = nn.max_pool(
            x,
            window_shape=(self.maxpool_kernel_size, self.maxpool_kernel_size),
            strides=(self.maxpool_kernel_size, self.maxpool_kernel_size),
            padding='VALID'
        )
        # Clamp
        x = jnp.clip(x, self.clamp_min, self.clamp_max)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128 
kernel_size = 3
num_groups = 16
scale_shape = (out_channels, 1, 1)
maxpool_kernel_size = 4
clamp_min = 0.0
clamp_max = 1.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, divides by a scalar, and applies GELU activation.
    """"""
    def __init__(self, input_size, output_size, divisor):
        super(Model, self).__init__()
        self.linear = nn.Linear(input_size, output_size)
        self.divisor = divisor

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, input_size).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, output_size).
        """"""
        x = self.linear(x)
        x = x / self.divisor
        x = torch.nn.functional.gelu(x)
        return x

batch_size = 1024
input_size = 8192
output_size = 8192
divisor = 10.0

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, output_size, divisor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model that performs a matrix multiplication, divides by a scalar, and applies GELU activation.
    """"""
    input_size: int
    output_size: int
    divisor: float

    @nn.compact
    def __call__(self, x):
        """"""
        Args:
            x (jnp.ndarray): Input tensor of shape (batch_size, input_size).
        Returns:
            jnp.ndarray: Output tensor of shape (batch_size, output_size).
        """"""
        x = nn.Dense(self.output_size)(x)
        x = x / self.divisor
        x = nn.gelu(x)
        return x

batch_size = 1024
input_size = 8192
output_size = 8192
divisor = 10.0

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, output_size, divisor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, subtracts two values, applies Mish activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value_1 = subtract_value_1
        self.subtract_value_2 = subtract_value_2

    def forward(self, x):
        x = self.conv(x)
        x = x - self.subtract_value_1
        x = x - self.subtract_value_2
        x = torch.nn.functional.mish(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
subtract_value_1 = 0.5
subtract_value_2 = 0.2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, subtracts two values, applies Mish activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    subtract_value_1: float
    subtract_value_2: float

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = x - self.subtract_value_1
        x = x - self.subtract_value_2
        x = mish(x)
        return x

def mish(x):
    return x * jnp.tanh(jax.nn.softplus(x))

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
subtract_value_1 = 0.5
subtract_value_2 = 0.2

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.
    """"""
    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape)) 

    def forward(self, x):
        # (batch_size, in_features) -> (batch_size, out_features)
        x = self.gemm(x)
        # (batch_size, out_features) -> (batch_size, out_features)
        x = self.group_norm(x)
        # (batch_size, out_features) -> (batch_size, out_features)
        x = x * torch.sigmoid(x)
        # (batch_size, out_features) -> (batch_size, out_features)
        x = x * self.multiply_weight
        # (batch_size, out_features) -> (batch_size, out_features)
        x = x * torch.sigmoid(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 256
multiply_weight_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, multiply_weight_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a GEMM, GroupNorm, Swish, Multiply, and Swish operations.
    """"""
    in_features: int
    out_features: int
    num_groups: int
    multiply_weight_shape: tuple

    @nn.compact
    def __call__(self, x):
        # (batch_size, in_features) -> (batch_size, out_features)
        x = nn.Dense(self.out_features)(x)
        # (batch_size, out_features) -> (batch_size, out_features)
        x = nn.GroupNorm(num_groups=self.num_groups, epsilon=1e-5, use_bias=True, use_scale=True)(x)
        # (batch_size, out_features) -> (batch_size, out_features)
        x = x * jax.nn.sigmoid(x)
        # (batch_size, out_features) -> (batch_size, out_features)
        multiply_weight = self.param('multiply_weight', nn.initializers.normal(), self.multiply_weight_shape)
        x = x * multiply_weight
        # (batch_size, out_features) -> (batch_size, out_features)
        x = x * jax.nn.sigmoid(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 256
multiply_weight_shape = (out_features,)

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, num_groups, multiply_weight_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a sequence of operations:
        - ConvTranspose3d
        - MaxPool3d
        - Softmax
        - Subtract
        - Swish
        - Max
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.max_pool = nn.MaxPool3d(kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding)
        self.subtract = nn.Parameter(torch.randn(out_channels)) # Assuming subtraction is element-wise across channels

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.max_pool(x)
        x = torch.softmax(x, dim=1) # Apply softmax across channels (dim=1)
        x = x - self.subtract.view(1, -1, 1, 1, 1) # Subtract across channels
        x = torch.sigmoid(x) * x # Swish activation
        x = torch.max(x, dim=1)[0] # Max pooling across channels
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
pool_kernel_size = 2
pool_stride = 2
pool_padding = 0

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class ConvTranspose3d(nn.Module):
    features: int
    kernel_size: int
    strides: int
    padding: int
    output_padding: int

    @nn.compact
    def __call__(self, x):
        # x: (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        kernel_shape = (self.kernel_size, self.kernel_size, self.kernel_size)
        strides = (self.strides, self.strides, self.strides)
        # Flax ConvTranspose expects NHWDC, so we use dimension_numbers accordingly
        conv = nn.ConvTranspose(
            features=self.features,
            kernel_size=kernel_shape,
            strides=strides,
            padding=[(self.padding, self.padding)]*3,
            kernel_init=nn.initializers.lecun_normal(),
            use_bias=True,
            dimension_numbers=('NDHWC', 'DHWIO', 'NDHWC'),
        )
        x = conv(x)
        # Output padding: manually pad if needed
        if self.output_padding > 0:
            pad = [(0, 0)]
            for _ in range(3):
                pad.append((0, self.output_padding))
            pad.append((0, 0))
            x = jnp.pad(x, pad)
        # Back to (N, C, D, H, W)
        x = jnp.transpose(x, (0, 4, 1, 2, 3))
        return x

class MaxPool3d(nn.Module):
    kernel_size: int
    stride: int
    padding: int

    @nn.compact
    def __call__(self, x):
        # x: (N, C, D, H, W)
        window_shape = (1, 1, self.kernel_size, self.kernel_size, self.kernel_size)
        strides = (1, 1, self.stride, self.stride, self.stride)
        pad = [(0, 0), (0, 0)]
        for _ in range(3):
            pad.append((self.padding, self.padding))
        x = jax.lax.reduce_window(
            x,
            -jnp.inf,
            jax.lax.max,
            window_shape,
            strides,
            pad
        )
        return x

class Model(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    pool_kernel_size: int
    pool_stride: int
    pool_padding: int

    def setup(self):
        self.conv_transpose = ConvTranspose3d(
            features=self.out_channels,
            kernel_size=self.kernel_size,
            strides=self.stride,
            padding=self.padding,
            output_padding=self.output_padding
        )
        self.max_pool = MaxPool3d(
            kernel_size=self.pool_kernel_size,
            stride=self.pool_stride,
            padding=self.pool_padding
        )
        self.subtract = self.param(
            'subtract',
            nn.initializers.normal(),
            (self.out_channels,)
        )

    def __call__(self, x):
        x = self.conv_transpose(x)
        x = self.max_pool(x)
        # Softmax across channels (axis=1)
        x = jax.nn.softmax(x, axis=1)
        # Subtract across channels
        x = x - self.subtract.reshape((1, -1, 1, 1, 1))
        # Swish activation
        x = jax.nn.sigmoid(x) * x
        # Max across channels (axis=1)
        x = jnp.max(x, axis=1)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
pool_kernel_size = 2
pool_stride = 2
pool_padding = 0

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, depth, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, divides by a constant, applies max pooling,
    global average pooling, adds a bias term, and sums along a specific dimension.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.divisor = divisor
        self.max_pool = nn.MaxPool3d(pool_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.sum_dim = sum_dim

    def forward(self, x):
        x = self.conv(x)
        x = x / self.divisor
        x = self.max_pool(x)
        x = self.global_avg_pool(x)
        x = x + self.bias
        x = torch.sum(x, dim=self.sum_dim)
        return x

batch_size   = 128  
in_channels  = 8            
out_channels = 16  
depth = 16; height = width = 64 
kernel_size = (3, 3, 3)
divisor = 2.0
pool_size = (2, 2, 2)
bias_shape = (out_channels, 1, 1, 1)
sum_dim = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, divides by a constant, applies max pooling,
    global average pooling, adds a bias term, and sums along a specific dimension.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: tuple
    divisor: float
    pool_size: tuple
    bias_shape: tuple
    sum_dim: int

    @nn.compact
    def __call__(self, x):
        # Conv3D: Flax expects (N, D, H, W, C), so we need to transpose
        # Input: (N, C, D, H, W) -> (N, D, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 4, 1))
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=self.kernel_size,
            strides=(1, 1, 1),
            padding='VALID',
            use_bias=False,
        )(x)
        x = x / self.divisor
        # MaxPool3D
        x = nn.max_pool(
            x,
            window_shape=self.pool_size,
            strides=self.pool_size,
            padding='VALID'
        )
        # Global average pooling to (N, 1, 1, 1, C)
        x = jnp.mean(x, axis=(1, 2, 3), keepdims=True)
        # Add bias: bias shape is (out_channels, 1, 1, 1)
        # Flax param: (C, 1, 1, 1) -> (1, 1, 1, 1, C) for broadcasting
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        bias = jnp.reshape(bias, (1, 1, 1, 1, self.out_channels))
        x = x + bias
        # Sum along sum_dim (original PyTorch: sum_dim=1 means channel dim)
        # After transpose, dims: (N, 1, 1, 1, C), so sum_dim=1 is correct if user wants to sum over D
        x = jnp.sum(x, axis=self.sum_dim)
        # Transpose back if needed (not needed here, as output is (N, 1, 1, C))
        return x

batch_size   = 128  
in_channels  = 8            
out_channels = 16  
depth = 16
height = width = 64 
kernel_size = (3, 3, 3)
divisor = 2.0
pool_size = (2, 2, 2)
bias_shape = (out_channels, 1, 1, 1)
sum_dim = 1

def get_inputs():
    # Return a list with a single input array, shape (batch_size, in_channels, depth, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, depth, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies LeakyReLU, sums with a tensor, clamps, and applies GELU activation.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))

    def forward(self, x):
        x = self.conv(x)
        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)
        x = x + self.sum_tensor
        x = torch.clamp(x, min=-1.0, max=1.0)
        x = torch.nn.functional.gelu(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
depth, height, width = 16, 64, 64
kernel_size = 3
sum_tensor_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, sum_tensor_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a 3D convolution, applies LeakyReLU, sums with a tensor, clamps, and applies GELU activation.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    sum_tensor_shape: tuple

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(1, 1, 1),
            padding='SAME',
            use_bias=True
        )(x)
        x = jax.nn.leaky_relu(x, negative_slope=0.2)
        sum_tensor = self.param(
            'sum_tensor',
            nn.initializers.normal(),
            self.sum_tensor_shape
        )
        x = x + sum_tensor
        x = jnp.clip(x, a_min=-1.0, a_max=1.0)
        x = jax.nn.gelu(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
depth, height, width = 16, 64, 64
kernel_size = 3
sum_tensor_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, sum_tensor_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, applies softmax, adds a bias term, scales the result, and applies sigmoid.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.softmax(x, dim=1)
        x = x + self.bias
        x = x * self.scaling_factor
        x = torch.sigmoid(x)
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height, width = 64, 64
kernel_size = 4
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, applies softmax, adds a bias term, scales the result, and applies sigmoid.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    output_padding: int
    bias_shape: tuple
    scaling_factor: float

    def setup(self):
        self.conv_transpose = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.padding, self.padding), (self.padding, self.padding)],
            kernel_init=nn.initializers.lecun_normal(),
            use_bias=False,
        )
        self.bias = self.param('bias', nn.initializers.normal(), self.bias_shape)

    def __call__(self, x):
        x = self.conv_transpose(x)
        x = jax.nn.softmax(x, axis=1)
        x = x + self.bias
        x = x * self.scaling_factor
        x = jax.nn.sigmoid(x)
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height, width = 64, 64
kernel_size = 4
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a convolution, applies Group Normalization, Tanh, HardSwish, 
    Residual Addition, and LogSumExp.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, groups, eps=1e-5):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(groups, out_channels, eps=eps)
        self.tanh = nn.Tanh()
        self.hard_swish = nn.Hardswish()

    def forward(self, x):
        # Convolution
        x_conv = self.conv(x)
        # Group Normalization
        x_norm = self.group_norm(x_conv)
        # Tanh
        x_tanh = self.tanh(x_norm)
        # HardSwish
        x_hard_swish = self.hard_swish(x_tanh)
        # Residual Addition
        x_res = x_conv + x_hard_swish
        # LogSumExp
        x_logsumexp = torch.logsumexp(x_res, dim=1, keepdim=True)
        return x_logsumexp

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
groups = 16

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class GroupNorm(nn.Module):
    num_groups: int
    num_channels: int
    eps: float = 1e-5

    @nn.compact
    def __call__(self, x):
        # x: (N, C, H, W)
        N, C, H, W = x.shape
        G = self.num_groups
        assert C % G == 0, ""num_channels must be divisible by num_groups""
        x = x.reshape(N, G, C // G, H, W)
        mean = jnp.mean(x, axis=(2, 3, 4), keepdims=True)
        var = jnp.var(x, axis=(2, 3, 4), keepdims=True)
        x = (x - mean) / jnp.sqrt(var + self.eps)
        x = x.reshape(N, C, H, W)
        gamma = self.param('gamma', nn.initializers.ones, (1, C, 1, 1))
        beta = self.param('beta', nn.initializers.zeros, (1, C, 1, 1))
        return x * gamma + beta

class Model(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    groups: int
    eps: float = 1e-5

    @nn.compact
    def __call__(self, x):
        # Convolution
        x_conv = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            use_bias=True,
            padding='SAME'
        )(x)
        # Group Normalization
        x_norm = GroupNorm(self.groups, self.out_channels, eps=self.eps)(x_conv)
        # Tanh
        x_tanh = jnp.tanh(x_norm)
        # HardSwish: x * relu6(x + 3) / 6
        x_hard_swish = x_tanh * jnp.clip(x_tanh + 3, 0, 6) / 6
        # Residual Addition
        x_res = x_conv + x_hard_swish
        # LogSumExp over channel dimension (axis=1), keepdim=True
        x_logsumexp = jax.scipy.special.logsumexp(x_res, axis=1, keepdims=True)
        return x_logsumexp

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
groups = 16

def get_inputs():
    # JAX expects float32 by default
    return [jnp.random.rand(batch_size, in_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, adds a value, takes the minimum, applies GELU, and multiplies by a value.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, add_value, multiply_value):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)
        self.add_value = add_value
        self.multiply_value = multiply_value

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x + self.add_value
        x = torch.min(x, torch.tensor(0.0, device=x.device))
        x = torch.nn.functional.gelu(x)
        x = x * self.multiply_value
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height, width = 64, 64
kernel_size = 4
stride = 2
add_value = 0.5
multiply_value = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, add_value, multiply_value]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed convolution, adds a value, takes the minimum, applies GELU, and multiplies by a value.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    add_value: float
    multiply_value: float

    @nn.compact
    def __call__(self, x):
        # Flax ConvTranspose uses (batch, height, width, channels) by default
        # Input is (batch, channels, height, width), so transpose to NHWC
        x = jnp.transpose(x, (0, 2, 3, 1))
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding='SAME',
            use_bias=True,
        )(x)
        x = x + self.add_value
        x = jnp.minimum(x, 0.0)
        x = nn.gelu(x)
        x = x * self.multiply_value
        # Transpose back to NCHW
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height, width = 64, 64
kernel_size = 4
stride = 2
add_value = 0.5
multiply_value = 2.0

def get_inputs():
    # Return a list with a single JAX array of shape (batch, channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, add_value, multiply_value]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model that performs a GEMM, BiasAdd, Hardtanh, Mish, and GroupNorm operations in sequence.
    """"""
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.hardtanh = nn.Hardtanh()
        self.mish = nn.Mish()
        self.groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=out_features)

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.gemm(x)
        x = x + self.bias
        x = self.hardtanh(x)
        x = self.mish(x)
        x = self.groupnorm(x)
        return x


batch_size = 1024
in_features = 8192
out_features = 8192
bias_shape = (out_features,)
num_groups = 256

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bias_shape, num_groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Hardtanh(nn.Module):
    min_val: float = -1.0
    max_val: float = 1.0

    @nn.compact
    def __call__(self, x):
        return jnp.clip(x, self.min_val, self.max_val)

class Mish(nn.Module):
    @nn.compact
    def __call__(self, x):
        return x * jnp.tanh(jax.nn.softplus(x))

class GroupNorm(nn.Module):
    num_groups: int
    num_channels: int
    eps: float = 1e-5

    @nn.compact
    def __call__(self, x):
        # x: (batch, features)
        # Reshape to (batch, num_groups, group_size)
        assert x.shape[-1] == self.num_channels
        group_size = self.num_channels // self.num_groups
        x_reshaped = x.reshape(x.shape[0], self.num_groups, group_size)
        mean = jnp.mean(x_reshaped, axis=(2,), keepdims=True)
        var = jnp.var(x_reshaped, axis=(2,), keepdims=True)
        x_norm = (x_reshaped - mean) / jnp.sqrt(var + self.eps)
        x_norm = x_norm.reshape(x.shape)
        gamma = self.param('gamma', nn.initializers.ones, (self.num_channels,))
        beta = self.param('beta', nn.initializers.zeros, (self.num_channels,))
        return x_norm * gamma + beta

class Model(nn.Module):
    in_features: int
    out_features: int
    bias_shape: tuple
    num_groups: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features, use_bias=False)(x)
        bias = self.param('bias', nn.initializers.normal(), self.bias_shape)
        x = x + bias
        x = Hardtanh()(x)
        x = Mish()(x)
        x = GroupNorm(num_groups=self.num_groups, num_channels=self.out_features)(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bias_shape = (out_features,)
num_groups = 256

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, bias_shape, num_groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.
    """"""
    def __init__(self, in_features, out_features, add_value_shape):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.add_value = nn.Parameter(torch.randn(add_value_shape)) 

    def forward(self, x):
        x = self.matmul(x)
        x = x + self.add_value
        x = torch.sigmoid(x) * x # Swish
        x = torch.tanh(x)
        x = torch.nn.functional.gelu(x) # GELU
        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
add_value_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, add_value_shape]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, adds a value, applies Swish, Tanh, GELU, and Hardtanh activation functions.
    """"""
    in_features: int
    out_features: int
    add_value_shape: tuple

    def setup(self):
        self.matmul = nn.Dense(self.out_features)
        self.add_value = self.param('add_value', jax.nn.initializers.normal(), self.add_value_shape)

    def __call__(self, x):
        x = self.matmul(x)
        x = x + self.add_value
        x = jax.nn.sigmoid(x) * x  # Swish
        x = jnp.tanh(x)
        x = jax.nn.gelu(x)
        x = jnp.clip(x, -1, 1)  # Hardtanh
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
add_value_shape = (out_features,)

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, add_value_shape]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed 3D convolution, multiplies by a scalar, applies max pooling, 
    global average pooling, and clamps the output.
    """"""
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.scale = scale
        self.maxpool = nn.MaxPool3d(kernel_size=maxpool_kernel_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.clamp_min = 0
        self.clamp_max = 1

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x * self.scale
        x = self.maxpool(x)
        x = self.global_avg_pool(x)
        x = torch.clamp(x, min=self.clamp_min, max=self.clamp_max)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
scale = 0.5
maxpool_kernel_size = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a transposed 3D convolution, multiplies by a scalar, applies max pooling, 
    global average pooling, and clamps the output.
    """"""
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    padding: int
    scale: float
    maxpool_kernel_size: int

    @nn.compact
    def __call__(self, x):
        # ConvTranspose3d in PyTorch: (N, C_in, D, H, W) -> (N, C_out, D_out, H_out, W_out)
        # Flax does not have ConvTranspose3d, so we use ConvTranspose with 3 spatial dims
        x = nn.ConvTranspose(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride, self.stride),
            padding=[(self.padding, self.padding)]*3,
            use_bias=True,
            dimension_numbers=('NCDHW', 'OIDHW', 'NCDHW')
        )(x)
        x = x * self.scale
        # MaxPool3d
        x = nn.max_pool(
            x,
            window_shape=(self.maxpool_kernel_size, self.maxpool_kernel_size, self.maxpool_kernel_size),
            strides=(self.maxpool_kernel_size, self.maxpool_kernel_size, self.maxpool_kernel_size),
            padding='VALID'
        )
        # Global average pooling to (N, C, 1, 1, 1)
        x = jnp.mean(x, axis=(2, 3, 4), keepdims=True)
        x = jnp.clip(x, a_min=0, a_max=1)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
scale = 0.5
maxpool_kernel_size = 2

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, depth, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.
    """"""
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.divide_value = divide_value

    def forward(self, x):
        x = self.matmul(x)
        x = self.bn(x)
        x = x + self.bias
        x = x / self.divide_value
        x = x * torch.sigmoid(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bn_eps = 1e-5
bn_momentum = 0.1
bias_shape = (1,)
divide_value = 1.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, batch normalization, bias addition, division, and Swish activation.
    """"""
    in_features: int
    out_features: int
    bn_eps: float = 1e-5
    bn_momentum: float = 0.1
    bias_shape: tuple = (1,)
    divide_value: float = 1.0

    def setup(self):
        self.matmul = nn.Dense(self.out_features, use_bias=False)
        self.bn = nn.BatchNorm(use_running_average=False, momentum=1.0 - self.bn_momentum, epsilon=self.bn_eps)
        self.bias = self.param('bias', nn.initializers.normal(), self.bias_shape)

    def __call__(self, x, *, train=True):
        x = self.matmul(x)
        x = self.bn(x, use_running_average=not train)
        x = x + self.bias
        x = x / self.divide_value
        x = x * jax.nn.sigmoid(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
bn_eps = 1e-5
bn_momentum = 0.1
bias_shape = (1,)
divide_value = 1.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    A model implementing the pattern ""Matmul_AvgPool_GELU_Scale_Max"".
    """"""
    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):
        super(Model, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.avg_pool = nn.AvgPool1d(kernel_size=pool_kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        """"""
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """"""
        x = self.matmul(x)
        x = self.avg_pool(x.unsqueeze(1)).squeeze(1)
        x = torch.nn.functional.gelu(x)
        x = x * self.scale_factor
        x = torch.max(x, dim=1).values
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
pool_kernel_size = 16
scale_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, pool_kernel_size, scale_factor]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    A model implementing the pattern ""Matmul_AvgPool_GELU_Scale_Max"".
    """"""
    in_features: int
    out_features: int
    pool_kernel_size: int
    scale_factor: float

    @nn.compact
    def __call__(self, x):
        # x: (batch_size, in_features)
        x = nn.Dense(self.out_features)(x)  # (batch_size, out_features)
        # Reshape for 1D pooling: (batch, channels=1, features)
        x = x[:, None, :]  # (batch_size, 1, out_features)
        # AvgPool1d: window_shape=(pool_kernel_size,), strides=(pool_kernel_size,)
        x = nn.avg_pool(
            x,
            window_shape=(1, self.pool_kernel_size),
            strides=(1, self.pool_kernel_size),
            padding='VALID'
        )  # (batch_size, 1, new_features)
        x = jnp.squeeze(x, axis=1)  # (batch_size, new_features)
        x = jax.nn.gelu(x)
        x = x * self.scale_factor
        x = jnp.max(x, axis=1)  # (batch_size,)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
pool_kernel_size = 16
scale_factor = 2.0

def get_inputs():
    return [jnp.random.rand(batch_size, in_features).astype(jnp.float32)]

def get_init_inputs():
    return [in_features, out_features, pool_kernel_size, scale_factor]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies GELU, and then applies Softmax.
    """"""
    def __init__(self, in_features, out_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)

    def forward(self, x):
        x = self.linear(x)
        x = torch.nn.functional.gelu(x)
        x = torch.nn.functional.softmax(x, dim=1)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Simple model that performs a matrix multiplication, applies GELU, and then applies Softmax.
    """"""
    in_features: int
    out_features: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = nn.gelu(x)
        x = nn.softmax(x, axis=1)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, subtraction, multiplication, and ReLU activation.
    """"""
    def __init__(self, in_features, out_features, subtract_value, multiply_value):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.subtract_value = subtract_value
        self.multiply_value = multiply_value

    def forward(self, x):
        x = self.linear(x)
        x = x - self.subtract_value
        x = x * self.multiply_value
        x = torch.relu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
subtract_value = 2.0
multiply_value = 1.5

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, subtract_value, multiply_value]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    """"""
    Model that performs a matrix multiplication, subtraction, multiplication, and ReLU activation.
    """"""
    in_features: int
    out_features: int
    subtract_value: float
    multiply_value: float

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.out_features)(x)
        x = x - self.subtract_value
        x = x * self.multiply_value
        x = jax.nn.relu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
subtract_value = 2.0
multiply_value = 1.5

def get_inputs():
    return [jnp.random.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, subtract_value, multiply_value]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        """"""
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param stride: Stride for the first convolutional layer
        :param downsample: Downsample layer for the shortcut connection
        """"""
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels * expansion, height, width)
        """"""
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

class Model(nn.Module):
    def __init__(self, layers, num_classes=1000):
        """"""
        :param block: Type of block to use (BasicBlock or Bottleneck)
        :param layers: List of integers specifying the number of blocks in each layer
        :param num_classes: Number of output classes
        """"""
        super(Model, self).__init__()
        self.in_channels = 64

        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(self.in_channels)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        block = Bottleneck

        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * block.expansion),
            )

        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, 3, height, width)
        :return: Output tensor, shape (batch_size, num_classes)
        """"""
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x

# Test code
batch_size = 10
height = 224
width = 224
layers = [3, 4, 23, 3]
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, height, width)]

def get_init_inputs():
    return [layers, num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Bottleneck(nn.Module):
    in_channels: int
    out_channels: int
    stride: int = 1
    downsample: nn.Module = None

    expansion: int = 4

    @nn.compact
    def __call__(self, x, train=True):
        identity = x

        out = nn.Conv(features=self.out_channels, kernel_size=(1,1), strides=(1,1), use_bias=False)(x)
        out = nn.BatchNorm(use_running_average=not train)(out)
        out = nn.relu(out)

        out = nn.Conv(features=self.out_channels, kernel_size=(3,3), strides=(self.stride, self.stride), padding='SAME', use_bias=False)(out)
        out = nn.BatchNorm(use_running_average=not train)(out)
        out = nn.relu(out)

        out = nn.Conv(features=self.out_channels * self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)(out)
        out = nn.BatchNorm(use_running_average=not train)(out)

        if self.downsample is not None:
            identity = self.downsample(identity, train=train)

        out = out + identity
        out = nn.relu(out)
        return out

class DownsampleLayer(nn.Module):
    in_channels: int
    out_channels: int
    stride: int

    @nn.compact
    def __call__(self, x, train=True):
        x = nn.Conv(features=self.out_channels, kernel_size=(1,1), strides=(self.stride, self.stride), use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        return x

class Sequential(nn.Module):
    layers: list

    def setup(self):
        self.layers_ = self.layers

    def __call__(self, x, train=True):
        for layer in self.layers_:
            x = layer(x, train=train)
        return x

class Model(nn.Module):
    layers_cfg: list
    num_classes: int = 1000

    def setup(self):
        self.in_channels = 64
        self.conv1 = nn.Conv(features=self.in_channels, kernel_size=(7,7), strides=(2,2), padding='SAME', use_bias=False)
        self.bn1 = nn.BatchNorm()
        self.maxpool = nn.max_pool
        self.relu = nn.relu

        self.layer1 = self._make_layer(64, self.layers_cfg[0])
        self.layer2 = self._make_layer(128, self.layers_cfg[1], stride=2)
        self.layer3 = self._make_layer(256, self.layers_cfg[2], stride=2)
        self.layer4 = self._make_layer(512, self.layers_cfg[3], stride=2)

        self.avgpool = lambda x: jnp.mean(x, axis=(2,3), keepdims=True)
        self.fc = nn.Dense(features=self.num_classes)

    def _make_layer(self, out_channels, blocks, stride=1):
        layers = []
        expansion = Bottleneck.expansion
        downsample = None
        if stride != 1 or self.in_channels != out_channels * expansion:
            downsample = DownsampleLayer(self.in_channels, out_channels * expansion, stride)
        layers.append(Bottleneck(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * expansion
        for _ in range(1, blocks):
            layers.append(Bottleneck(self.in_channels, out_channels))
        return Sequential(layers)

    def __call__(self, x, train=True):
        x = self.conv1(x)
        x = self.bn1(x, use_running_average=not train)
        x = self.relu(x)
        x = self.maxpool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        x = self.layer1(x, train=train)
        x = self.layer2(x, train=train)
        x = self.layer3(x, train=train)
        x = self.layer4(x, train=train)

        x = self.avgpool(x)
        x = x.reshape((x.shape[0], -1))
        x = self.fc(x)
        return x

# Test code
batch_size = 10
height = 224
width = 224
layers = [3, 4, 23, 3]
num_classes = 1000

def get_inputs():
    return [jnp.ones((batch_size, 3, height, width), dtype=jnp.float32)]

def get_init_inputs():
    return [layers, num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        Initialize the VGG16 model.
        
        :param num_classes: The number of output classes (default is 1000 for ImageNet)
        """"""
        super(Model, self).__init__()
        
        # VGG16 architecture: 5 blocks of convolutional layers followed by max pooling
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 5
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        # Fully connected layers
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.0),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.0),
            nn.Linear(4096, num_classes)
        )
    
    def forward(self, x):
        """"""
        Forward pass of the VGG16 model.
        
        :param x: The input tensor, shape (batch_size, 3, 224, 224)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, 224, 224)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, *, train: bool = False):
        # Block 1
        x = nn.Conv(features=64, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=64, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')

        # Block 2
        x = nn.Conv(features=128, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=128, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')

        # Block 3
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')

        # Block 4
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')

        # Block 5
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')

        # Flatten
        x = x.reshape((x.shape[0], -1))

        # Classifier
        x = nn.Dense(features=4096)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.0)(x, deterministic=not train)
        x = nn.Dense(features=4096)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.0)(x, deterministic=not train)
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    # JAX expects NHWC by default, but Flax Conv2D supports NCHW if input is NCHW.
    # We'll keep NCHW to match PyTorch.
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, 3, 224, 224))
    return [x]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        Initialize the VGG19 model.

        :param num_classes: The number of output classes (default is 1000 for ImageNet)
        """"""
        super(Model, self).__init__()
        
        # VGG19 architecture: 16 Conv layers + 5 MaxPool layers + 3 Fully Connected layers
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 5
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.0),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.0),
            nn.Linear(4096, num_classes)
        )
    
    def forward(self, x):
        """"""
        Forward pass of the VGG19 model.

        :param x: The input tensor, shape (batch_size, 3, 224, 224)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, 224, 224)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class VGG19Features(nn.Module):
    @nn.compact
    def __call__(self, x):
        # Block 1
        x = nn.Conv(features=64, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=64, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        # Block 2
        x = nn.Conv(features=128, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=128, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        # Block 3
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=256, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        # Block 4
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        # Block 5
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(features=512, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        return x

class VGG19Classifier(nn.Module):
    num_classes: int

    @nn.compact
    def __call__(self, x):
        x = x.reshape((x.shape[0], -1))  # flatten
        x = nn.Dense(features=4096)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.0, deterministic=True)(x)
        x = nn.Dense(features=4096)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.0, deterministic=True)(x)
        x = nn.Dense(features=self.num_classes)(x)
        return x

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, *, train=False):
        # Input x: (batch, 3, 224, 224)
        # Flax expects (batch, H, W, C)
        if x.shape[1] == 3:
            x = jnp.transpose(x, (0,2,3,1))
        x = VGG19Features()(x)
        # x: (batch, 7, 7, 512)
        x = jnp.transpose(x, (0,3,1,2))  # (batch, 512, 7, 7)
        x = x.reshape((x.shape[0], -1))  # flatten
        x = VGG19Classifier(self.num_classes)(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    # Return a list with a single input array of shape (batch_size, 3, 224, 224)
    return [jnp.random.rand(batch_size, 3, 224, 224).astype(jnp.float32)]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_input_features: int, num_output_features: int):
        """"""
        :param num_input_features: The number of input feature maps
        :param num_output_features: The number of output feature maps
        """"""
        super(Model, self).__init__()
        self.transition = nn.Sequential(
            nn.BatchNorm2d(num_input_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )

    def forward(self, x):
        """"""
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Downsampled tensor with reduced number of feature maps
        """"""
        return self.transition(x)

batch_size = 128
num_input_features = 32
num_output_features = 64
height, width = 256, 256

def get_inputs():
    return [torch.rand(batch_size, num_input_features, height, width)]

def get_init_inputs():
    return [num_input_features, num_output_features]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    num_input_features: int
    num_output_features: int

    @nn.compact
    def __call__(self, x):
        # BatchNorm over channel axis (axis=1 for NCHW)
        x = nn.BatchNorm(use_running_average=False, axis=1, momentum=0.1, epsilon=1e-5)(x)
        x = nn.relu(x)
        x = nn.Conv(
            features=self.num_output_features,
            kernel_size=(1, 1),
            strides=(1, 1),
            use_bias=False
        )(x)
        # AvgPool2d with kernel_size=2, stride=2, NCHW
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID', data_format='NCHW')
        return x

batch_size = 128
num_input_features = 32
num_output_features = 64
height, width = 256, 256

def get_inputs():
    # Return a list with a single input array, shape (batch, channels, height, width)
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, num_input_features, height, width),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [num_input_features, num_output_features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):
        """"""
        :param num_layers: The number of layers in the dense block
        :param num_input_features: The number of input feature maps
        :param growth_rate: The growth rate for the dense block (new features added per layer)
        """"""
        super(Model, self).__init__()
        layers = []
        for i in range(num_layers):
            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))
        self.layers = nn.ModuleList(layers)

    def _make_layer(self, in_features: int, growth_rate: int):
        """"""
        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.
        """"""
        return nn.Sequential(
            nn.BatchNorm2d(in_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),
            nn.Dropout(0.0)
        )
    
    def forward(self, x):
        """"""
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)
        """"""
        features = [x]
        for layer in self.layers:
            new_feature = layer(x)
            features.append(new_feature)
            x = torch.cat(features, 1)  # Concatenate along channel axis
        return x
    
batch_size = 10
num_layers = 6
num_input_features = 32
growth_rate = 32
height, width = 224, 224

def get_inputs():
    return [torch.rand(batch_size, num_input_features, height, width)]

def get_init_inputs():
    return [num_layers, num_input_features , growth_rate]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import List

class Model(nn.Module):
    num_layers: int
    num_input_features: int
    growth_rate: int

    @nn.compact
    def __call__(self, x):
        features = [x]
        for i in range(self.num_layers):
            in_features = self.num_input_features + i * self.growth_rate
            new_feature = self._make_layer(in_features, self.growth_rate)(x)
            features.append(new_feature)
            x = jnp.concatenate(features, axis=1)
        return x

    def _make_layer(self, in_features, growth_rate):
        return nn.Sequential([
            nn.BatchNorm(use_running_average=False, axis=1),
            nn.relu,
            nn.Conv(features=growth_rate, kernel_size=(3, 3), padding='SAME', use_bias=False),
            nn.Dropout(rate=0.0, deterministic=True)
        ])

batch_size = 10
num_layers = 6
num_input_features = 32
growth_rate = 32
height, width = 224, 224

def get_inputs():
    return [jnp.random.rand(batch_size, num_input_features, height, width)]

def get_init_inputs():
    return [num_layers, num_input_features, growth_rate]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class DenseBlock(nn.Module):
    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):
        """"""
        :param num_layers: The number of layers in the dense block
        :param num_input_features: The number of input feature maps
        :param growth_rate: The growth rate for the dense block (new features added per layer)
        """"""
        super(DenseBlock, self).__init__()
        layers = []
        for i in range(num_layers):
            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))
        self.layers = nn.ModuleList(layers)

    def _make_layer(self, in_features: int, growth_rate: int):
        """"""
        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.
        """"""
        return nn.Sequential(
            nn.BatchNorm2d(in_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),
            nn.Dropout(0.0)
        )

    def forward(self, x):
        """"""
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)
        """"""
        features = [x]
        for layer in self.layers:
            new_feature = layer(x)
            features.append(new_feature)
            x = torch.cat(features, 1)  # Concatenate along channel axis
        return x

class TransitionLayer(nn.Module):
    def __init__(self, num_input_features: int, num_output_features: int):
        """"""
        :param num_input_features: The number of input feature maps
        :param num_output_features: The number of output feature maps
        """"""
        super(TransitionLayer, self).__init__()
        self.transition = nn.Sequential(
            nn.BatchNorm2d(num_input_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )

    def forward(self, x):
        """"""
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Downsampled tensor with reduced number of feature maps
        """"""
        return self.transition(x)

class Model(nn.Module):
    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):
        """"""
        :param growth_rate: The growth rate of the DenseNet (new features added per layer)
        :param num_classes: The number of output classes for classification
        """"""
        super(Model, self).__init__()

        # Initial convolution and pooling
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )

        # Each dense block is followed by a transition layer, except the last one
        num_features = 64
        block_layers = [6, 12, 24, 16]  # Corresponding layers in DenseNet121

        self.dense_blocks = nn.ModuleList()
        self.transition_layers = nn.ModuleList()

        for i, num_layers in enumerate(block_layers):
            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)
            self.dense_blocks.append(block)
            num_features = num_features + num_layers * growth_rate

            if i != len(block_layers) - 1:
                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)
                self.transition_layers.append(transition)
                num_features = num_features // 2

        # Final batch norm and classifier
        self.final_bn = nn.BatchNorm2d(num_features)
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        :param x: Input tensor of shape (batch_size, 3, height, width)
        :return: Output tensor of shape (batch_size, num_classes)
        """"""
        x = self.features(x)

        for i, block in enumerate(self.dense_blocks):
            x = block(x)
            if i != len(self.dense_blocks) - 1:
                x = self.transition_layers[i](x)

        x = self.final_bn(x)
        x = F.relu(x, inplace=True)
        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Testing the DenseNet121 model
batch_size = 10
num_classes = 10
height, width = 224, 224  # Standard input size for DenseNet

def get_inputs():
    return [torch.rand(batch_size, 3, height, width)]

def get_init_inputs():
    return [32, num_classes]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class DenseBlock(nn.Module):
    num_layers: int
    num_input_features: int
    growth_rate: int

    @nn.compact
    def __call__(self, x):
        features = [x]
        for i in range(self.num_layers):
            in_features = self.num_input_features + i * self.growth_rate
            # BatchNorm + ReLU + Conv2D + Dropout
            y = nn.BatchNorm(use_running_average=False, axis=1)(x)
            y = nn.relu(y)
            y = nn.Conv(self.growth_rate, kernel_size=(3,3), padding='SAME', use_bias=False, feature_group_count=1)(y)
            y = nn.Dropout(0.0, deterministic=True)(y)
            features.append(y)
            x = jnp.concatenate(features, axis=1)
        return x

class TransitionLayer(nn.Module):
    num_input_features: int
    num_output_features: int

    @nn.compact
    def __call__(self, x):
        x = nn.BatchNorm(use_running_average=False, axis=1)(x)
        x = nn.relu(x)
        x = nn.Conv(self.num_output_features, kernel_size=(1,1), use_bias=False)(x)
        # AvgPool2d(kernel_size=2, stride=2)
        x = nn.avg_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        return x

class Model(nn.Module):
    growth_rate: int = 32
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, train: bool = False):
        # Initial convolution and pooling
        x = nn.Conv(64, kernel_size=(7,7), strides=(2,2), padding='SAME', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=not train, axis=1)(x)
        x = nn.relu(x)
        # MaxPool2d(kernel_size=3, stride=2, padding=1)
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        num_features = 64
        block_layers = [6, 12, 24, 16]  # DenseNet121

        dense_blocks = []
        transition_layers = []
        for i, num_layers in enumerate(block_layers):
            dense_blocks.append(
                DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=self.growth_rate)
            )
            num_features = num_features + num_layers * self.growth_rate
            if i != len(block_layers) - 1:
                transition_layers.append(
                    TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)
                )
                num_features = num_features // 2

        # Sequentially apply dense blocks and transition layers
        for i, block in enumerate(dense_blocks):
            x = block(x)
            if i != len(dense_blocks) - 1:
                x = transition_layers[i](x)

        x = nn.BatchNorm(use_running_average=not train, axis=1)(x)
        x = nn.relu(x)
        # Adaptive avg pool to (1,1)
        x = jnp.mean(x, axis=(2,3), keepdims=True)
        x = x.reshape((x.shape[0], -1))
        x = nn.Dense(self.num_classes)(x)
        return x

# Testing the DenseNet121 model
batch_size = 10
num_classes = 10
height, width = 224, 224  # Standard input size for DenseNet

def get_inputs():
    return [jnp.random.rand(batch_size, 3, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [32, num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class DenseBlock(nn.Module):
    def __init__(self, num_layers: int, num_input_features: int, growth_rate: int):
        """"""
        :param num_layers: The number of layers in the dense block
        :param num_input_features: The number of input feature maps
        :param growth_rate: The growth rate for the dense block (new features added per layer)
        """"""
        super(DenseBlock, self).__init__()
        layers = []
        for i in range(num_layers):
            layers.append(self._make_layer(num_input_features + i * growth_rate, growth_rate))
        self.layers = nn.ModuleList(layers)

    def _make_layer(self, in_features: int, growth_rate: int):
        """"""
        Creates a single layer with BatchNorm, ReLU, Conv2D, and Dropout.
        """"""
        return nn.Sequential(
            nn.BatchNorm2d(in_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_features, growth_rate, kernel_size=3, padding=1, bias=False),
            nn.Dropout(0.0)
        )

    def forward(self, x):
        """"""
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Concatenated output tensor with shape (batch_size, num_output_features, height, width)
        """"""
        features = [x]
        for layer in self.layers:
            new_feature = layer(x)
            features.append(new_feature)
            x = torch.cat(features, 1)  # Concatenate along channel axis
        return x

class TransitionLayer(nn.Module):
    def __init__(self, num_input_features: int, num_output_features: int):
        """"""
        :param num_input_features: The number of input feature maps
        :param num_output_features: The number of output feature maps
        """"""
        super(TransitionLayer, self).__init__()
        self.transition = nn.Sequential(
            nn.BatchNorm2d(num_input_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )

    def forward(self, x):
        """"""
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Downsampled tensor with reduced number of feature maps
        """"""
        return self.transition(x)

class Model(nn.Module):
    def __init__(self, growth_rate: int = 32, num_classes: int = 1000):
        """"""
        :param growth_rate: The growth rate of the DenseNet (new features added per layer)
        :param num_classes: The number of output classes for classification
        """"""
        super(Model, self).__init__()

        # Initial convolution and pooling
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )

        # Each dense block is followed by a transition layer, except the last one
        num_features = 64
        block_layers = [6, 12, 48, 32]  # Corresponding layers in DenseNet201

        self.dense_blocks = nn.ModuleList()
        self.transition_layers = nn.ModuleList()

        for i, num_layers in enumerate(block_layers):
            block = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=growth_rate)
            self.dense_blocks.append(block)
            num_features = num_features + num_layers * growth_rate

            if i != len(block_layers) - 1:
                transition = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)
                self.transition_layers.append(transition)
                num_features = num_features // 2

        # Final batch norm and classifier
        self.final_bn = nn.BatchNorm2d(num_features)
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """"""
        :param x: Input tensor of shape (batch_size, 3, height, width)
        :return: Output tensor of shape (batch_size, num_classes)
        """"""
        x = self.features(x)

        for i, block in enumerate(self.dense_blocks):
            x = block(x)
            if i != len(self.dense_blocks) - 1:
                x = self.transition_layers[i](x)

        x = self.final_bn(x)
        x = F.relu(x, inplace=True)
        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Testing the DenseNet201 model
batch_size = 10
num_classes = 10
height, width = 224, 224  # Standard input size for DenseNet

def get_inputs():
    return [torch.rand(batch_size, 3, height, width)]

def get_init_inputs():
    return [32, num_classes]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class DenseBlock(nn.Module):
    num_layers: int
    num_input_features: int
    growth_rate: int

    @nn.compact
    def __call__(self, x):
        features = [x]
        for i in range(self.num_layers):
            in_features = self.num_input_features + i * self.growth_rate
            new_feature = self._make_layer(in_features, self.growth_rate)(x)
            features.append(new_feature)
            x = jnp.concatenate(features, axis=1)
        return x

    def _make_layer(self, in_features, growth_rate):
        return nn.Sequential([
            nn.BatchNorm(use_running_average=False, axis=1),
            nn.relu,
            nn.Conv(features=growth_rate, kernel_size=(3,3), padding='SAME', use_bias=False),
            nn.Dropout(rate=0.0, deterministic=True)
        ])

class TransitionLayer(nn.Module):
    num_input_features: int
    num_output_features: int

    @nn.compact
    def __call__(self, x):
        x = nn.BatchNorm(use_running_average=False, axis=1)(x)
        x = nn.relu(x)
        x = nn.Conv(features=self.num_output_features, kernel_size=(1,1), use_bias=False)(x)
        x = nn.avg_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        return x

class Model(nn.Module):
    growth_rate: int = 32
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x):
        # Initial convolution and pooling
        x = nn.Conv(features=64, kernel_size=(7,7), strides=(2,2), padding='SAME', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False, axis=1)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        num_features = 64
        block_layers = [6, 12, 48, 32]
        dense_blocks = []
        transition_layers = []

        for i, num_layers in enumerate(block_layers):
            x = DenseBlock(num_layers=num_layers, num_input_features=num_features, growth_rate=self.growth_rate)(x)
            num_features = num_features + num_layers * self.growth_rate
            if i != len(block_layers) - 1:
                x = TransitionLayer(num_input_features=num_features, num_output_features=num_features // 2)(x)
                num_features = num_features // 2

        x = nn.BatchNorm(use_running_average=False, axis=1)(x)
        x = nn.relu(x)
        # Adaptive avg pool to (1,1)
        x = jnp.mean(x, axis=(2,3), keepdims=True)
        x = x.reshape((x.shape[0], -1))
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Testing the DenseNet201 model
batch_size = 10
num_classes = 10
height, width = 224, 224

def get_inputs():
    return [jnp.random.rand(batch_size, 3, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [32, num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class FireModule(nn.Module):
    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):
        """"""
        :param in_channels: Number of input channels
        :param squeeze_channels: Number of output channels for the squeeze layer
        :param expand1x1_channels: Number of output channels for the 1x1 expand layer
        :param expand3x3_channels: Number of output channels for the 3x3 expand layer
        """"""
        super(FireModule, self).__init__()
        
        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)
        self.squeeze_activation = nn.ReLU(inplace=True)
        
        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)
        self.expand1x1_activation = nn.ReLU(inplace=True)
        
        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)
        self.expand3x3_activation = nn.ReLU(inplace=True)
    
    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, expand1x1_channels + expand3x3_channels, height, width)
        """"""
        x = self.squeeze_activation(self.squeeze(x))
        return torch.cat([
            self.expand1x1_activation(self.expand1x1(x)),
            self.expand3x3_activation(self.expand3x3(x))
        ], 1)

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        :param num_classes: Number of output classes
        """"""
        super(Model, self).__init__()
        
        self.features = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=7, stride=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
            FireModule(96, 16, 64, 64),
            FireModule(128, 16, 64, 64),
            FireModule(128, 32, 128, 128),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
            FireModule(256, 32, 128, 128),
            FireModule(256, 48, 192, 192),
            FireModule(384, 48, 192, 192),
            FireModule(384, 64, 256, 256),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
            FireModule(512, 64, 256, 256),
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.0),
            nn.Conv2d(512, num_classes, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
    
    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, 3, height, width)
        :return: Output tensor, shape (batch_size, num_classes)
        """"""
        x = self.features(x)
        x = self.classifier(x)
        return torch.flatten(x, 1)

# Test code
batch_size = 64
input_channels = 3
height = 512
width = 512
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, input_channels, height, width)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class FireModule(nn.Module):
    in_channels: int
    squeeze_channels: int
    expand1x1_channels: int
    expand3x3_channels: int

    @nn.compact
    def __call__(self, x):
        # Squeeze layer
        x = nn.Conv(self.squeeze_channels, kernel_size=(1,1), strides=(1,1), use_bias=True)(x)
        x = nn.relu(x)
        # Expand 1x1
        x1 = nn.Conv(self.expand1x1_channels, kernel_size=(1,1), strides=(1,1), use_bias=True)(x)
        x1 = nn.relu(x1)
        # Expand 3x3
        x3 = nn.Conv(self.expand3x3_channels, kernel_size=(3,3), strides=(1,1), padding='SAME', use_bias=True)(x)
        x3 = nn.relu(x3)
        # Concatenate along channel axis
        return jnp.concatenate([x1, x3], axis=1)

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, train: bool = False):
        # Input shape: (batch, channels, height, width)
        # Flax expects (batch, height, width, channels), so we need to transpose
        x = jnp.transpose(x, (0,2,3,1))  # (N, H, W, C)

        # Conv1
        x = nn.Conv(96, kernel_size=(7,7), strides=(2,2), use_bias=True)(x)
        x = nn.relu(x)
        # MaxPool1
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')
        # Fire modules
        x = FireModule(96, 16, 64, 64)(x)
        x = FireModule(128, 16, 64, 64)(x)
        x = FireModule(128, 32, 128, 128)(x)
        # MaxPool2
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')
        x = FireModule(256, 32, 128, 128)(x)
        x = FireModule(256, 48, 192, 192)(x)
        x = FireModule(384, 48, 192, 192)(x)
        x = FireModule(384, 64, 256, 256)(x)
        # MaxPool3
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')
        x = FireModule(512, 64, 256, 256)(x)
        # Classifier
        x = nn.Dropout(rate=0.0, deterministic=not train)(x)
        x = nn.Conv(self.num_classes, kernel_size=(1,1), strides=(1,1), use_bias=True)(x)
        x = nn.relu(x)
        # AdaptiveAvgPool2d((1,1)) equivalent: mean over H and W
        x = jnp.mean(x, axis=(1,2), keepdims=True)
        # Flatten
        x = x.reshape((x.shape[0], -1))
        return x

# Test code
batch_size = 64
input_channels = 3
height = 512
width = 512
num_classes = 1000

def get_inputs():
    # Return a list with a single input array, shape (batch, channels, height, width)
    return [jax.random.uniform(jax.random.PRNGKey(0), (batch_size, input_channels, height, width))]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):
        """"""
        MobileNetV1 architecture implementation.

        :param num_classes: The number of output classes (default: 1000)
        :param input_channels: The number of input channels (default: 3 for RGB images)
        :param alpha: Width multiplier (default: 1.0)
        """"""
        super(Model, self).__init__()
        
        def conv_bn(inp, oup, stride):
            return nn.Sequential(
                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
                nn.BatchNorm2d(oup),
                nn.ReLU(inplace=True)
            )
        
        def conv_dw(inp, oup, stride):
            return nn.Sequential(
                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),
                nn.BatchNorm2d(inp),
                nn.ReLU(inplace=True),
                
                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
                nn.ReLU(inplace=True),
            )
        
        self.model = nn.Sequential(
            conv_bn(input_channels, int(32 * alpha), 2),
            conv_dw(int(32 * alpha), int(64 * alpha), 1),
            conv_dw(int(64 * alpha), int(128 * alpha), 2),
            conv_dw(int(128 * alpha), int(128 * alpha), 1),
            conv_dw(int(128 * alpha), int(256 * alpha), 2),
            conv_dw(int(256 * alpha), int(256 * alpha), 1),
            conv_dw(int(256 * alpha), int(512 * alpha), 2),
            conv_dw(int(512 * alpha), int(512 * alpha), 1),
            conv_dw(int(512 * alpha), int(512 * alpha), 1),
            conv_dw(int(512 * alpha), int(512 * alpha), 1),
            conv_dw(int(512 * alpha), int(512 * alpha), 1),
            conv_dw(int(512 * alpha), int(512 * alpha), 1),
            conv_dw(int(512 * alpha), int(1024 * alpha), 2),
            conv_dw(int(1024 * alpha), int(1024 * alpha), 1),
            nn.AvgPool2d(7),
        )
        self.fc = nn.Linear(int(1024 * alpha), num_classes)
    
    def forward(self, x):
        """"""
        :param x: The input tensor, shape (batch_size, input_channels, height, width)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = self.model(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000
alpha = 1.0

def get_inputs():
    return [torch.rand(batch_size, input_channels, height, width)]

def get_init_inputs():
    return [num_classes, input_channels, alpha]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class ConvBN(nn.Module):
    inp: int
    oup: int
    stride: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(features=self.oup, kernel_size=(3,3), strides=(self.stride, self.stride), padding='SAME', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False, momentum=0.9, epsilon=1e-5)(x)
        x = nn.relu(x)
        return x

class ConvDW(nn.Module):
    inp: int
    oup: int
    stride: int

    @nn.compact
    def __call__(self, x):
        # Depthwise conv
        x = nn.Conv(features=self.inp, kernel_size=(3,3), strides=(self.stride, self.stride), padding='SAME', feature_group_count=self.inp, use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False, momentum=0.9, epsilon=1e-5)(x)
        x = nn.relu(x)
        # Pointwise conv
        x = nn.Conv(features=self.oup, kernel_size=(1,1), strides=(1,1), padding='VALID', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False, momentum=0.9, epsilon=1e-5)(x)
        x = nn.relu(x)
        return x

class Model(nn.Module):
    num_classes: int = 1000
    input_channels: int = 3
    alpha: float = 1.0

    @nn.compact
    def __call__(self, x, train=True):
        def ch(v):
            return int(v * self.alpha)

        x = ConvBN(self.input_channels, ch(32), 2)(x)
        x = ConvDW(ch(32), ch(64), 1)(x)
        x = ConvDW(ch(64), ch(128), 2)(x)
        x = ConvDW(ch(128), ch(128), 1)(x)
        x = ConvDW(ch(128), ch(256), 2)(x)
        x = ConvDW(ch(256), ch(256), 1)(x)
        x = ConvDW(ch(256), ch(512), 2)(x)
        x = ConvDW(ch(512), ch(512), 1)(x)
        x = ConvDW(ch(512), ch(512), 1)(x)
        x = ConvDW(ch(512), ch(512), 1)(x)
        x = ConvDW(ch(512), ch(512), 1)(x)
        x = ConvDW(ch(512), ch(512), 1)(x)
        x = ConvDW(ch(512), ch(1024), 2)(x)
        x = ConvDW(ch(1024), ch(1024), 1)(x)
        # AvgPool2d(7) in PyTorch is global average pooling for 224x224 input
        # After all the strides, the feature map is 7x7, so pool over all
        x = jnp.mean(x, axis=(2,3), keepdims=False)
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000
alpha = 1.0

def get_inputs():
    # Flax expects NHWC by default, but we keep NCHW for direct mapping
    # We'll transpose to NHWC for Flax
    x = jnp.random.rand(batch_size, input_channels, height, width).astype(jnp.float32)
    x = jnp.transpose(x, (0,2,3,1))  # NCHW -> NHWC
    return [x]

def get_init_inputs():
    return [num_classes, input_channels, alpha]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, layer_sizes, output_size):
        """"""
        :param input_size: The number of input features
        :param layer_sizes: A list of ints containing the sizes of each hidden layer
        :param output_size: The number of output features
        """"""
        super(Model, self).__init__()
        
        layers = []
        current_input_size = input_size
        
        for layer_size in layer_sizes:
            layers.append(nn.Linear(current_input_size, layer_size))
            layers.append(nn.ReLU())
            current_input_size = layer_size
        
        layers.append(nn.Linear(current_input_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        """"""
        :param x: The input tensor, shape (batch_size, input_size)
        :return: The output tensor, shape (batch_size, output_size)
        """"""
        return self.network(x)

# Test code
batch_size = 128
input_size = 16384
layer_sizes = [16384, 16384]
output_size = 8192

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, layer_sizes, output_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    layer_sizes: list
    output_size: int

    @nn.compact
    def __call__(self, x):
        current_input_size = self.input_size
        for layer_size in self.layer_sizes:
            x = nn.Dense(features=layer_size)(x)
            x = nn.relu(x)
            current_input_size = layer_size
        x = nn.Dense(features=self.output_size)(x)
        return x

# Test code
batch_size = 128
input_size = 16384
layer_sizes = [16384, 16384]
output_size = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, layer_sizes, output_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        MobileNetV2 architecture implementation in PyTorch.

        :param num_classes: The number of output classes. Default is 1000.
        """"""
        super(Model, self).__init__()
        
        def _make_divisible(v, divisor, min_value=None):
            """"""
            This function ensures that the number of channels is divisible by the divisor.
            """"""
            if min_value is None:
                min_value = divisor
            new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
            # Make sure that round down does not go down by more than 10%.
            if new_v < 0.9 * v:
                new_v += divisor
            return new_v

        def _inverted_residual_block(inp, oup, stride, expand_ratio):
            """"""
            Inverted Residual Block for MobileNetV2.
            """"""
            hidden_dim = int(inp * expand_ratio)
            use_res_connect = stride == 1 and inp == oup

            layers = []
            if expand_ratio != 1:
                # Pointwise convolution
                layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))
                layers.append(nn.BatchNorm2d(hidden_dim))
                layers.append(nn.ReLU6(inplace=True))

            layers.extend([
                # Depthwise convolution
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # Pointwise linear convolution
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
            ])

            if use_res_connect:
                return nn.Sequential(*layers), True
            else:
                return nn.Sequential(*layers), False

        # MobileNetV2 architecture
        input_channel = 32
        last_channel = 1280
        inverted_residual_setting = [
            # t, c, n, s
            [1, 16, 1, 1],
            [6, 24, 2, 2],
            [6, 32, 3, 2],
            [6, 64, 4, 2],
            [6, 96, 3, 1],
            [6, 160, 3, 2],
            [6, 320, 1, 1],
        ]

        # Building first layer
        features = [nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),
                    nn.BatchNorm2d(input_channel),
                    nn.ReLU6(inplace=True)]

        # Building inverted residual blocks
        for t, c, n, s in inverted_residual_setting:
            output_channel = _make_divisible(c, 8)
            for i in range(n):
                stride = s if i == 0 else 1
                features.append(_inverted_residual_block(input_channel, output_channel, stride, expand_ratio=t)[0])
                input_channel = output_channel

        # Building last several layers
        features.append(nn.Conv2d(input_channel, last_channel, 1, 1, 0, bias=False))
        features.append(nn.BatchNorm2d(last_channel))
        features.append(nn.ReLU6(inplace=True))

        # Final layer
        features.append(nn.AdaptiveAvgPool2d((1, 1)))

        self.features = nn.Sequential(*features)

        # Linear layer
        self.classifier = nn.Sequential(
            nn.Dropout(0.0),
            nn.Linear(last_channel, num_classes),
        )

        # Weight initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        """"""
        Forward pass of the MobileNetV2 model.

        :param x: The input tensor, shape (batch_size, 3, 224, 224)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, 224, 224)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Any, Callable, Sequence

def _make_divisible(v, divisor, min_value=None):
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v

class ReLU6(nn.Module):
    @nn.compact
    def __call__(self, x):
        return jnp.clip(x, 0, 6)

class ConvBNReLU(nn.Module):
    out_channels: int
    kernel_size: int
    stride: int
    groups: int = 1

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding='SAME',
            use_bias=False,
            feature_group_count=self.groups,
        )(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = ReLU6()(x)
        return x

class InvertedResidual(nn.Module):
    inp: int
    oup: int
    stride: int
    expand_ratio: int

    @nn.compact
    def __call__(self, x):
        hidden_dim = int(self.inp * self.expand_ratio)
        use_res_connect = self.stride == 1 and self.inp == self.oup

        out = x
        if self.expand_ratio != 1:
            out = ConvBNReLU(
                out_channels=hidden_dim,
                kernel_size=1,
                stride=1
            )(out)
        # Depthwise
        out = ConvBNReLU(
            out_channels=hidden_dim,
            kernel_size=3,
            stride=self.stride,
            groups=hidden_dim
        )(out)
        # Pointwise linear
        out = nn.Conv(
            features=self.oup,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            use_bias=False
        )(out)
        out = nn.BatchNorm(use_running_average=False)(out)
        if use_res_connect:
            return x + out
        else:
            return out

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x):
        input_channel = 32
        last_channel = 1280
        inverted_residual_setting = [
            # t, c, n, s
            [1, 16, 1, 1],
            [6, 24, 2, 2],
            [6, 32, 3, 2],
            [6, 64, 4, 2],
            [6, 96, 3, 1],
            [6, 160, 3, 2],
            [6, 320, 1, 1],
        ]

        # First layer
        x = nn.Conv(
            features=input_channel,
            kernel_size=(3, 3),
            strides=(2, 2),
            padding='SAME',
            use_bias=False
        )(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = ReLU6()(x)

        # Inverted residual blocks
        in_c = input_channel
        for t, c, n, s in inverted_residual_setting:
            out_c = _make_divisible(c, 8)
            for i in range(n):
                stride = s if i == 0 else 1
                x = InvertedResidual(
                    inp=in_c,
                    oup=out_c,
                    stride=stride,
                    expand_ratio=t
                )(x)
                in_c = out_c

        # Last several layers
        x = nn.Conv(
            features=last_channel,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            use_bias=False
        )(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = ReLU6()(x)

        # Global average pooling
        x = jnp.mean(x, axis=(1, 2), keepdims=True)
        x = x.reshape((x.shape[0], -1))

        # Classifier
        x = nn.Dropout(rate=0.0, deterministic=False)(x)
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    return [jnp.ones((batch_size, 224, 224, 3), dtype=jnp.float32)]

def get_init_inputs():
    return [num_classes]

# Example usage:
# model = Model(num_classes=num_classes)
# rng = jax.random.PRNGKey(0)
# variables = model.init(rng, jnp.ones((batch_size, 224, 224, 3)))
# out = model.apply(variables, jnp.ones((batch_size, 224, 224, 3)))"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        EfficientNetB0 architecture implementation in PyTorch.

        :param num_classes: The number of output classes (default is 1000 for ImageNet).
        """"""
        super(Model, self).__init__()
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        # MBConv blocks
        self.blocks = nn.Sequential(
            # MBConv1 (32, 16, 1, 1)
            MBConv(32, 16, kernel_size=3, stride=1, expand_ratio=1),
            # MBConv6 (16, 24, 2, 6)
            MBConv(16, 24, kernel_size=3, stride=2, expand_ratio=6),
            # MBConv6 (24, 24, 1, 6)
            MBConv(24, 24, kernel_size=3, stride=1, expand_ratio=6),
            # MBConv6 (24, 40, 2, 6)
            MBConv(24, 40, kernel_size=5, stride=2, expand_ratio=6),
            # MBConv6 (40, 40, 1, 6)
            MBConv(40, 40, kernel_size=5, stride=1, expand_ratio=6),
            # MBConv6 (40, 80, 2, 6)
            MBConv(40, 80, kernel_size=3, stride=2, expand_ratio=6),
            # MBConv6 (80, 80, 1, 6)
            MBConv(80, 80, kernel_size=3, stride=1, expand_ratio=6),
            # MBConv6 (80, 112, 1, 6)
            MBConv(80, 112, kernel_size=5, stride=1, expand_ratio=6),
            # MBConv6 (112, 112, 1, 6)
            MBConv(112, 112, kernel_size=5, stride=1, expand_ratio=6),
            # MBConv6 (112, 192, 2, 6)
            MBConv(112, 192, kernel_size=5, stride=2, expand_ratio=6),
            # MBConv6 (192, 192, 1, 6)
            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),
            # MBConv6 (192, 192, 1, 6)
            MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6),
            # MBConv6 (192, 320, 1, 6)
            MBConv(192, 320, kernel_size=3, stride=1, expand_ratio=6)
        )
        
        # Final convolutional layer
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        # Fully connected layer
        self.fc = nn.Linear(1280, num_classes)
    
    def forward(self, x):
        """"""
        Forward pass of the EfficientNetB0 model.

        :param x: The input tensor, shape (batch_size, 3, 224, 224)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.blocks(x)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class MBConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio):
        """"""
        MBConv block implementation.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param kernel_size: Kernel size for the depthwise convolution.
        :param stride: Stride for the depthwise convolution.
        :param expand_ratio: Expansion ratio for the intermediate channels.
        """"""
        super(MBConv, self).__init__()
        
        self.use_residual = (stride == 1 and in_channels == out_channels)
        hidden_dim = in_channels * expand_ratio
        
        if expand_ratio != 1:
            self.expand_conv = nn.Sequential(
                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True)
            )
        
        self.depthwise_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True)
        )
        
        self.project_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(out_channels)
        )
    
    def forward(self, x):
        """"""
        Forward pass of the MBConv block.

        :param x: The input tensor, shape (batch_size, in_channels, H, W)
        :return: The output tensor, shape (batch_size, out_channels, H', W')
        """"""
        identity = x
        
        if hasattr(self, 'expand_conv'):
            x = self.expand_conv(x)
        
        x = self.depthwise_conv(x)
        x = self.project_conv(x)
        
        if self.use_residual:
            x += identity
        
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, 224, 224)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class MBConv(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int
    expand_ratio: int

    @nn.compact
    def __call__(self, x, train=True):
        use_residual = (self.stride == 1 and self.in_channels == self.out_channels)
        hidden_dim = self.in_channels * self.expand_ratio

        out = x
        if self.expand_ratio != 1:
            out = nn.Conv(features=hidden_dim, kernel_size=(1,1), strides=(1,1), padding='VALID', use_bias=False, name='expand_conv')(out)
            out = nn.BatchNorm(use_running_average=not train, name='expand_bn')(out)
            out = nn.relu6(out)

        # Depthwise convolution
        out = nn.Conv(
            features=hidden_dim,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            padding=[(self.kernel_size//2, self.kernel_size//2), (self.kernel_size//2, self.kernel_size//2)],
            feature_group_count=hidden_dim,
            use_bias=False,
            name='depthwise_conv'
        )(out)
        out = nn.BatchNorm(use_running_average=not train, name='depthwise_bn')(out)
        out = nn.relu6(out)

        # Project
        out = nn.Conv(features=self.out_channels, kernel_size=(1,1), strides=(1,1), padding='VALID', use_bias=False, name='project_conv')(out)
        out = nn.BatchNorm(use_running_average=not train, name='project_bn')(out)

        if use_residual:
            out = out + x

        return out

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, train=True):
        # Initial conv
        x = nn.Conv(features=32, kernel_size=(3,3), strides=(2,2), padding=[(1,1),(1,1)], use_bias=False, name='conv1')(x)
        x = nn.BatchNorm(use_running_average=not train, name='bn1')(x)
        x = nn.relu(x)

        # MBConv blocks
        x = MBConv(32, 16, kernel_size=3, stride=1, expand_ratio=1, name='mbconv1')(x, train=train)
        x = MBConv(16, 24, kernel_size=3, stride=2, expand_ratio=6, name='mbconv2')(x, train=train)
        x = MBConv(24, 24, kernel_size=3, stride=1, expand_ratio=6, name='mbconv3')(x, train=train)
        x = MBConv(24, 40, kernel_size=5, stride=2, expand_ratio=6, name='mbconv4')(x, train=train)
        x = MBConv(40, 40, kernel_size=5, stride=1, expand_ratio=6, name='mbconv5')(x, train=train)
        x = MBConv(40, 80, kernel_size=3, stride=2, expand_ratio=6, name='mbconv6')(x, train=train)
        x = MBConv(80, 80, kernel_size=3, stride=1, expand_ratio=6, name='mbconv7')(x, train=train)
        x = MBConv(80, 112, kernel_size=5, stride=1, expand_ratio=6, name='mbconv8')(x, train=train)
        x = MBConv(112, 112, kernel_size=5, stride=1, expand_ratio=6, name='mbconv9')(x, train=train)
        x = MBConv(112, 192, kernel_size=5, stride=2, expand_ratio=6, name='mbconv10')(x, train=train)
        x = MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6, name='mbconv11')(x, train=train)
        x = MBConv(192, 192, kernel_size=5, stride=1, expand_ratio=6, name='mbconv12')(x, train=train)
        x = MBConv(192, 320, kernel_size=3, stride=1, expand_ratio=6, name='mbconv13')(x, train=train)

        # Final conv
        x = nn.Conv(features=1280, kernel_size=(1,1), strides=(1,1), padding='VALID', use_bias=False, name='conv2')(x)
        x = nn.BatchNorm(use_running_average=not train, name='bn2')(x)
        x = nn.relu(x)

        # Adaptive avg pool to (1,1)
        x = jnp.mean(x, axis=(2,3), keepdims=True)
        x = x.reshape((x.shape[0], -1))

        # FC
        x = nn.Dense(features=self.num_classes, name='fc')(x)
        return x

# Test code
batch_size = 10
num_classes = 1000

def get_inputs():
    return [jnp.ones((batch_size, 3, 224, 224), dtype=jnp.float32)]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        EfficientNetB1 architecture implementation.

        :param num_classes: The number of output classes (default is 1000 for ImageNet).
        """"""
        super(Model, self).__init__()
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        # MBConv blocks
        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)
        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)
        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)
        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)
        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)
        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)
        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)
        
        # Final convolutional layer
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        # Fully connected layer
        self.fc = nn.Linear(1280, num_classes)
    
    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):
        """"""
        Creates a MBConv block.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param stride: Stride of the depthwise convolution.
        :param expand_ratio: Expansion ratio for the hidden layer.
        :return: A sequential MBConv block.
        """"""
        hidden_dim = round(in_channels * expand_ratio)
        return nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(out_channels),
        )
    
    def forward(self, x):
        """"""
        Forward pass of the EfficientNetB1 model.

        :param x: Input tensor, shape (batch_size, 3, 240, 240)
        :return: Output tensor, shape (batch_size, num_classes)
        """"""
        x = F.relu(self.bn1(self.conv1(x)))
        
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

# Test code
batch_size = 10
input_shape = (3, 240, 240)
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class ReLU6(nn.Module):
    @nn.compact
    def __call__(self, x):
        return jnp.clip(x, 0, 6)

class MBConvBlock(nn.Module):
    in_channels: int
    out_channels: int
    stride: int
    expand_ratio: int

    @nn.compact
    def __call__(self, x, train=True):
        hidden_dim = int(round(self.in_channels * self.expand_ratio))
        # Expansion phase
        x1 = nn.Conv(features=hidden_dim, kernel_size=(1,1), strides=(1,1), use_bias=False, padding='VALID')(x)
        x1 = nn.BatchNorm(use_running_average=not train)(x1)
        x1 = ReLU6()(x1)
        # Depthwise conv
        x1 = nn.Conv(features=hidden_dim, kernel_size=(3,3), strides=(self.stride, self.stride), 
                     use_bias=False, padding='SAME', feature_group_count=hidden_dim)(x1)
        x1 = nn.BatchNorm(use_running_average=not train)(x1)
        x1 = ReLU6()(x1)
        # Projection
        x1 = nn.Conv(features=self.out_channels, kernel_size=(1,1), strides=(1,1), use_bias=False, padding='VALID')(x1)
        x1 = nn.BatchNorm(use_running_average=not train)(x1)
        return x1

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, train=True):
        # Initial conv
        x = nn.Conv(features=32, kernel_size=(3,3), strides=(2,2), use_bias=False, padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = nn.relu(x)
        # MBConv blocks
        x = MBConvBlock(32, 16, 1, 1)(x, train=train)
        x = MBConvBlock(16, 24, 2, 6)(x, train=train)
        x = MBConvBlock(24, 40, 2, 6)(x, train=train)
        x = MBConvBlock(40, 80, 2, 6)(x, train=train)
        x = MBConvBlock(80, 112, 1, 6)(x, train=train)
        x = MBConvBlock(112, 192, 2, 6)(x, train=train)
        x = MBConvBlock(192, 320, 1, 6)(x, train=train)
        # Final conv
        x = nn.Conv(features=1280, kernel_size=(1,1), strides=(1,1), use_bias=False, padding='VALID')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = nn.relu(x)
        # Adaptive avg pool to (1,1)
        x = jnp.mean(x, axis=(2,3), keepdims=True)
        # Flatten
        x = x.reshape((x.shape[0], -1))
        # FC
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code
batch_size = 10
input_shape = (3, 240, 240)
num_classes = 1000

def get_inputs():
    # JAX expects NHWC by default, but Flax Conv2D supports NCHW if input is NCHW
    # We'll keep NCHW to match PyTorch
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, *input_shape), dtype=jnp.float32)
    return [x]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        EfficientNetB2 architecture implementation.

        :param num_classes: The number of output classes (default is 1000 for ImageNet).
        """"""
        super(Model, self).__init__()
        
        # Define the EfficientNetB2 architecture components
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.relu = nn.ReLU(inplace=True)
        
        # Define the MBConv blocks
        self.mbconv1 = self._make_mbconv_block(32, 96, 1, 3)
        self.mbconv2 = self._make_mbconv_block(96, 144, 2, 6)
        self.mbconv3 = self._make_mbconv_block(144, 192, 2, 6)
        self.mbconv4 = self._make_mbconv_block(192, 288, 2, 6)
        self.mbconv5 = self._make_mbconv_block(288, 384, 1, 6)
        
        # Final layers
        self.conv_final = nn.Conv2d(384, 1408, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn_final = nn.BatchNorm2d(1408)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(1408, num_classes)
    
    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):
        """"""
        Helper function to create a MBConv block.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param stride: Stride for the depthwise convolution.
        :param expand_ratio: Expansion ratio for the MBConv block.
        :return: A sequential container of layers forming the MBConv block.
        """"""
        layers = []
        expanded_channels = in_channels * expand_ratio
        
        # Expansion phase
        if expand_ratio != 1:
            layers.append(nn.Conv2d(in_channels, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))
            layers.append(nn.BatchNorm2d(expanded_channels))
            layers.append(nn.ReLU(inplace=True))
        
        # Depthwise convolution
        layers.append(nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3, stride=stride, padding=1, groups=expanded_channels, bias=False))
        layers.append(nn.BatchNorm2d(expanded_channels))
        layers.append(nn.ReLU(inplace=True))
        
        # Squeeze and Excitation
        layers.append(nn.AdaptiveAvgPool2d((1, 1)))
        layers.append(nn.Conv2d(expanded_channels, expanded_channels // 4, kernel_size=1, stride=1, padding=0, bias=False))
        layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Conv2d(expanded_channels // 4, expanded_channels, kernel_size=1, stride=1, padding=0, bias=False))
        layers.append(nn.Sigmoid())
        
        # Output phase
        layers.append(nn.Conv2d(expanded_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        """"""
        Forward pass of the EfficientNetB2 model.

        :param x: The input tensor, shape (batch_size, 3, 224, 224)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.relu(self.bn_final(self.conv_final(x)))
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# Test code
batch_size = 2
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, 224, 224)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class SqueezeExcite(nn.Module):
    in_channels: int
    se_ratio: float = 0.25

    @nn.compact
    def __call__(self, x):
        # x: (batch, H, W, C)
        channels = self.in_channels
        reduced_channels = max(1, int(channels * self.se_ratio))
        # Squeeze: global average pooling
        se = jnp.mean(x, axis=(1, 2), keepdims=True)
        se = nn.Conv(features=reduced_channels, kernel_size=(1, 1), strides=(1, 1), use_bias=False)(se)
        se = nn.relu(se)
        se = nn.Conv(features=channels, kernel_size=(1, 1), strides=(1, 1), use_bias=False)(se)
        se = nn.sigmoid(se)
        return x * se

class MBConvBlock(nn.Module):
    in_channels: int
    out_channels: int
    stride: int
    expand_ratio: int

    @nn.compact
    def __call__(self, x):
        # x: (batch, H, W, C)
        residual = x
        expanded_channels = self.in_channels * self.expand_ratio

        # Expansion phase
        if self.expand_ratio != 1:
            x = nn.Conv(features=expanded_channels, kernel_size=(1, 1), strides=(1, 1), use_bias=False)(x)
            x = nn.BatchNorm(use_running_average=False)(x)
            x = nn.relu(x)
        else:
            expanded_channels = self.in_channels

        # Depthwise convolution
        x = nn.Conv(
            features=expanded_channels,
            kernel_size=(3, 3),
            strides=(self.stride, self.stride),
            padding='SAME',
            feature_group_count=expanded_channels,
            use_bias=False
        )(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.relu(x)

        # Squeeze and Excitation
        x = SqueezeExcite(expanded_channels)(x)

        # Output phase
        x = nn.Conv(features=self.out_channels, kernel_size=(1, 1), strides=(1, 1), use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False)(x)

        # Residual connection if possible
        if self.stride == 1 and self.in_channels == self.out_channels:
            x = x + residual
        return x

class AdaptiveAvgPool2d(nn.Module):
    output_size: tuple

    @nn.compact
    def __call__(self, x):
        # x: (batch, H, W, C)
        return jnp.mean(x, axis=(1, 2), keepdims=True)

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x):
        # x: (batch, 3, 224, 224)
        # Flax expects (batch, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 1))

        # Stem
        x = nn.Conv(features=32, kernel_size=(3, 3), strides=(2, 2), padding='SAME', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.relu(x)

        # MBConv blocks
        x = MBConvBlock(32, 96, stride=1, expand_ratio=3)(x)
        x = MBConvBlock(96, 144, stride=2, expand_ratio=6)(x)
        x = MBConvBlock(144, 192, stride=2, expand_ratio=6)(x)
        x = MBConvBlock(192, 288, stride=2, expand_ratio=6)(x)
        x = MBConvBlock(288, 384, stride=1, expand_ratio=6)(x)

        # Final layers
        x = nn.Conv(features=1408, kernel_size=(1, 1), strides=(1, 1), use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.relu(x)
        x = AdaptiveAvgPool2d((1, 1))(x)
        x = x.reshape((x.shape[0], -1))
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code
batch_size = 2
num_classes = 1000

def get_inputs():
    return [jnp.random.rand(batch_size, 3, 224, 224).astype(jnp.float32)]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, in_channels, out_channels, groups=3):
        """"""
        ShuffleNet unit implementation.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param groups: Number of groups for group convolution.
        """"""
        super(Model, self).__init__()
        
        # Ensure the output channels are divisible by groups
        assert out_channels % 4 == 0
        mid_channels = out_channels // 4
        
        # First 1x1 group convolution
        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        
        # Depthwise 3x3 convolution
        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_channels)
        
        # Second 1x1 group convolution
        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)
        
        # Shuffle operation
        self.shuffle = ChannelShuffle(groups)
        
        # Shortcut connection if input and output channels are the same
        if in_channels == out_channels:
            self.shortcut = nn.Sequential()
        else:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        """"""
        Forward pass for ShuffleNet unit.

        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels, height, width)
        """"""
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.shuffle(out)
        out = F.relu(self.bn3(self.conv3(out)))
        
        out += self.shortcut(x)
        return out

class ChannelShuffle(nn.Module):
    def __init__(self, groups):
        """"""
        Channel shuffle operation.

        :param groups: Number of groups for shuffling.
        """"""
        super(ChannelShuffle, self).__init__()
        self.groups = groups
    
    def forward(self, x):
        """"""
        Forward pass for channel shuffle.

        :param x: Input tensor, shape (batch_size, channels, height, width)
        :return: Output tensor, shape (batch_size, channels, height, width)
        """"""
        batch_size, channels, height, width = x.size()
        channels_per_group = channels // self.groups
        
        # Reshape
        x = x.view(batch_size, self.groups, channels_per_group, height, width)
        
        # Transpose
        x = x.transpose(1, 2).contiguous()
        
        # Flatten
        x = x.view(batch_size, -1, height, width)
        
        return x
    
batch_size = 10
input_channels = 240
out_channels = 480
groups = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, input_channels, height, width)]

def get_init_inputs():
    return [input_channels, out_channels, groups]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class ChannelShuffle(nn.Module):
    groups: int

    @nn.compact
    def __call__(self, x):
        # x: (batch, channels, height, width)
        batch_size, channels, height, width = x.shape
        channels_per_group = channels // self.groups
        # Reshape to (batch, groups, channels_per_group, height, width)
        x = x.reshape((batch_size, self.groups, channels_per_group, height, width))
        # Transpose to (batch, channels_per_group, groups, height, width)
        x = jnp.transpose(x, (0, 2, 1, 3, 4))
        # Flatten back to (batch, channels, height, width)
        x = x.reshape((batch_size, channels, height, width))
        return x

class Model(nn.Module):
    in_channels: int
    out_channels: int
    groups: int = 3

    @nn.compact
    def __call__(self, x, train: bool = True):
        assert self.out_channels % 4 == 0
        mid_channels = self.out_channels // 4

        # First 1x1 group convolution
        conv1 = nn.Conv(
            features=mid_channels,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            feature_group_count=self.groups,
            use_bias=False,
            name='conv1'
        )
        bn1 = nn.BatchNorm(use_running_average=not train, name='bn1')
        # Depthwise 3x3 convolution
        conv2 = nn.Conv(
            features=mid_channels,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding='SAME',
            feature_group_count=mid_channels,
            use_bias=False,
            name='conv2'
        )
        bn2 = nn.BatchNorm(use_running_average=not train, name='bn2')
        # Second 1x1 group convolution
        conv3 = nn.Conv(
            features=self.out_channels,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            feature_group_count=self.groups,
            use_bias=False,
            name='conv3'
        )
        bn3 = nn.BatchNorm(use_running_average=not train, name='bn3')
        shuffle = ChannelShuffle(self.groups)

        # Shortcut connection
        if self.in_channels == self.out_channels:
            def shortcut_fn(x, train):
                return x
        else:
            def shortcut_fn(x, train):
                y = nn.Conv(
                    features=self.out_channels,
                    kernel_size=(1, 1),
                    strides=(1, 1),
                    padding='VALID',
                    use_bias=False,
                    name='shortcut_conv'
                )(x)
                y = nn.BatchNorm(use_running_average=not train, name='shortcut_bn')(y)
                return y

        out = conv1(x)
        out = bn1(out)
        out = nn.relu(out)
        out = conv2(out)
        out = bn2(out)
        out = shuffle(out)
        out = conv3(out)
        out = bn3(out)
        out = nn.relu(out)
        out = out + shortcut_fn(x, train)
        return out

batch_size = 10
input_channels = 240
out_channels = 480
groups = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    return [jnp.random.rand(batch_size, input_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [input_channels, out_channels, groups]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class ShuffleNetUnit(nn.Module):
    def __init__(self, in_channels, out_channels, groups=3):
        """"""
        ShuffleNet unit implementation.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param groups: Number of groups for group convolution.
        """"""
        super(ShuffleNetUnit, self).__init__()
        
        # Ensure the output channels are divisible by groups
        assert out_channels % 4 == 0
        mid_channels = out_channels // 4
        
        # First 1x1 group convolution
        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        
        # Depthwise 3x3 convolution
        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, groups=mid_channels, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_channels)
        
        # Second 1x1 group convolution
        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=groups, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)
        
        # Shuffle operation
        self.shuffle = ChannelShuffle(groups)
        
        # Shortcut connection if input and output channels are the same
        if in_channels == out_channels:
            self.shortcut = nn.Sequential()
        else:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        """"""
        Forward pass for ShuffleNet unit.

        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels, height, width)
        """"""
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.shuffle(out)
        out = F.relu(self.bn3(self.conv3(out)))
        
        out += self.shortcut(x)
        return out

class ChannelShuffle(nn.Module):
    def __init__(self, groups):
        """"""
        Channel shuffle operation.

        :param groups: Number of groups for shuffling.
        """"""
        super(ChannelShuffle, self).__init__()
        self.groups = groups
    
    def forward(self, x):
        """"""
        Forward pass for channel shuffle.

        :param x: Input tensor, shape (batch_size, channels, height, width)
        :return: Output tensor, shape (batch_size, channels, height, width)
        """"""
        batch_size, channels, height, width = x.size()
        channels_per_group = channels // self.groups
        
        # Reshape
        x = x.view(batch_size, self.groups, channels_per_group, height, width)
        
        # Transpose
        x = x.transpose(1, 2).contiguous()
        
        # Flatten
        x = x.view(batch_size, -1, height, width)
        
        return x

class Model(nn.Module):
    def __init__(self, num_classes=1000, groups=3, stages_repeats=[3, 7, 3], stages_out_channels=[24, 240, 480, 960]):
        """"""
        ShuffleNet architecture.

        :param num_classes: Number of output classes.
        :param groups: Number of groups for group convolution.
        :param stages_repeats: List of ints specifying the number of repeats for each stage.
        :param stages_out_channels: List of ints specifying the output channels for each stage.
        """"""
        super(Model, self).__init__()
        
        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1], stages_repeats[0], groups)
        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2], stages_repeats[1], groups)
        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3], stages_repeats[2], groups)
        
        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn5 = nn.BatchNorm2d(1024)
        
        self.fc = nn.Linear(1024, num_classes)
    
    def _make_stage(self, in_channels, out_channels, repeats, groups):
        """"""
        Helper function to create a stage of ShuffleNet units.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param repeats: Number of ShuffleNet units in the stage.
        :param groups: Number of groups for group convolution.
        :return: nn.Sequential containing the stage.
        """"""
        layers = []
        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))
        for _ in range(1, repeats):
            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        """"""
        Forward pass for ShuffleNet.

        :param x: Input tensor, shape (batch_size, 3, height, width)
        :return: Output tensor, shape (batch_size, num_classes)
        """"""
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        
        x = F.relu(self.bn5(self.conv5(x)))
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, input_channels, height, width)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

def channel_shuffle(x, groups):
    # x: (batch, channels, height, width)
    batch_size, channels, height, width = x.shape
    channels_per_group = channels // groups
    # Reshape
    x = x.reshape(batch_size, groups, channels_per_group, height, width)
    # Transpose group and channel dims
    x = jnp.transpose(x, (0, 2, 1, 3, 4))
    # Flatten
    x = x.reshape(batch_size, channels, height, width)
    return x

class ChannelShuffle(nn.Module):
    groups: int

    @nn.compact
    def __call__(self, x):
        return channel_shuffle(x, self.groups)

class ShuffleNetUnit(nn.Module):
    in_channels: int
    out_channels: int
    groups: int = 3

    @nn.compact
    def __call__(self, x, *, train=True):
        assert self.out_channels % 4 == 0
        mid_channels = self.out_channels // 4

        # First 1x1 group convolution
        conv1 = nn.Conv(
            features=mid_channels,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            use_bias=False,
            feature_group_count=self.groups,
            name='conv1'
        )
        bn1 = nn.BatchNorm(use_running_average=not train, name='bn1')
        out = conv1(x)
        out = bn1(out)
        out = nn.relu(out)

        # Depthwise 3x3 convolution
        conv2 = nn.Conv(
            features=mid_channels,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding='SAME',
            use_bias=False,
            feature_group_count=mid_channels,
            name='conv2'
        )
        bn2 = nn.BatchNorm(use_running_average=not train, name='bn2')
        out = conv2(out)
        out = bn2(out)

        # Shuffle operation
        out = ChannelShuffle(self.groups)(out)

        # Second 1x1 group convolution
        conv3 = nn.Conv(
            features=self.out_channels,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            use_bias=False,
            feature_group_count=self.groups,
            name='conv3'
        )
        bn3 = nn.BatchNorm(use_running_average=not train, name='bn3')
        out = conv3(out)
        out = bn3(out)
        out = nn.relu(out)

        # Shortcut connection
        if self.in_channels == self.out_channels:
            shortcut = x
        else:
            shortcut = nn.Conv(
                features=self.out_channels,
                kernel_size=(1, 1),
                strides=(1, 1),
                padding='VALID',
                use_bias=False,
                name='shortcut_conv'
            )(x)
            shortcut = nn.BatchNorm(use_running_average=not train, name='shortcut_bn')(shortcut)

        out = out + shortcut
        return out

class Sequential(nn.Module):
    layers: list

    def setup(self):
        self._layers = [layer for layer in self.layers]

    def __call__(self, x, *, train=True):
        for layer in self._layers:
            # If the layer is a Flax module, call with train if possible
            if hasattr(layer, '__call__'):
                try:
                    x = layer(x, train=train)
                except TypeError:
                    x = layer(x)
            else:
                x = layer(x)
        return x

class Model(nn.Module):
    num_classes: int = 1000
    groups: int = 3
    stages_repeats: list = (3, 7, 3)
    stages_out_channels: list = (24, 240, 480, 960)

    @nn.compact
    def __call__(self, x, *, train=True):
        # Initial conv
        x = nn.Conv(
            features=self.stages_out_channels[0],
            kernel_size=(3, 3),
            strides=(2, 2),
            padding='SAME',
            use_bias=False,
            name='conv1'
        )(x)
        x = nn.BatchNorm(use_running_average=not train, name='bn1')(x)
        x = nn.relu(x)
        x = nn.max_pool(
            x,
            window_shape=(3, 3),
            strides=(2, 2),
            padding='SAME'
        )

        # Stages
        x = self._make_stage(
            self.stages_out_channels[0],
            self.stages_out_channels[1],
            self.stages_repeats[0],
            self.groups,
            name='stage2'
        )(x, train=train)
        x = self._make_stage(
            self.stages_out_channels[1],
            self.stages_out_channels[2],
            self.stages_repeats[1],
            self.groups,
            name='stage3'
        )(x, train=train)
        x = self._make_stage(
            self.stages_out_channels[2],
            self.stages_out_channels[3],
            self.stages_repeats[2],
            self.groups,
            name='stage4'
        )(x, train=train)

        # Final conv
        x = nn.Conv(
            features=1024,
            kernel_size=(1, 1),
            strides=(1, 1),
            padding='VALID',
            use_bias=False,
            name='conv5'
        )(x)
        x = nn.BatchNorm(use_running_average=not train, name='bn5')(x)
        x = nn.relu(x)

        # Global average pooling
        x = jnp.mean(x, axis=(2, 3), keepdims=True)
        x = x.reshape((x.shape[0], -1))
        x = nn.Dense(features=self.num_classes, name='fc')(x)
        return x

    def _make_stage(self, in_channels, out_channels, repeats, groups, name=None):
        layers = []
        layers.append(ShuffleNetUnit(in_channels, out_channels, groups))
        for _ in range(1, repeats):
            layers.append(ShuffleNetUnit(out_channels, out_channels, groups))
        return Sequential(layers=layers, name=name)

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    return [jnp.random.rand(batch_size, input_channels, height, width).astype(jnp.float32)]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_channels, stages, block_widths, output_classes):
        """"""
        :param input_channels: int, Number of input channels for the first layer
        :param stages: int, Number of stages in the RegNet architecture
        :param block_widths: List[int], Width (number of channels) for each block in the stages
        :param output_classes: int, Number of output classes for classification
        """"""
        super(Model, self).__init__()

        self.stages = stages
        self.block_widths = block_widths
        
        layers = []
        current_channels = input_channels
        
        # Construct the stages with their respective blocks
        for i in range(stages):
            layers.append(self._make_stage(current_channels, block_widths[i]))
            current_channels = block_widths[i]
        
        self.feature_extractor = nn.Sequential(*layers)
        
        # Final fully connected layer for classification
        self.fc = nn.Linear(block_widths[-1], output_classes)
    
    def _make_stage(self, in_channels, out_channels):
        """"""
        Creates a simple block for each stage.
        :param in_channels: int, number of input channels
        :param out_channels: int, number of output channels
        :return: nn.Sequential block with convolutional layers
        """"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

    def forward(self, x):
        """"""
        Forward pass through the RegNet model.
        :param x: torch.Tensor of shape (batch_size, input_channels, height, width)
        :return: torch.Tensor of shape (batch_size, output_classes)
        """"""
        x = self.feature_extractor(x)
        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling
        x = self.fc(x)
        return x

# Test code for the RegNet model
batch_size = 8
input_channels = 3
image_height, image_width = 224, 224
stages = 3
block_widths = [64, 128, 256]
output_classes = 10

def get_inputs():
    """""" Generates random input tensor of shape (batch_size, input_channels, height, width) """"""
    return [torch.rand(batch_size, input_channels, image_height, image_width)]

def get_init_inputs():
    """""" Initializes model parameters """"""
    return [input_channels, stages, block_widths, output_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_channels: int
    stages: int
    block_widths: list
    output_classes: int

    @nn.compact
    def __call__(self, x):
        current_channels = self.input_channels
        for i in range(self.stages):
            x = self._make_stage(current_channels, self.block_widths[i])(x)
            current_channels = self.block_widths[i]
        # Global Average Pooling over spatial dimensions (H, W)
        x = jnp.mean(x, axis=(2, 3))
        x = nn.Dense(self.output_classes)(x)
        return x

    def _make_stage(self, in_channels, out_channels):
        # Returns a Flax module that applies the stage
        return StageBlock(in_channels, out_channels)

class StageBlock(nn.Module):
    in_channels: int
    out_channels: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(self.out_channels, kernel_size=(3,3), padding='SAME', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.relu(x)
        x = nn.Conv(self.out_channels, kernel_size=(3,3), padding='SAME', use_bias=False)(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2,2), strides=(2,2), padding='VALID')
        return x

# Test code for the RegNet model
batch_size = 8
input_channels = 3
image_height, image_width = 224, 224
stages = 3
block_widths = [64, 128, 256]
output_classes = 10

def get_inputs():
    """"""Generates random input tensor of shape (batch_size, input_channels, height, width)""""""
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, input_channels, image_height, image_width))
    return [x]

def get_init_inputs():
    """"""Initializes model parameters""""""
    return [input_channels, stages, block_widths, output_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0.1, emb_dropout=0.1):
        """"""
        Vision Transformer (ViT) model.

        :param image_size: The size of the input image (assumed to be square).
        :param patch_size: The size of each patch (assumed to be square).
        :param num_classes: The number of output classes.
        :param dim: The dimensionality of the embedding space.
        :param depth: The number of transformer layers.
        :param heads: The number of attention heads.
        :param mlp_dim: The dimensionality of the MLP (Multi-Layer Perceptron) in the transformer.
        :param channels: The number of channels in the input image (default is 3 for RGB).
        :param dropout: Dropout rate applied in the MLP.
        :param emb_dropout: Dropout rate applied to the embedded patches.
        """"""
        super(Model, self).__init__()
        
        assert image_size % patch_size == 0, ""Image dimensions must be divisible by the patch size.""
        num_patches = (image_size // patch_size) ** 2
        patch_dim = channels * patch_size ** 2
        
        self.patch_size = patch_size
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.patch_to_embedding = nn.Linear(patch_dim, dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(emb_dropout)
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),
            num_layers=depth
        )
        
        self.to_cls_token = nn.Identity()
        self.mlp_head = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, num_classes)
        )
    
    def forward(self, img):
        """"""
        Forward pass of the Vision Transformer.

        :param img: The input image tensor, shape (batch_size, channels, image_size, image_size).
        :return: The output tensor, shape (batch_size, num_classes).
        """"""
        p = self.patch_size
        
        x = img.unfold(2, p, p).unfold(3, p, p).reshape(img.shape[0], -1, p*p*img.shape[1])
        x = self.patch_to_embedding(x)
        
        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding
        x = self.dropout(x)
        
        x = self.transformer(x)
        
        x = self.to_cls_token(x[:, 0])
        return self.mlp_head(x)

# Test code
image_size = 224
patch_size = 16
num_classes = 10
dim = 512
depth = 6
heads = 8
mlp_dim = 2048
channels = 3
dropout = 0.0
emb_dropout = 0.0

def get_inputs():
    return [torch.rand(2, channels, image_size, image_size)]

def get_init_inputs():
    return [image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels, dropout, emb_dropout]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Any, Callable
import numpy as np

class MLP(nn.Module):
    dim: int
    mlp_dim: int
    dropout: float

    @nn.compact
    def __call__(self, x, train: bool):
        x = nn.Dense(self.mlp_dim)(x)
        x = nn.gelu(x)
        x = nn.Dropout(self.dropout)(x, deterministic=not train)
        x = nn.Dense(self.dim)(x)
        x = nn.Dropout(self.dropout)(x, deterministic=not train)
        return x

class TransformerEncoderLayer(nn.Module):
    dim: int
    heads: int
    mlp_dim: int
    dropout: float

    @nn.compact
    def __call__(self, x, train: bool):
        # Multi-head self-attention
        residual = x
        x = nn.LayerNorm()(x)
        x = nn.SelfAttention(
            num_heads=self.heads,
            qkv_features=self.dim,
            out_features=self.dim,
            dropout_rate=self.dropout,
            deterministic=not train,
        )(x)
        x = nn.Dropout(self.dropout)(x, deterministic=not train)
        x = x + residual

        # MLP
        residual = x
        x = nn.LayerNorm()(x)
        x = MLP(dim=self.dim, mlp_dim=self.mlp_dim, dropout=self.dropout)(x, train)
        x = x + residual
        return x

class TransformerEncoder(nn.Module):
    dim: int
    depth: int
    heads: int
    mlp_dim: int
    dropout: float

    @nn.compact
    def __call__(self, x, train: bool):
        for _ in range(self.depth):
            x = TransformerEncoderLayer(
                dim=self.dim,
                heads=self.heads,
                mlp_dim=self.mlp_dim,
                dropout=self.dropout
            )(x, train)
        return x

class Model(nn.Module):
    image_size: int
    patch_size: int
    num_classes: int
    dim: int
    depth: int
    heads: int
    mlp_dim: int
    channels: int = 3
    dropout: float = 0.1
    emb_dropout: float = 0.1

    def setup(self):
        assert self.image_size % self.patch_size == 0, ""Image dimensions must be divisible by the patch size.""
        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.patch_dim = self.channels * self.patch_size ** 2

        self.patch_to_embedding = nn.Dense(self.dim)
        self.pos_embedding = self.param(
            'pos_embedding',
            nn.initializers.normal(stddev=1.0),
            (1, self.num_patches + 1, self.dim)
        )
        self.cls_token = self.param(
            'cls_token',
            nn.initializers.normal(stddev=1.0),
            (1, 1, self.dim)
        )
        self.dropout_layer = nn.Dropout(self.emb_dropout)
        self.transformer = TransformerEncoder(
            dim=self.dim,
            depth=self.depth,
            heads=self.heads,
            mlp_dim=self.mlp_dim,
            dropout=self.dropout
        )
        self.mlp_head = nn.Sequential([
            nn.Dense(self.mlp_dim),
            nn.gelu,
            nn.Dropout(self.dropout),
            nn.Dense(self.num_classes)
        ])

    def __call__(self, img, train: bool = True):
        # img: (batch, channels, image_size, image_size)
        B = img.shape[0]
        p = self.patch_size
        # Convert to patches
        # (B, C, H, W) -> (B, num_patches, patch_dim)
        h = w = self.image_size
        assert img.shape[2] == h and img.shape[3] == w
        # Rearrange using jax.lax APIs
        # Step 1: reshape to (B, C, h//p, p, w//p, p)
        img = img.reshape(B, self.channels, h // p, p, w // p, p)
        # Step 2: transpose to (B, h//p, w//p, C, p, p)
        img = img.transpose(0, 2, 4, 1, 3, 5)
        # Step 3: reshape to (B, num_patches, patch_dim)
        img = img.reshape(B, self.num_patches, self.patch_dim)
        x = self.patch_to_embedding(img)
        # Add class token
        cls_tokens = jnp.tile(self.cls_token, (B, 1, 1))
        x = jnp.concatenate([cls_tokens, x], axis=1)
        x = x + self.pos_embedding
        x = self.dropout_layer(x, deterministic=not train)
        x = self.transformer(x, train)
        x = x[:, 0]  # take cls token
        for layer in self.mlp_head:
            if isinstance(layer, nn.Dropout):
                x = layer(x, deterministic=not train)
            elif callable(layer):
                x = layer(x)
            else:
                x = layer(x)
        return x

# Test code
image_size = 224
patch_size = 16
num_classes = 10
dim = 512
depth = 6
heads = 8
mlp_dim = 2048
channels = 3
dropout = 0.0
emb_dropout = 0.0

def get_inputs():
    # Return a list with a single input array of shape (2, channels, image_size, image_size)
    rng = np.random.default_rng(0)
    arr = rng.standard_normal((2, channels, image_size, image_size), dtype=np.float32)
    return [jnp.array(arr)]

def get_init_inputs():
    return [image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels, dropout, emb_dropout]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    # --------------------------------------------------------
# Swin Transformer
# Copyright (c) 2021 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

import torch
import torch.nn as nn
import torch.nn.functional as F
from itertools import repeat
import collections.abc

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """"""
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """"""
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """"""
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """"""
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class SwinMLPBlock(nn.Module):
    r"""""" Swin MLP Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        drop (float, optional): Dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """"""

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, ""shift_size must in 0-window_size""

        self.padding = [self.window_size - self.shift_size, self.shift_size,
                        self.window_size - self.shift_size, self.shift_size]  # P_l,P_r,P_t,P_b

        self.norm1 = norm_layer(dim)
        # use group convolution to implement multi-head MLP
        self.spatial_mlp = nn.Conv1d(self.num_heads * self.window_size ** 2,
                                     self.num_heads * self.window_size ** 2,
                                     kernel_size=1,
                                     groups=self.num_heads)

        self.drop_path = nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # shift
        if self.shift_size > 0:
            P_l, P_r, P_t, P_b = self.padding
            shifted_x = F.pad(x, [0, 0, P_l, P_r, P_t, P_b], ""constant"", 0)
        else:
            shifted_x = x
        _, _H, _W, _ = shifted_x.shape

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # Window/Shifted-Window Spatial MLP
        x_windows_heads = x_windows.view(-1, self.window_size * self.window_size, self.num_heads, C // self.num_heads)
        x_windows_heads = x_windows_heads.transpose(1, 2)  # nW*B, nH, window_size*window_size, C//nH
        x_windows_heads = x_windows_heads.reshape(-1, self.num_heads * self.window_size * self.window_size,
                                                  C // self.num_heads)
        spatial_mlp_windows = self.spatial_mlp(x_windows_heads)  # nW*B, nH*window_size*window_size, C//nH
        spatial_mlp_windows = spatial_mlp_windows.view(-1, self.num_heads, self.window_size * self.window_size,
                                                       C // self.num_heads).transpose(1, 2)
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size * self.window_size, C)

        # merge windows
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(spatial_mlp_windows, self.window_size, _H, _W)  # B H' W' C

        # reverse shift
        if self.shift_size > 0:
            P_l, P_r, P_t, P_b = self.padding
            x = shifted_x[:, P_t:-P_b, P_l:-P_r, :].contiguous()
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x


class PatchMerging(nn.Module):
    r"""""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """"""

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """"""
        x: B, H*W, C
        """"""
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""
        assert H % 2 == 0 and W % 2 == 0, f""x size ({H}*{W}) are not even.""

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

class BasicLayer(nn.Module):
    """""" A basic Swin MLP layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        drop (float, optional): Dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """"""

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., drop=0., drop_path=0.,
                 norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinMLPBlock(dim=dim, input_resolution=input_resolution,
                         num_heads=num_heads, window_size=window_size,
                         shift_size=0 if (i % 2 == 0) else window_size // 2,
                         mlp_ratio=mlp_ratio,
                         drop=drop,
                         drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                         norm_layer=norm_layer)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
            return tuple(x)
        return tuple(repeat(x, n))
    return parse
to_2tuple = _ntuple(2)

class PatchEmbed(nn.Module):
    r"""""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """"""

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f""Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).""
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class Model(nn.Module):
    r"""""" Swin MLP

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin MLP layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        drop_rate (float): Dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """"""

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               drop=drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
    
batch_size = 10
image_size = 224

def get_inputs():
    return [torch.rand(batch_size, 3, image_size, image_size)]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
import numpy as np
from functools import partial
from itertools import repeat
import collections.abc

def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
            return tuple(x)
        return tuple(repeat(x, n))
    return parse
to_2tuple = _ntuple(2)

class Mlp(nn.Module):
    in_features: int
    hidden_features: int = None
    out_features: int = None
    act_layer: callable = nn.gelu
    drop: float = 0.

    @nn.compact
    def __call__(self, x, deterministic=True):
        out_features = self.out_features or self.in_features
        hidden_features = self.hidden_features or self.in_features
        x = nn.Dense(hidden_features)(x)
        x = self.act_layer(x)
        x = nn.Dropout(self.drop)(x, deterministic=deterministic)
        x = nn.Dense(out_features)(x)
        x = nn.Dropout(self.drop)(x, deterministic=deterministic)
        return x

def window_partition(x, window_size):
    """"""
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """"""
    B, H, W, C = x.shape
    x = x.reshape(B, H // window_size, window_size, W // window_size, window_size, C)
    x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))
    windows = x.reshape(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size, H, W):
    """"""
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """"""
    B = int(windows.shape[0] // (H * W // window_size // window_size))
    x = windows.reshape(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))
    x = x.reshape(B, H, W, -1)
    return x

class SwinMLPBlock(nn.Module):
    dim: int
    input_resolution: tuple
    num_heads: int
    window_size: int = 7
    shift_size: int = 0
    mlp_ratio: float = 4.
    drop: float = 0.
    drop_path: float = 0.
    act_layer: callable = nn.gelu
    norm_layer: callable = nn.LayerNorm

    @nn.compact
    def __call__(self, x, deterministic=True):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""

        shortcut = x
        x = self.norm_layer()(x)
        x = x.reshape(B, H, W, C)

        # window/shift logic
        window_size = self.window_size
        shift_size = self.shift_size
        if min(self.input_resolution) <= window_size:
            shift_size = 0
            window_size = min(self.input_resolution)
        assert 0 <= shift_size < window_size, ""shift_size must in 0-window_size""

        padding = [window_size - shift_size, shift_size,
                   window_size - shift_size, shift_size]  # P_l,P_r,P_t,P_b

        # shift
        if shift_size > 0:
            P_l, P_r, P_t, P_b = padding
            # pad order: ((before_H, after_H), (before_W, after_W))
            x = jnp.pad(x, ((0,0), (P_t, P_b), (P_l, P_r), (0,0)), mode='constant')
        _B, _H, _W, _C = x.shape

        # partition windows
        x_windows = window_partition(x, window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.reshape(-1, window_size * window_size, C)  # nW*B, window_size*window_size, C

        # Window/Shifted-Window Spatial MLP
        nH = self.num_heads
        x_windows_heads = x_windows.reshape(-1, window_size * window_size, nH, C // nH)
        x_windows_heads = jnp.transpose(x_windows_heads, (0, 2, 1, 3))  # nW*B, nH, window_size*window_size, C//nH
        x_windows_heads = x_windows_heads.reshape(-1, nH * window_size * window_size, C // nH)

        # Grouped Conv1D: implement as DenseGeneral with axis splitting
        # (batch, length, features) -> (batch, length, features)
        # groups = nH, features = C//nH
        def grouped_dense(x, features, groups, name=None):
            # x: (batch, length, features)
            in_features = x.shape[-1]
            assert in_features * groups == features
            x = x.reshape(x.shape[0], x.shape[1] * groups, in_features // groups)
            x = nn.DenseGeneral(features=(features//groups,), axis=-1, name=name)(x)
            x = x.reshape(x.shape[0], features)
            return x

        # Instead, use Conv1D with groups
        spatial_mlp = nn.Conv(features=nH * window_size * window_size,
                              kernel_size=(1,),
                              feature_group_count=nH,
                              name='spatial_mlp')
        # But Flax Conv expects (batch, length, channels)
        # So swap axes
        x_windows_heads = jnp.transpose(x_windows_heads, (0,2,1))  # (batch, features, length)
        spatial_mlp_windows = spatial_mlp(x_windows_heads)
        spatial_mlp_windows = jnp.transpose(spatial_mlp_windows, (0,2,1))  # (batch, length, features)
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, nH, window_size * window_size, C // nH)
        spatial_mlp_windows = jnp.transpose(spatial_mlp_windows, (0,2,1,3))
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, window_size * window_size, C)

        # merge windows
        spatial_mlp_windows = spatial_mlp_windows.reshape(-1, window_size, window_size, C)
        x = window_reverse(spatial_mlp_windows, window_size, _H, _W)  # B H' W' C

        # reverse shift
        if shift_size > 0:
            P_l, P_r, P_t, P_b = padding
            x = x[:, P_t:_H-P_b, P_l:_W-P_r, :]
        x = x.reshape(B, H * W, C)

        # FFN
        x = shortcut + x
        x = x + Mlp(self.dim, int(self.dim * self.mlp_ratio), act_layer=self.act_layer, drop=self.drop)(self.norm_layer()(x), deterministic=deterministic)
        return x

class PatchMerging(nn.Module):
    input_resolution: tuple
    dim: int
    norm_layer: callable = nn.LayerNorm

    @nn.compact
    def __call__(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""
        assert H % 2 == 0 and W % 2 == 0, f""x size ({H}*{W}) are not even.""

        x = x.reshape(B, H, W, C)
        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = jnp.concatenate([x0, x1, x2, x3], axis=-1)  # B H/2 W/2 4*C
        x = x.reshape(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm_layer()(x)
        x = nn.Dense(2 * C, use_bias=False)(x)
        return x

class BasicLayer(nn.Module):
    dim: int
    input_resolution: tuple
    depth: int
    num_heads: int
    window_size: int
    mlp_ratio: float = 4.
    drop: float = 0.
    drop_path: float = 0.
    norm_layer: callable = nn.LayerNorm
    downsample: callable = None
    use_checkpoint: bool = False

    @nn.compact
    def __call__(self, x, deterministic=True):
        for i in range(self.depth):
            x = SwinMLPBlock(
                dim=self.dim,
                input_resolution=self.input_resolution,
                num_heads=self.num_heads,
                window_size=self.window_size,
                shift_size=0 if (i % 2 == 0) else self.window_size // 2,
                mlp_ratio=self.mlp_ratio,
                drop=self.drop,
                drop_path=self.drop_path if not isinstance(self.drop_path, (list, tuple)) else self.drop_path[i],
                norm_layer=self.norm_layer
            )(x, deterministic=deterministic)
        if self.downsample is not None:
            x = self.downsample(self.input_resolution, dim=self.dim, norm_layer=self.norm_layer)(x)
        return x

class PatchEmbed(nn.Module):
    img_size: int = 224
    patch_size: int = 4
    in_chans: int = 3
    embed_dim: int = 96
    norm_layer: callable = None

    def setup(self):
        img_size = to_2tuple(self.img_size)
        patch_size = to_2tuple(self.patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]
        self.proj = nn.Conv(self.embed_dim, kernel_size=self.patch_size, strides=self.patch_size)
        if self.norm_layer is not None:
            self.norm = self.norm_layer()
        else:
            self.norm = None

    def __call__(self, x):
        B, C, H, W = x.shape
        assert H == self.img_size[0] and W == self.img_size[1], \
            f""Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).""
        x = jnp.transpose(x, (0,2,3,1))  # (B, H, W, C)
        x = self.proj(x)  # (B, H', W', embed_dim)
        x = x.reshape(B, -1, self.embed_dim)  # (B, Ph*Pw, C)
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops

class Model(nn.Module):
    img_size: int = 224
    patch_size: int = 4
    in_chans: int = 3
    num_classes: int = 1000
    embed_dim: int = 96
    depths: list = (2, 2, 6, 2)
    num_heads: list = (3, 6, 12, 24)
    window_size: int = 7
    mlp_ratio: float = 4.
    drop_rate: float = 0.
    drop_path_rate: float = 0.1
    norm_layer: callable = nn.LayerNorm
    patch_norm: bool = True
    use_checkpoint: bool = False

    def setup(self):
        self.num_layers = len(self.depths)
        self.num_features = int(self.embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = self.mlp_ratio

        self.patch_embed = PatchEmbed(
            img_size=self.img_size, patch_size=self.patch_size, in_chans=self.in_chans, embed_dim=self.embed_dim,
            norm_layer=self.norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        self.pos_drop = nn.Dropout(self.drop_rate)

        # stochastic depth
        dpr = np.linspace(0, self.drop_path_rate, sum(self.depths)).tolist()

        # build layers
        self.layers = []
        for i_layer in range(self.num_layers):
            downsample = PatchMerging if (i_layer < self.num_layers - 1) else None
            layer = BasicLayer(
                dim=int(self.embed_dim * 2 ** i_layer),
                input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                  patches_resolution[1] // (2 ** i_layer)),
                depth=self.depths[i_layer],
                num_heads=self.num_heads[i_layer],
                window_size=self.window_size,
                mlp_ratio=self.mlp_ratio,
                drop=self.drop_rate,
                drop_path=dpr[sum(self.depths[:i_layer]):sum(self.depths[:i_layer + 1])],
                norm_layer=self.norm_layer,
                downsample=downsample,
                use_checkpoint=self.use_checkpoint
            )
            self.layers.append(layer)
        self.layers = self.layers

        self.norm = self.norm_layer()
        self.head = nn.Dense(self.num_classes) if self.num_classes > 0 else lambda x: x

    def __call__(self, x, deterministic=True):
        x = self.patch_embed(x)
        x = self.pos_drop(x, deterministic=deterministic)
        for layer in self.layers:
            x = layer(x, deterministic=deterministic)
        x = self.norm(x)  # B L C
        x = jnp.mean(x, axis=1)  # global average pooling over sequence
        x = self.head(x)
        return x

batch_size = 10
image_size = 224

def get_inputs():
    return [jnp.array(np.random.rand(batch_size, 3, image_size, image_size), dtype=jnp.float32)]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_layer_sizes, output_size):
        """"""
        :param input_size: The number of input features
        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer
        :param output_size: The number of output features
        """"""
        super(Model, self).__init__()
        
        layers = []
        current_input_size = input_size
        
        for hidden_size in hidden_layer_sizes:
            layers.append(nn.Linear(current_input_size, hidden_size))
            layers.append(nn.ReLU())
            current_input_size = hidden_size
        
        layers.append(nn.Linear(current_input_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        """"""
        :param x: The input tensor, shape (batch_size, input_size)
        :return: The output tensor, shape (batch_size, output_size)
        """"""
        return self.network(x)

# Test code
batch_size = 128
input_size = 16384
hidden_layer_sizes = [32768, 32768]
output_size = 16384

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_layer_sizes, output_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_layer_sizes: list
    output_size: int

    @nn.compact
    def __call__(self, x):
        current_input_size = self.input_size
        for hidden_size in self.hidden_layer_sizes:
            x = nn.Dense(hidden_size)(x)
            x = nn.relu(x)
            current_input_size = hidden_size
        x = nn.Dense(self.output_size)(x)
        return x

# Test code
batch_size = 128
input_size = 16384
hidden_layer_sizes = [32768, 32768]
output_size = 16384

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_layer_sizes, output_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    # --------------------------------------------------------
# Swin Transformer V2
# Copyright (c) 2022 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import collections
from itertools import repeat

def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
            return tuple(x)
        return tuple(repeat(x, n))
    return parse
to_2tuple = _ntuple(2)

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """"""
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """"""
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """"""
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """"""
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r"""""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.
    """"""

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,
                 pretrained_window_size=[0, 0]):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.pretrained_window_size = pretrained_window_size
        self.num_heads = num_heads

        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)

        # mlp to generate continuous relative position bias
        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),
                                     nn.ReLU(inplace=True),
                                     nn.Linear(512, num_heads, bias=False))

        # get relative_coords_table
        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)
        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)
        relative_coords_table = torch.stack(
            torch.meshgrid([relative_coords_h,
                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2
        if pretrained_window_size[0] > 0:
            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)
            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)
        else:
            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)
            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)
        relative_coords_table *= 8  # normalize to -8, 8
        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(
            torch.abs(relative_coords_table) + 1.0) / np.log2(8)

        self.register_buffer(""relative_coords_table"", relative_coords_table)

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer(""relative_position_index"", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(dim))
            self.v_bias = nn.Parameter(torch.zeros(dim))
        else:
            self.q_bias = None
            self.v_bias = None
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """"""
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """"""
        B_, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # cosine attention
        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))
        logit_scale = torch.clamp(self.logit_scale.to(x.device), max=torch.log(torch.tensor(1. / 0.01, device=x.device))).exp()
        attn = attn * logit_scale

        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)
        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class SwinTransformerBlock(nn.Module):
    r"""""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        pretrained_window_size (int): Window size in pre-training.
    """"""

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, ""shift_size must in 0-window_size""

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,
            pretrained_window_size=to_2tuple(pretrained_window_size))

        self.drop_path = nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer(""attn_mask"", attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""

        shortcut = x
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(self.norm1(x))

        # FFN
        x = x + self.drop_path(self.norm2(self.mlp(x)))

        return x


class PatchMerging(nn.Module):
    r"""""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """"""

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(2 * dim)

    def forward(self, x):
        """"""
        x: B, H*W, C
        """"""
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""
        assert H % 2 == 0 and W % 2 == 0, f""x size ({H}*{W}) are not even.""

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.reduction(x)
        x = self.norm(x)

        return x

class BasicLayer(nn.Module):
    """""" A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
        pretrained_window_size (int): Local window size in pre-training.
    """"""

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,
                 pretrained_window_size=0):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer,
                                 pretrained_window_size=pretrained_window_size)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x
    

class PatchEmbed(nn.Module):
    r"""""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """"""

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f""Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).""
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x


class Model(nn.Module):
    r"""""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.
    """"""

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, patch_norm=True,
                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint,
                               pretrained_window_size=pretrained_window_sizes[i_layer])
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

batch_size = 10
image_size = 224

def get_inputs():
    return [torch.rand(batch_size, 3, image_size, image_size)]

def get_init_inputs():
    return []
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
import numpy as np
from flax import linen as nn
from typing import Any, Callable, Sequence, Optional, Tuple
import functools

def _ntuple(n):
    def parse(x):
        if isinstance(x, (tuple, list)):
            return tuple(x)
        return tuple([x] * n)
    return parse
to_2tuple = _ntuple(2)

class Mlp(nn.Module):
    in_features: int
    hidden_features: Optional[int] = None
    out_features: Optional[int] = None
    act_layer: Callable = nn.gelu
    drop: float = 0.

    @nn.compact
    def __call__(self, x, deterministic=True):
        out_features = self.out_features or self.in_features
        hidden_features = self.hidden_features or self.in_features
        x = nn.Dense(hidden_features)(x)
        x = self.act_layer(x)
        x = nn.Dropout(self.drop)(x, deterministic=deterministic)
        x = nn.Dense(out_features)(x)
        x = nn.Dropout(self.drop)(x, deterministic=deterministic)
        return x

def window_partition(x, window_size):
    """"""
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """"""
    B, H, W, C = x.shape
    x = x.reshape(B, H // window_size, window_size, W // window_size, window_size, C)
    x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))
    windows = x.reshape(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size, H, W):
    """"""
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """"""
    B = int(windows.shape[0] // (H * W // window_size // window_size))
    x = windows.reshape(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))
    x = x.reshape(B, H, W, -1)
    return x

class WindowAttention(nn.Module):
    dim: int
    window_size: Tuple[int, int]
    num_heads: int
    qkv_bias: bool = True
    attn_drop: float = 0.
    proj_drop: float = 0.
    pretrained_window_size: Tuple[int, int] = (0, 0)

    def setup(self):
        self.logit_scale = self.param(
            ""logit_scale"",
            lambda rng, shape: jnp.log(10 * jnp.ones(shape, dtype=jnp.float32)),
            (self.num_heads, 1, 1)
        )

        self.cpb_mlp_dense1 = nn.Dense(512, use_bias=True)
        self.cpb_mlp_dense2 = nn.Dense(self.num_heads, use_bias=False)

        Wh, Ww = self.window_size
        ph, pw = self.pretrained_window_size
        relative_coords_h = jnp.arange(-(Wh - 1), Wh, dtype=jnp.float32)
        relative_coords_w = jnp.arange(-(Ww - 1), Ww, dtype=jnp.float32)
        mesh = jnp.meshgrid(relative_coords_h, relative_coords_w, indexing='ij')
        relative_coords_table = jnp.stack(mesh, axis=-1)[None, ...]  # 1, 2*Wh-1, 2*Ww-1, 2

        if ph > 0:
            relative_coords_table = relative_coords_table.at[..., 0].divide(ph - 1)
            relative_coords_table = relative_coords_table.at[..., 1].divide(pw - 1)
        else:
            relative_coords_table = relative_coords_table.at[..., 0].divide(Wh - 1)
            relative_coords_table = relative_coords_table.at[..., 1].divide(Ww - 1)
        relative_coords_table = relative_coords_table * 8
        relative_coords_table = jnp.sign(relative_coords_table) * jnp.log2(jnp.abs(relative_coords_table) + 1.0) / np.log2(8)
        self.relative_coords_table = self.variable(""constants"", ""relative_coords_table"", lambda: relative_coords_table)

        coords_h = jnp.arange(Wh)
        coords_w = jnp.arange(Ww)
        mesh2 = jnp.meshgrid(coords_h, coords_w, indexing='ij')
        coords = jnp.stack(mesh2)  # 2, Wh, Ww
        coords_flatten = coords.reshape(2, -1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = jnp.transpose(relative_coords, (1, 2, 0))  # Wh*Ww, Wh*Ww, 2
        relative_coords = relative_coords.at[:, :, 0].add(Wh - 1)
        relative_coords = relative_coords.at[:, :, 1].add(Ww - 1)
        relative_coords = relative_coords.at[:, :, 0].multiply(2 * Ww - 1)
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.relative_position_index = self.variable(""constants"", ""relative_position_index"", lambda: relative_position_index)

        self.qkv = nn.Dense(self.dim * 3, use_bias=False)
        if self.qkv_bias:
            self.q_bias = self.param(""q_bias"", nn.initializers.zeros, (self.dim,))
            self.v_bias = self.param(""v_bias"", nn.initializers.zeros, (self.dim,))
        else:
            self.q_bias = None
            self.v_bias = None
        self.attn_drop_layer = nn.Dropout(self.attn_drop)
        self.proj = nn.Dense(self.dim)
        self.proj_drop_layer = nn.Dropout(self.proj_drop)

    def __call__(self, x, mask=None, deterministic=True):
        B_, N, C = x.shape
        if self.qkv_bias:
            qkv_bias = jnp.concatenate([
                self.q_bias,
                jnp.zeros_like(self.v_bias),
                self.v_bias
            ])
        else:
            qkv_bias = None
        qkv = self.qkv(x)
        if qkv_bias is not None:
            qkv = qkv + qkv_bias
        qkv = qkv.reshape(B_, N, 3, self.num_heads, C // self.num_heads)
        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))
        q, k, v = qkv[0], qkv[1], qkv[2]

        q = q / (jnp.linalg.norm(q, axis=-1, keepdims=True) + 1e-6)
        k = k / (jnp.linalg.norm(k, axis=-1, keepdims=True) + 1e-6)
        attn = jnp.einsum('bnqd,bnkd->bnqk', q, k)
        attn = jnp.matmul(q, jnp.swapaxes(k, -2, -1))

        logit_scale = jnp.clip(self.logit_scale, a_max=jnp.log(1. / 0.01)).exp()
        attn = attn * logit_scale

        relative_coords_table = self.relative_coords_table.value
        rel_coords_flat = relative_coords_table.reshape(-1, 2)
        rel_bias = self.cpb_mlp_dense1(rel_coords_flat)
        rel_bias = nn.relu(rel_bias)
        rel_bias = self.cpb_mlp_dense2(rel_bias)
        rel_bias = rel_bias.reshape(-1, self.num_heads)
        relative_position_index = self.relative_position_index.value
        rel_bias = rel_bias[relative_position_index.reshape(-1)]
        rel_bias = rel_bias.reshape(self.window_size[0] * self.window_size[1],
                                   self.window_size[0] * self.window_size[1], self.num_heads)
        rel_bias = jnp.transpose(rel_bias, (2, 0, 1))
        rel_bias = 16 * nn.sigmoid(rel_bias)
        attn = attn + rel_bias[None, ...]

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.reshape(B_ // nW, nW, self.num_heads, N, N) + mask[None, :, None, :, :]
            attn = attn.reshape(-1, self.num_heads, N, N)
            attn = nn.softmax(attn, axis=-1)
        else:
            attn = nn.softmax(attn, axis=-1)

        attn = self.attn_drop_layer(attn, deterministic=deterministic)
        x = jnp.matmul(attn, v)
        x = jnp.transpose(x, (0, 2, 1, 3)).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop_layer(x, deterministic=deterministic)
        return x

class SwinTransformerBlock(nn.Module):
    dim: int
    input_resolution: Tuple[int, int]
    num_heads: int
    window_size: int = 7
    shift_size: int = 0
    mlp_ratio: float = 4.
    qkv_bias: bool = True
    drop: float = 0.
    attn_drop: float = 0.
    drop_path: float = 0.
    act_layer: Callable = nn.gelu
    norm_layer: Callable = nn.LayerNorm
    pretrained_window_size: int = 0

    def setup(self):
        H, W = self.input_resolution
        window_size = self.window_size
        shift_size = self.shift_size
        if min(self.input_resolution) <= self.window_size:
            shift_size = 0
            window_size = min(self.input_resolution)
        self.window_size_ = window_size
        self.shift_size_ = shift_size

        self.norm1 = self.norm_layer()
        self.attn = WindowAttention(
            dim=self.dim,
            window_size=to_2tuple(window_size),
            num_heads=self.num_heads,
            qkv_bias=self.qkv_bias,
            attn_drop=self.attn_drop,
            proj_drop=self.drop,
            pretrained_window_size=to_2tuple(self.pretrained_window_size)
        )
        self.norm2 = self.norm_layer()
        mlp_hidden_dim = int(self.dim * self.mlp_ratio)
        self.mlp = Mlp(in_features=self.dim, hidden_features=mlp_hidden_dim, act_layer=self.act_layer, drop=self.drop)

        if self.shift_size_ > 0:
            img_mask = jnp.zeros((1, H, W, 1))
            h_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))
            w_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))
            cnt = 0
            mask = np.zeros((1, H, W, 1), dtype=np.float32)
            for h in h_slices:
                for w in w_slices:
                    mask[(slice(None), h, w, slice(None))] = cnt
                    cnt += 1
            mask_windows = window_partition(mask, window_size)
            mask_windows = mask_windows.reshape(-1, window_size * window_size)
            attn_mask = mask_windows[:, None, :] - mask_windows[:, :, None]
            attn_mask = np.where(attn_mask != 0, -100.0, 0.0)
            self.attn_mask = self.variable(""constants"", ""attn_mask"", lambda: jnp.array(attn_mask))
        else:
            self.attn_mask = None

    def __call__(self, x, deterministic=True):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W
        shortcut = x
        x = x.reshape(B, H, W, C)

        if self.shift_size_ > 0:
            shifted_x = jnp.roll(x, shift=(-self.shift_size_, -self.shift_size_), axis=(1, 2))
        else:
            shifted_x = x

        x_windows = window_partition(shifted_x, self.window_size_)
        x_windows = x_windows.reshape(-1, self.window_size_ * self.window_size_, C)

        attn_mask = self.attn_mask.value if self.attn_mask is not None else None
        attn_windows = self.attn(x_windows, mask=attn_mask, deterministic=deterministic)

        attn_windows = attn_windows.reshape(-1, self.window_size_, self.window_size_, C)
        shifted_x = window_reverse(attn_windows, self.window_size_, H, W)

        if self.shift_size_ > 0:
            x = jnp.roll(shifted_x, shift=(self.shift_size_, self.shift_size_), axis=(1, 2))
        else:
            x = shifted_x
        x = x.reshape(B, H * W, C)
        x = shortcut + self.norm1(x)
        x = x + self.norm2(self.mlp(x, deterministic=deterministic))
        return x

class PatchMerging(nn.Module):
    input_resolution: Tuple[int, int]
    dim: int
    norm_layer: Callable = nn.LayerNorm

    def setup(self):
        self.reduction = nn.Dense(2 * self.dim, use_bias=False)
        self.norm = self.norm_layer()

    def __call__(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W
        assert H % 2 == 0 and W % 2 == 0
        x = x.reshape(B, H, W, C)
        x0 = x[:, 0::2, 0::2, :]
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        x = jnp.concatenate([x0, x1, x2, x3], axis=-1)
        x = x.reshape(B, -1, 4 * C)
        x = self.reduction(x)
        x = self.norm(x)
        return x

class BasicLayer(nn.Module):
    dim: int
    input_resolution: Tuple[int, int]
    depth: int
    num_heads: int
    window_size: int
    mlp_ratio: float = 4.
    qkv_bias: bool = True
    drop: float = 0.
    attn_drop: float = 0.
    drop_path: float = 0.
    norm_layer: Callable = nn.LayerNorm
    downsample: Optional[Any] = None
    use_checkpoint: bool = False
    pretrained_window_size: int = 0

    def setup(self):
        self.blocks = [
            SwinTransformerBlock(
                dim=self.dim,
                input_resolution=self.input_resolution,
                num_heads=self.num_heads,
                window_size=self.window_size,
                shift_size=0 if (i % 2 == 0) else self.window_size // 2,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=self.qkv_bias,
                drop=self.drop,
                attn_drop=self.attn_drop,
                drop_path=self.drop_path if not isinstance(self.drop_path, (list, tuple)) else self.drop_path[i],
                norm_layer=self.norm_layer,
                pretrained_window_size=self.pretrained_window_size
            ) for i in range(self.depth)
        ]
        if self.downsample is not None:
            self.downsample_layer = self.downsample(self.input_resolution, dim=self.dim, norm_layer=self.norm_layer)
        else:
            self.downsample_layer = None

    def __call__(self, x, deterministic=True):
        for blk in self.blocks:
            x = blk(x, deterministic=deterministic)
        if self.downsample_layer is not None:
            x = self.downsample_layer(x)
        return x

class PatchEmbed(nn.Module):
    img_size: int = 224
    patch_size: int = 4
    in_chans: int = 3
    embed_dim: int = 96
    norm_layer: Optional[Callable] = None

    def setup(self):
        img_size = to_2tuple(self.img_size)
        patch_size = to_2tuple(self.patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size_ = img_size
        self.patch_size_ = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]
        self.proj = nn.Conv(self.embed_dim, kernel_size=patch_size, strides=patch_size, use_bias=True)
        if self.norm_layer is not None:
            self.norm = self.norm_layer()
        else:
            self.norm = None

    def __call__(self, x):
        B, C, H, W = x.shape
        assert H == self.img_size_[0] and W == self.img_size_[1]
        x = jnp.transpose(x, (0, 2, 3, 1))  # B, H, W, C
        x = self.proj(x)
        x = x.reshape(B, -1, self.embed_dim)
        if self.norm is not None:
            x = self.norm(x)
        return x

class Model(nn.Module):
    img_size: int = 224
    patch_size: int = 4
    in_chans: int = 3
    num_classes: int = 1000
    embed_dim: int = 96
    depths: Sequence[int] = (2, 2, 6, 2)
    num_heads: Sequence[int] = (3, 6, 12, 24)
    window_size: int = 7
    mlp_ratio: float = 4.
    qkv_bias: bool = True
    drop_rate: float = 0.
    attn_drop_rate: float = 0.
    drop_path_rate: float = 0.1
    norm_layer: Callable = nn.LayerNorm
    patch_norm: bool = True
    use_checkpoint: bool = False
    pretrained_window_sizes: Sequence[int] = (0, 0, 0, 0)

    def setup(self):
        self.num_layers = len(self.depths)
        self.num_features = int(self.embed_dim * 2 ** (self.num_layers - 1))
        self.patch_embed = PatchEmbed(
            img_size=self.img_size,
            patch_size=self.patch_size,
            in_chans=self.in_chans,
            embed_dim=self.embed_dim,
            norm_layer=self.norm_layer if self.patch_norm else None
        )
        self.patches_resolution = self.patch_embed.patches_resolution
        self.pos_drop = nn.Dropout(self.drop_rate)
        dpr = np.linspace(0, self.drop_path_rate, sum(self.depths)).tolist()
        self.layers = []
        for i_layer in range(self.num_layers):
            layer = BasicLayer(
                dim=int(self.embed_dim * 2 ** i_layer),
                input_resolution=(self.patches_resolution[0] // (2 ** i_layer),
                                  self.patches_resolution[1] // (2 ** i_layer)),
                depth=self.depths[i_layer],
                num_heads=self.num_heads[i_layer],
                window_size=self.window_size,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=self.qkv_bias,
                drop=self.drop_rate,
                attn_drop=self.attn_drop_rate,
                drop_path=dpr[sum(self.depths[:i_layer]):sum(self.depths[:i_layer + 1])],
                norm_layer=self.norm_layer,
                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                use_checkpoint=self.use_checkpoint,
                pretrained_window_size=self.pretrained_window_sizes[i_layer]
            )
            self.layers.append(layer)
        self.norm = self.norm_layer()
        self.head = nn.Dense(self.num_features if self.num_classes <= 0 else self.num_classes)
        self.avgpool = lambda x: jnp.mean(x, axis=1, keepdims=True)

    def forward_features(self, x, deterministic=True):
        x = self.patch_embed(x)
        x = self.pos_drop(x, deterministic=deterministic)
        for layer in self.layers:
            x = layer(x, deterministic=deterministic)
        x = self.norm(x)
        x = self.avgpool(x)
        x = x.reshape(x.shape[0], -1)
        return x

    def __call__(self, x, deterministic=True):
        x = self.forward_features(x, deterministic=deterministic)
        x = self.head(x)
        return x

batch_size = 10
image_size = 224

def get_inputs():
    return [jnp.array(np.random.rand(batch_size, 3, image_size, image_size), dtype=jnp.float32)]

def get_init_inputs():
    return []"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, embed_dim, num_heads):
        """"""
        Attention Block using Multihead Self-Attention.
        :param embed_dim: Embedding dimension (the number of channels)
        :param num_heads: Number of attention heads
        """"""
        super(Model, self).__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        """"""
        Forward pass of the AttentionBlock.
        :param x: Input tensor of shape (B, C, H, W)
        :return: Output tensor of the same shape (B, C, H, W)
        """"""
        B, C, H, W = x.shape
        x = x.view(B, C, H * W).permute(2, 0, 1)  # (seq_len, batch_size, embed_dim)
        attn_output, _ = self.attn(x, x, x)
        x = self.norm(attn_output + x)  # (seq_len, batch_size, embed_dim)
        x = x.permute(1, 2, 0).view(B, C, H, W)
        return x

embed_dim = 128
num_heads = 4
batch_size = 2
num_channels = embed_dim
image_height = 128
image_width = 128

def get_inputs():
    return [torch.rand(batch_size, num_channels, image_height, image_width)]

def get_init_inputs():
    return [embed_dim, num_heads]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class MultiheadSelfAttention(nn.Module):
    embed_dim: int
    num_heads: int

    @nn.compact
    def __call__(self, x):
        # x: (seq_len, batch_size, embed_dim)
        seq_len, batch_size, embed_dim = x.shape
        head_dim = self.embed_dim // self.num_heads
        assert self.embed_dim % self.num_heads == 0, ""embed_dim must be divisible by num_heads""

        # Project inputs to Q, K, V
        qkv = nn.Dense(3 * self.embed_dim, use_bias=True)(x)  # (seq_len, batch_size, 3*embed_dim)
        qkv = qkv.reshape(seq_len, batch_size, 3, self.num_heads, head_dim)
        qkv = qkv.transpose(2, 3, 1, 0, 4)  # (3, num_heads, batch_size, seq_len, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (num_heads, batch_size, seq_len, head_dim)

        # Compute attention scores
        attn_scores = jnp.einsum('hbqd,hbkd->hbqk', q, k)  # (num_heads, batch_size, seq_len, seq_len)
        attn_scores = attn_scores / jnp.sqrt(head_dim)
        attn_weights = nn.softmax(attn_scores, axis=-1)  # (num_heads, batch_size, seq_len, seq_len)

        # Weighted sum of values
        attn_output = jnp.einsum('hbqk,hbkd->hbqd', attn_weights, v)  # (num_heads, batch_size, seq_len, head_dim)
        attn_output = attn_output.transpose(2, 1, 0, 3)  # (seq_len, batch_size, num_heads, head_dim)
        attn_output = attn_output.reshape(seq_len, batch_size, self.embed_dim)  # (seq_len, batch_size, embed_dim)

        # Final output projection
        out = nn.Dense(self.embed_dim, use_bias=True)(attn_output)  # (seq_len, batch_size, embed_dim)
        return out

class Model(nn.Module):
    embed_dim: int
    num_heads: int

    def setup(self):
        self.attn = MultiheadSelfAttention(self.embed_dim, self.num_heads)
        self.norm = nn.LayerNorm()

    def __call__(self, x):
        # x: (B, C, H, W)
        B, C, H, W = x.shape
        x_reshaped = x.reshape(B, C, H * W).transpose(2, 0, 1)  # (seq_len, batch_size, embed_dim)
        attn_output = self.attn(x_reshaped)
        x_out = self.norm(attn_output + x_reshaped)
        x_out = x_out.transpose(1, 2, 0).reshape(B, C, H, W)
        return x_out

embed_dim = 128
num_heads = 4
batch_size = 2
num_channels = embed_dim
image_height = 128
image_width = 128

def get_inputs():
    return [jnp.random.rand(batch_size, num_channels, image_height, image_width).astype(jnp.float32)]

def get_init_inputs():
    return [embed_dim, num_heads]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, num_classes, embed_dim=512, num_heads=8, num_layers=6, 
                 mlp_ratio=4.0, patch_size=4, in_channels=3, image_size=32):
        """"""
        Convolutional Vision Transformer (CViT) implementation.
        :param num_classes: Number of output classes for classification.
        :param embed_dim: Dimensionality of the embedding space.
        :param num_heads: Number of attention heads.
        :param num_layers: Number of transformer layers.
        :param mlp_ratio: Ratio of the MLP hidden dimension to the embedding dimension.
        :param patch_size: Size of the convolutional patches.
        :param in_channels: Number of input channels (e.g., 3 for RGB images).
        :param image_size: Height/width of the square input image.
        """"""
        super(Model, self).__init__()

        self.patch_size = patch_size
        self.image_size = image_size
        self.embed_dim = embed_dim

        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)
        num_patches = (image_size // patch_size) ** 2  # Total number of patches after conv
        self.linear_proj = nn.Linear(embed_dim * num_patches, embed_dim)

        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=int(embed_dim * mlp_ratio),
                dropout=0.0,
                batch_first=True
            ) for _ in range(num_layers)
        ])

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.fc_out = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        """"""
        Forward pass of the CViT model.
        :param x: Input tensor of shape (B, C, H, W)
        :return: Output tensor of shape (B, num_classes)
        """"""
        B = x.size(0)
        x = self.conv1(x)                  # (B, embed_dim, H/patch_size, W/patch_size)
        x = x.flatten(start_dim=1)         # (B, embed_dim * num_patches)
        x = self.linear_proj(x)            # (B, embed_dim)

        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)
        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)  # (B, 2, embed_dim)

        for layer in self.transformer_layers:
            x = layer(x)

        return self.fc_out(x[:, 0])        # Use [CLS] token for classification

# === Test config ===
batch_size = 10
image_size = 32
embed_dim = 128
in_channels = 3
num_heads = 4
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, in_channels, image_size, image_size)]

def get_init_inputs():
    return [num_classes, embed_dim, num_heads]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Conv2d(nn.Module):
    in_channels: int
    out_channels: int
    kernel_size: int
    stride: int

    @nn.compact
    def __call__(self, x):
        # x: (B, C, H, W) -> (B, H, W, C)
        x = jnp.transpose(x, (0, 2, 3, 1))
        conv = nn.Conv(
            features=self.out_channels,
            kernel_size=(self.kernel_size, self.kernel_size),
            strides=(self.stride, self.stride),
            use_bias=True,
            kernel_init=nn.initializers.lecun_normal()
        )
        x = conv(x)
        # (B, H', W', out_channels) -> (B, out_channels, H', W')
        x = jnp.transpose(x, (0, 3, 1, 2))
        return x

class Linear(nn.Module):
    in_features: int
    out_features: int

    @nn.compact
    def __call__(self, x):
        dense = nn.Dense(features=self.out_features, use_bias=True)
        return dense(x)

class MLP(nn.Module):
    embed_dim: int
    mlp_dim: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(self.mlp_dim)(x)
        x = nn.gelu(x)
        x = nn.Dense(self.embed_dim)(x)
        return x

class TransformerEncoderLayer(nn.Module):
    embed_dim: int
    num_heads: int
    mlp_dim: int

    @nn.compact
    def __call__(self, x):
        # x: (B, N, D)
        # LayerNorm + MultiHead Self-Attention
        y = nn.LayerNorm()(x)
        y = nn.SelfAttention(
            num_heads=self.num_heads,
            qkv_features=self.embed_dim,
            out_features=self.embed_dim,
            use_bias=True,
            broadcast_dropout=False,
            dropout_rate=0.0
        )(y)
        x = x + y
        # LayerNorm + MLP
        y = nn.LayerNorm()(x)
        y = MLP(self.embed_dim, self.mlp_dim)(y)
        x = x + y
        return x

class Model(nn.Module):
    num_classes: int
    embed_dim: int = 512
    num_heads: int = 8
    num_layers: int = 6
    mlp_ratio: float = 4.0
    patch_size: int = 4
    in_channels: int = 3
    image_size: int = 32

    def setup(self):
        self.conv1 = Conv2d(
            in_channels=self.in_channels,
            out_channels=self.embed_dim,
            kernel_size=self.patch_size,
            stride=self.patch_size
        )
        num_patches = (self.image_size // self.patch_size) ** 2
        self.num_patches = num_patches
        self.linear_proj = Linear(
            in_features=self.embed_dim * num_patches,
            out_features=self.embed_dim
        )
        self.transformer_layers = [
            TransformerEncoderLayer(
                embed_dim=self.embed_dim,
                num_heads=self.num_heads,
                mlp_dim=int(self.embed_dim * self.mlp_ratio)
            ) for _ in range(self.num_layers)
        ]
        self.cls_token = self.param(
            'cls_token',
            nn.initializers.zeros,
            (1, 1, self.embed_dim)
        )
        self.fc_out = Linear(
            in_features=self.embed_dim,
            out_features=self.num_classes
        )

    def __call__(self, x):
        # x: (B, C, H, W)
        B = x.shape[0]
        x = self.conv1(x)  # (B, embed_dim, H/patch, W/patch)
        x = x.reshape((B, -1))  # (B, embed_dim * num_patches)
        x = self.linear_proj(x)  # (B, embed_dim)
        cls_tokens = jnp.tile(self.cls_token, (B, 1, 1))  # (B, 1, embed_dim)
        x = jnp.expand_dims(x, axis=1)  # (B, 1, embed_dim)
        x = jnp.concatenate([cls_tokens, x], axis=1)  # (B, 2, embed_dim)
        for layer in self.transformer_layers:
            x = layer(x)
        logits = self.fc_out(x[:, 0])  # (B, num_classes)
        return logits

# === Test config ===
batch_size = 10
image_size = 32
embed_dim = 128
in_channels = 3
num_heads = 4
num_classes = 1000

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(
        key,
        (batch_size, in_channels, image_size, image_size),
        minval=0.0,
        maxval=1.0
    )
    return [x]

def get_init_inputs():
    return [num_classes, embed_dim, num_heads]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """"""
        Initialize the Vanilla RNN model.

        :param input_size: The number of input features (int).
        :param hidden_size: The size of the hidden state (int).
        :param output_size: The number of output features (int).
        """"""
        super(Model, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden
        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output
        self.tanh = nn.Tanh()  # Activation function for hidden state

    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:
        """"""
        Forward pass of the Vanilla RNN.

        :param x: Input tensor of shape (seq_len, batch_size, input_size)
        :param h0: Initial hidden state tensor of shape (batch_size, hidden_size)
        :return: Output tensor of shape (seq_len, batch_size, output_size)
        """"""
        seq_len, batch_size, _ = x.size()
        hidden = h0.to(x.device)
        outputs = []

        for t in range(seq_len):
            combined = torch.cat((x[t], hidden), dim=1)  # Concatenate input and hidden state
            hidden = self.tanh(self.i2h(combined))  # Update hidden state
            output = self.h2o(hidden)  # Compute output
            outputs.append(output)

        return torch.stack(outputs, dim=0)  # (seq_len, batch_size, output_size)

# === Test configuration ===
batch_size = 8
input_size = 1024
hidden_size = 256
output_size = 128
sequence_length = 256

def get_inputs():
    return [
        torch.rand(sequence_length, batch_size, input_size),
        torch.rand(batch_size, hidden_size)
    ]

def get_init_inputs():
    return [input_size, hidden_size, output_size]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    output_size: int

    @nn.compact
    def __call__(self, x, h0):
        """"""
        x: (seq_len, batch_size, input_size)
        h0: (batch_size, hidden_size)
        Returns: (seq_len, batch_size, output_size)
        """"""
        i2h = nn.Dense(self.hidden_size)
        h2o = nn.Dense(self.output_size)
        seq_len, batch_size, _ = x.shape
        hidden = h0

        def step_fn(carry, x_t):
            hidden = carry
            combined = jnp.concatenate([x_t, hidden], axis=1)
            hidden_new = jnp.tanh(i2h(combined))
            output = h2o(hidden_new)
            return hidden_new, output

        # Scan over time dimension (axis=0)
        _, outputs = jax.lax.scan(step_fn, hidden, x)
        return outputs  # (seq_len, batch_size, output_size)

# === Test configuration ===
batch_size = 8
input_size = 1024
hidden_size = 256
output_size = 128
sequence_length = 256

def get_inputs(key=None):
    if key is None:
        key = jax.random.PRNGKey(0)
    k1, k2 = jax.random.split(key)
    x = jax.random.uniform(k1, (sequence_length, batch_size, input_size))
    h0 = jax.random.uniform(k2, (batch_size, hidden_size))
    return [x, h0]

def get_init_inputs():
    return [input_size, hidden_size, output_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):
        """"""
        Initialize the LSTM model.

        :param input_size: The number of expected features in the input `x`
        :param hidden_size: The number of features in the hidden state `h`
        :param num_layers: Number of recurrent layers
        :param output_size: The number of output features
        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer
        """"""
        super(Model, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                            batch_first=True, dropout=dropout, bidirectional=False)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, h0=None, c0=None):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :param h0: Optional initial hidden state (num_layers, batch_size, hidden_size)
        :param c0: Optional initial cell state (num_layers, batch_size, hidden_size)
        :return: The output tensor, shape (batch_size, output_size)
        """"""
        batch_size = x.size(0)

        if h0 is None:
            h0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)
        if c0 is None:
            c0 = torch.randn(self.num_layers, batch_size, self.hidden_size, device=x.device)

        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)
        out = self.fc(out[:, -1, :])     # out: (batch_size, output_size)

        return out

# === Test configuration ===
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    return [torch.rand(batch_size, sequence_length, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class LSTMCell(nn.Module):
    hidden_size: int

    @nn.compact
    def __call__(self, carry, x):
        h, c = carry
        input_size = x.shape[-1]
        # Concatenate input and hidden state
        concat = jnp.concatenate([x, h], axis=-1)
        # Linear for all gates at once
        gates = nn.Dense(4 * self.hidden_size)(concat)
        i, f, g, o = jnp.split(gates, 4, axis=-1)
        i = jax.nn.sigmoid(i)
        f = jax.nn.sigmoid(f)
        o = jax.nn.sigmoid(o)
        g = jnp.tanh(g)
        new_c = f * c + i * g
        new_h = o * jnp.tanh(new_c)
        return (new_h, new_c), new_h

class LSTMStack(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int
    dropout: float = 0.0

    @nn.compact
    def __call__(self, x, h0=None, c0=None, train: bool = False):
        # x: (batch, seq, input_size)
        batch_size = x.shape[0]
        seq_len = x.shape[1]
        hs = []
        cs = []
        outs = x
        new_hs = []
        new_cs = []
        for layer in range(self.num_layers):
            cell = LSTMCell(self.hidden_size)
            # Initial hidden/cell state
            if h0 is not None:
                h = h0[layer]
            else:
                h = jnp.zeros((batch_size, self.hidden_size), dtype=outs.dtype)
            if c0 is not None:
                c = c0[layer]
            else:
                c = jnp.zeros((batch_size, self.hidden_size), dtype=outs.dtype)
            carry = (h, c)
            # Scan over sequence
            carry, y = nn.scan(
                cell,
                variable_broadcast=""params"",
                split_rngs={""params"": False},
                in_axes=1,
                out_axes=1,
                name=f""lstm_layer_{layer}""
            )(carry, outs)
            outs = y
            if self.dropout > 0.0 and layer < self.num_layers - 1:
                outs = nn.Dropout(rate=self.dropout, deterministic=not train)(outs)
            new_hs.append(carry[0])
            new_cs.append(carry[1])
        # outs: (batch, seq, hidden_size)
        h_n = jnp.stack(new_hs, axis=0)  # (num_layers, batch, hidden_size)
        c_n = jnp.stack(new_cs, axis=0)  # (num_layers, batch, hidden_size)
        return outs, (h_n, c_n)

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int
    output_size: int
    dropout: float = 0.0

    @nn.compact
    def __call__(self, x, h0=None, c0=None, train: bool = False):
        # x: (batch, seq, input_size)
        lstm = LSTMStack(
            input_size=self.input_size,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=self.dropout
        )
        out, (h_n, c_n) = lstm(x, h0, c0, train=train)
        # Take last time step
        out_last = out[:, -1, :]  # (batch, hidden_size)
        out_proj = nn.Dense(self.output_size)(out_last)
        return out_proj

# === Test configuration ===
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    return [jnp.random.rand(batch_size, sequence_length, input_size).astype(jnp.float32)]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):
        """"""
        Initialize the LSTM model.

        :param input_size: The number of expected features in the input `x`
        :param hidden_size: The number of features in the hidden state `h`
        :param num_layers: Number of recurrent layers
        :param output_size: The number of output features
        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`
        """"""
        super(Model, self).__init__()
        # Initialize hidden state with random values
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x,h0,c0):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :return: The output tensor, shape (batch_size, sequence_length, output_size)
        """"""
        
        # Forward propagate LSTM
        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)
        
        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)
        
        return state[0]

# Test code
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int
    output_size: int
    dropout: float = 0.0

    @nn.compact
    def __call__(self, x, h0, c0):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :param h0: Initial hidden state, shape (num_layers, batch_size, hidden_size)
        :param c0: Initial cell state, shape (num_layers, batch_size, hidden_size)
        :return: The output hidden state, shape (num_layers, batch_size, hidden_size)
        """"""
        batch_size, seq_len, _ = x.shape

        # Flax's LSTMCell is single-layer, so we stack them manually
        class StackedLSTM(nn.Module):
            hidden_size: int
            num_layers: int
            dropout: float

            @nn.compact
            def __call__(self, x, h0, c0, train=False):
                outputs = []
                h_n = []
                c_n = []
                seq_len = x.shape[1]
                input_ = x
                for layer in range(self.num_layers):
                    lstm_cell = nn.scan(
                        nn.LSTMCell,
                        variable_broadcast=""params"",
                        split_rngs={""params"": False},
                        in_axes=1,
                        out_axes=1,
                        name=f""lstm_layer_{layer}""
                    )(name=f""cell_{layer}"", features=self.hidden_size)
                    h, c = h0[layer], c0[layer]
                    out, (h_last, c_last) = lstm_cell(input_, (h, c))
                    if self.dropout > 0.0 and layer < self.num_layers - 1:
                        out = nn.Dropout(rate=self.dropout, deterministic=not train)(out)
                    input_ = out
                    h_n.append(h_last)
                    c_n.append(c_last)
                return input_, (jnp.stack(h_n, axis=0), jnp.stack(c_n, axis=0))

        lstm = StackedLSTM(self.hidden_size, self.num_layers, self.dropout)
        out, (hn, cn) = lstm(x, h0, c0, train=False)
        # out: (batch, seq, hidden)
        out_last = out[:, -1, :]  # (batch, hidden)
        fc = nn.Dense(self.output_size)
        out_fc = fc(out_last)  # (batch, output_size)
        return hn

# Test code
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, sequence_length, input_size))
    h0 = jax.random.uniform(key, (num_layers, batch_size, hidden_size))
    c0 = jax.random.uniform(key, (num_layers, batch_size, hidden_size))
    return [x, h0, c0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):
        """"""
        Initialize the LSTM model.

        :param input_size: The number of expected features in the input `x`
        :param hidden_size: The number of features in the hidden state `h`
        :param num_layers: Number of recurrent layers
        :param output_size: The number of output features
        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`
        """"""
        super(Model, self).__init__()
        # Initialize hidden state with random values
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=False)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x, h0, c0):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :return: The output tensor, shape (batch_size, sequence_length, output_size)
        """"""
        
        # Forward propagate LSTM
        out, state = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)
        
        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)
        
        return state[1]

# Test code
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers, batch_size, hidden_size)),torch.rand((num_layers, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int
    output_size: int
    dropout: float = 0.0

    @nn.compact
    def __call__(self, x, h0, c0):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :param h0: Initial hidden state, shape (num_layers, batch_size, hidden_size)
        :param c0: Initial cell state, shape (num_layers, batch_size, hidden_size)
        :return: The cell state after processing, shape (num_layers, batch_size, hidden_size)
        """"""
        batch_size, seq_len, _ = x.shape

        def lstm_cell():
            return nn.OptimizedLSTMCell()

        # Stack LSTM layers
        cells = [lstm_cell() for _ in range(self.num_layers)]

        # Prepare initial states
        h = [h0[i] for i in range(self.num_layers)]
        c = [c0[i] for i in range(self.num_layers)]

        def step_fn(carry, x_t):
            h_list, c_list = carry
            new_h_list = []
            new_c_list = []
            input_t = x_t
            for i, cell in enumerate(cells):
                h_i, c_i = h_list[i], c_list[i]
                (new_c_i, new_h_i) = cell((c_i, h_i), input_t)
                input_t = new_h_i
                if self.dropout > 0.0 and i < self.num_layers - 1:
                    input_t = nn.Dropout(rate=self.dropout, deterministic=False)(input_t)
                new_h_list.append(new_h_i)
                new_c_list.append(new_c_i)
            return (new_h_list, new_c_list), input_t

        # Scan over sequence
        (final_h, final_c), outputs = jax.lax.scan(
            step_fn,
            (h, c),
            jnp.swapaxes(x, 0, 1)  # (seq_len, batch, input_size)
        )
        # outputs: (seq_len, batch, hidden_size)
        outputs = jnp.swapaxes(outputs, 0, 1)  # (batch, seq_len, hidden_size)

        # Decode the hidden state of the last time step
        out = outputs[:, -1, :]  # (batch, hidden_size)
        out = nn.Dense(self.output_size)(out)  # (batch, output_size)

        # Return the final cell state (as in the PyTorch code)
        final_c = jnp.stack(final_c, axis=0)  # (num_layers, batch, hidden_size)
        return final_c

# Test code
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, sequence_length, input_size))
    h0 = jax.random.uniform(key, (num_layers, batch_size, hidden_size))
    c0 = jax.random.uniform(key, (num_layers, batch_size, hidden_size))
    return [x, h0, c0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):
        """"""
        Initialize the LSTM model.

        :param input_size: The number of expected features in the input `x`
        :param hidden_size: The number of features in the hidden state `h`
        :param num_layers: Number of recurrent layers
        :param output_size: The number of output features
        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`
        """"""
        super(Model, self).__init__()
        # Initialize hidden state with random values
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, output_size)
    
    def forward(self, x,h0,c0):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :return: The output tensor, shape (batch_size, sequence_length, output_size)
        """"""
        # Forward propagate LSTM
        out, hn = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)
        
        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)
        
        return out

# Test code
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    return [torch.rand(batch_size, sequence_length, input_size),torch.rand((num_layers*2, batch_size, hidden_size)),torch.rand((num_layers*2, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int
    output_size: int
    dropout: float = 0.0

    @nn.compact
    def __call__(self, x, h0, c0):
        """"""
        Forward pass through the LSTM model.

        :param x: The input tensor, shape (batch_size, sequence_length, input_size)
        :param h0: Initial hidden state, shape (num_layers*2, batch_size, hidden_size)
        :param c0: Initial cell state, shape (num_layers*2, batch_size, hidden_size)
        :return: The output tensor, shape (batch_size, output_size)
        """"""
        batch_size, seq_len, _ = x.shape
        num_directions = 2  # bidirectional

        # Flax's LSTMCell is unidirectional; we implement multi-layer, bidirectional LSTM manually
        def lstm_layer(inputs, carry, cell_fw, cell_bw, dropout_rate, train):
            # Forward direction
            def scan_fw(carry, x):
                carry, y = cell_fw(carry, x)
                if dropout_rate > 0.0 and train:
                    y = nn.Dropout(rate=dropout_rate)(y, deterministic=not train)
                return carry, y
            _, ys_fw = nn.scan(scan_fw, variable_broadcast=""params"", split_rngs={""params"": False, ""dropout"": True}, in_axes=1, out_axes=1, length=seq_len)(carry[0], inputs)
            # Backward direction
            def scan_bw(carry, x):
                carry, y = cell_bw(carry, x)
                if dropout_rate > 0.0 and train:
                    y = nn.Dropout(rate=dropout_rate)(y, deterministic=not train)
                return carry, y
            _, ys_bw = nn.scan(scan_bw, variable_broadcast=""params"", split_rngs={""params"": False, ""dropout"": True}, in_axes=1, out_axes=1, length=seq_len)(carry[1], jnp.flip(inputs, axis=1))
            ys_bw = jnp.flip(ys_bw, axis=1)
            # Concatenate outputs
            ys = jnp.concatenate([ys_fw, ys_bw], axis=-1)
            return ys, (carry[0], carry[1])

        # Prepare initial states for all layers and directions
        h = h0
        c = c0
        out = x
        for layer in range(self.num_layers):
            # Each layer has forward and backward LSTMCell
            cell_fw = nn.LSTMCell(name=f""lstm_fw_{layer}"", hidden_size=self.hidden_size)
            cell_bw = nn.LSTMCell(name=f""lstm_bw_{layer}"", hidden_size=self.hidden_size)
            # Get initial state for this layer and direction
            h_fw = (h[layer * num_directions + 0], c[layer * num_directions + 0])
            h_bw = (h[layer * num_directions + 1], c[layer * num_directions + 1])
            # Dropout only on all but last layer
            dropout_rate = self.dropout if (layer < self.num_layers - 1) else 0.0
            out, (h_fw_last, h_bw_last) = lstm_layer(out, (h_fw, h_bw), cell_fw, cell_bw, dropout_rate, self.is_mutable_collection('dropout'))
            # For next layer, out is (batch, seq, hidden*2)
        # out: (batch, seq, hidden*2)
        # Take last time step
        out_last = out[:, -1, :]
        out = nn.Dense(self.output_size)(out_last)
        return out

# Test code
batch_size = 10
sequence_length = 512
input_size = 128
hidden_size = 256
num_layers = 6
output_size = 10
dropout = 0.0

def get_inputs():
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, sequence_length, input_size))
    h0 = jax.random.uniform(key, (num_layers*2, batch_size, hidden_size))
    c0 = jax.random.uniform(key, (num_layers*2, batch_size, hidden_size))
    return [x, h0, c0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers, output_size, dropout]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        """"""
        :param input_size: The number of expected features in the input x
        :param hidden_size: The number of features in the hidden state h
        :param num_layers: Number of recurrent layers (default: 1)
        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)
        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)
        """"""
        super(Model, self).__init__()
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)
    
    def forward(self, x,h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)
        :return: output, h_n
            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)
            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)
        """"""
        output, h_n = self.gru(x, h0)
        return output

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int = 3
    bias: bool = True
    batch_first: bool = False

    @nn.compact
    def __call__(self, x, h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h0: The initial hidden state for the input sequence, shape (num_layers, batch_size, hidden_size)
        :return: output, h_n
            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, hidden_size) if batch_first=False, otherwise (batch_size, seq_len, hidden_size)
            - h_n: The hidden state for t = seq_len, shape (num_layers, batch_size, hidden_size)
        """"""
        # If batch_first, swap axes to (seq_len, batch_size, input_size)
        if self.batch_first:
            x = jnp.swapaxes(x, 0, 1)  # (seq_len, batch_size, input_size)

        batch_size = x.shape[1]
        seq_len = x.shape[0]

        # Define GRUCells for each layer
        cells = [nn.GRUCell(name=f""gru_cell_{i}"", hidden_size=self.hidden_size, use_bias=self.bias) for i in range(self.num_layers)]

        def step_fn(carry, x_t):
            h_prev = carry  # (num_layers, batch_size, hidden_size)
            h_next = []
            input_t = x_t
            for i, cell in enumerate(cells):
                h_i, _ = cell(h_prev[i], input_t)
                h_next.append(h_i)
                input_t = h_i
            h_next = jnp.stack(h_next, axis=0)
            return h_next, input_t  # carry, output

        # Scan over sequence
        h_init = h0
        outputs, final_h = jax.lax.scan(step_fn, h_init, x)
        # outputs: (seq_len, batch_size, hidden_size)
        # final_h: (num_layers, batch_size, hidden_size)

        if self.batch_first:
            outputs = jnp.swapaxes(outputs, 0, 1)  # (batch_size, seq_len, hidden_size)

        return outputs, final_h

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (seq_len, batch_size, input_size))
    h0 = jax.random.uniform(jax.random.PRNGKey(1), (num_layers, batch_size, hidden_size))
    return [x, h0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_layer_sizes, output_size):
        """"""
        :param input_size: The number of input features
        :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer
        :param output_size: The number of output features
        """"""
        super(Model, self).__init__()
        
        layers = []
        current_input_size = input_size
        
        for hidden_size in hidden_layer_sizes:
            layers.append(nn.Linear(current_input_size, hidden_size))
            layers.append(nn.ReLU())
            current_input_size = hidden_size
        
        layers.append(nn.Linear(current_input_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        """"""
        :param x: The input tensor, shape (batch_size, input_size)
        :return: The output tensor, shape (batch_size, output_size)
        """"""
        return self.network(x)

# Test code
batch_size = 1024
input_size = 8192
hidden_layer_sizes = [1024] * 16  # deep network with wider layers
output_size = 8192

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_layer_sizes, output_size]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_layer_sizes: list
    output_size: int

    @nn.compact
    def __call__(self, x):
        current_input_size = self.input_size
        for hidden_size in self.hidden_layer_sizes:
            x = nn.Dense(hidden_size)(x)
            x = nn.relu(x)
            current_input_size = hidden_size
        x = nn.Dense(self.output_size)(x)
        return x

# Test code
batch_size = 1024
input_size = 8192
hidden_layer_sizes = [1024] * 16  # deep network with wider layers
output_size = 8192

def get_inputs():
    return [jnp.random.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_layer_sizes, output_size]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        """"""
        :param input_size: The number of expected features in the input x
        :param hidden_size: The number of features in the hidden state h
        :param num_layers: Number of recurrent layers (default: 1)
        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)
        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)
        """"""
        super(Model, self).__init__()
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=False)
    
    def forward(self, x,h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)
        :return: output, h_n
            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)
            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)
        """"""
        output, h_n = self.gru(x, h0)
        return h_n

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int = 3
    bias: bool = True
    batch_first: bool = False

    @nn.compact
    def __call__(self, x, h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h0: The initial hidden state for the input sequence, shape (num_layers, batch_size, hidden_size)
        :return: h_n: The hidden state for t = seq_len, shape (num_layers, batch_size, hidden_size)
        """"""
        # Flax's nn.GRUCell is a cell, not a full GRU layer, so we need to unroll manually
        # We'll implement a multi-layer GRU stack

        def gru_step(carry, x_t):
            h_prev = carry  # (num_layers, batch_size, hidden_size)
            h_next = []
            input_t = x_t
            for layer in range(self.num_layers):
                gru_cell = nn.GRUCell(
                    name=f""gru_cell_{layer}"",
                    use_bias=self.bias,
                )
                h, new_state = gru_cell(input_t, h_prev[layer])
                h_next.append(new_state)
                input_t = h  # output of this layer is input to next
            h_next = jnp.stack(h_next, axis=0)
            return h_next, input_t  # carry, y_t

        # Prepare input shape
        if self.batch_first:
            # (batch, seq, feature) -> (seq, batch, feature)
            x = jnp.transpose(x, (1, 0, 2))
        seq_len, batch_size, _ = x.shape

        # h0: (num_layers, batch_size, hidden_size)
        carry = h0

        outputs = []
        for t in range(seq_len):
            carry, y_t = gru_step(carry, x[t])
            outputs.append(y_t)
        outputs = jnp.stack(outputs, axis=0)  # (seq_len, batch_size, hidden_size)

        # For compatibility, output shape should match PyTorch's GRU output
        # output: (seq_len, batch_size, hidden_size)
        # h_n: (num_layers, batch_size, hidden_size)
        if self.batch_first:
            outputs = jnp.transpose(outputs, (1, 0, 2))  # (batch, seq, hidden)

        h_n = carry
        return h_n

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (seq_len, batch_size, input_size))
    h0 = jax.random.uniform(jax.random.PRNGKey(1), (num_layers, batch_size, hidden_size))
    return [x, h0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        """"""
        :param input_size: The number of expected features in the input x
        :param hidden_size: The number of features in the hidden state h
        :param num_layers: Number of recurrent layers (default: 1)
        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)
        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)
        """"""
        super(Model, self).__init__()
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)
        self.h0 = torch.randn((num_layers * 2, batch_size, hidden_size))
    
    def forward(self, x,h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)
        :return: output, h_n
            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)
            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)
        """"""
        output, h_n = self.gru(x, h0)
        return output

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class GRUStack(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int = 3
    bias: bool = True
    batch_first: bool = False

    @nn.compact
    def __call__(self, x, h0):
        # x: (seq_len, batch_size, input_size) or (batch_size, seq_len, input_size)
        # h0: (num_layers * 2, batch_size, hidden_size)
        num_directions = 2  # bidirectional
        batch_dim = 0 if not self.batch_first else 1

        # Prepare input shape
        if self.batch_first:
            batch_size, seq_len, _ = x.shape
            x = jnp.transpose(x, (1, 0, 2))  # (seq_len, batch_size, input_size)
        else:
            seq_len, batch_size, _ = x.shape

        # Split h0 for each direction and layer
        h0 = h0.reshape(self.num_layers, num_directions, batch_size, self.hidden_size)

        outputs = []
        h_n = []

        for layer in range(self.num_layers):
            layer_outs = []
            for direction in range(num_directions):
                # Each direction has its own GRUCell
                cell = nn.GRUCell(name=f""gru_cell_l{layer}_d{direction}"")
                hx = h0[layer, direction]
                if direction == 0:
                    # forward
                    seq = x
                else:
                    # backward
                    seq = jnp.flip(x, axis=0)
                outs = []
                for t in range(seq_len):
                    hx, _ = cell(hx, seq[t])
                    outs.append(hx)
                outs = jnp.stack(outs, axis=0)  # (seq_len, batch_size, hidden_size)
                if direction == 1:
                    outs = jnp.flip(outs, axis=0)
                layer_outs.append(outs)
                h_n.append(hx)
            # Concatenate directions
            x = jnp.concatenate(layer_outs, axis=-1)  # (seq_len, batch_size, hidden_size*2)
            outputs.append(x)
        output = x  # output from last layer
        h_n = jnp.stack(h_n, axis=0)  # (num_layers*2, batch_size, hidden_size)
        if self.batch_first:
            output = jnp.transpose(output, (1, 0, 2))  # (batch_size, seq_len, hidden_size*2)
        return output, h_n

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int = 3
    bias: bool = True
    batch_first: bool = False

    @nn.compact
    def __call__(self, x, h0):
        gru = GRUStack(
            input_size=self.input_size,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            bias=self.bias,
            batch_first=self.batch_first,
        )
        output, h_n = gru(x, h0)
        return output

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    # (seq_len, batch_size, input_size), (num_layers*2, batch_size, hidden_size)
    x = jax.random.uniform(jax.random.PRNGKey(0), (seq_len, batch_size, input_size))
    h0 = jax.random.uniform(jax.random.PRNGKey(1), (num_layers*2, batch_size, hidden_size))
    return [x, h0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        """"""
        :param input_size: The number of expected features in the input x
        :param hidden_size: The number of features in the hidden state h
        :param num_layers: Number of recurrent layers (default: 1)
        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)
        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)
        """"""
        super(Model, self).__init__()
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)
    
    def forward(self, x,h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)
        :return: output, h_n
            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)
            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)
        """"""
        output, h_n = self.gru(x, h0)
        return h_n

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    input_size: int
    hidden_size: int
    num_layers: int = 3
    bias: bool = True
    batch_first: bool = False

    @nn.compact
    def __call__(self, x, h0):
        """"""
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size)
        :return: h_n
        """"""
        # Bidirectional GRU: num_directions = 2
        num_directions = 2
        # Flax's nn.GRUCell is unidirectional, so we need to implement bidirectional stacking manually
        def gru_layer(inputs, h, fw_cell, bw_cell):
            # inputs: (seq_len, batch_size, input_size)
            # h: (num_directions, batch_size, hidden_size)
            # Forward
            def fw_scan(carry, x):
                h = fw_cell(carry, x)
                return h, h
            fw_init = h[0]
            fw_outputs, fw_states = jax.lax.scan(fw_scan, fw_init, inputs)
            # Backward
            def bw_scan(carry, x):
                h = bw_cell(carry, x)
                return h, h
            bw_init = h[1]
            bw_inputs = jnp.flip(inputs, axis=0)
            bw_outputs, bw_states = jax.lax.scan(bw_scan, bw_init, bw_inputs)
            bw_outputs = jnp.flip(bw_outputs, axis=0)
            # Concatenate outputs
            outputs = jnp.concatenate([fw_outputs, bw_outputs], axis=-1)
            hn = jnp.stack([fw_outputs[-1], bw_outputs[0]], axis=0)
            return outputs, hn

        # Prepare input
        if self.batch_first:
            # (batch, seq, feat) -> (seq, batch, feat)
            x = jnp.transpose(x, (1, 0, 2))

        seq_len, batch_size, _ = x.shape

        # h0: (num_layers * num_directions, batch_size, hidden_size)
        h0 = h0.reshape((self.num_layers, num_directions, batch_size, self.hidden_size))

        outputs = x
        hn_list = []
        for layer in range(self.num_layers):
            fw_cell = nn.GRUCell(name=f'fw_gru_{layer}', use_bias=self.bias)
            bw_cell = nn.GRUCell(name=f'bw_gru_{layer}', use_bias=self.bias)
            h_layer = h0[layer]  # (2, batch, hidden_size)
            outputs, hn = gru_layer(outputs, h_layer, fw_cell, bw_cell)
            # For next layer, outputs shape: (seq_len, batch_size, hidden_size*2)
            # For next layer, split outputs for each direction
            # We'll split outputs for each direction for the next layer
            # But GRU expects input_size, so we keep outputs as is
            hn_list.append(hn)
        # hn_list: list of (2, batch, hidden_size), len=num_layers
        h_n = jnp.concatenate(hn_list, axis=0)  # (num_layers*2, batch, hidden_size)

        if self.batch_first:
            # outputs: (seq, batch, hidden*2) -> (batch, seq, hidden*2)
            outputs = jnp.transpose(outputs, (1, 0, 2))

        return h_n

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    x = jax.random.uniform(jax.random.PRNGKey(0), (seq_len, batch_size, input_size))
    h0 = jax.random.uniform(jax.random.PRNGKey(1), (num_layers*2, batch_size, hidden_size))
    return [x, h0]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py

class Model(nn.Module):
    """"""
    A vanilla multi-head masked self-attention layer with a projection at the end.
    It is possible to use torch.nn.MultiheadAttention here but I am including an
    explicit implementation here to show that there is nothing too scary here.
    """"""

    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):
        super().__init__()
        assert n_embd % n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(n_embd, 3 * n_embd)
        # output projection
        self.c_proj = nn.Linear(n_embd, n_embd)
        # regularization
        self.attn_dropout = nn.Dropout(attn_pdrop)
        self.resid_dropout = nn.Dropout(resid_pdrop)
        # causal mask to ensure that attention is only applied to the left in the input sequence
        self.register_buffer(""bias"", torch.tril(torch.ones(max_seqlen, max_seqlen))
                                     .view(1, 1, max_seqlen, max_seqlen))
        self.n_head = n_head
        self.n_embd = n_embd

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

batch_size = 128
max_seqlen = 1024
seq_len = 512
n_embd = 768
n_head = 8
attn_pdrop = 0.0
resid_pdrop = 0.0

def get_inputs():
    return [torch.rand(batch_size, seq_len, n_embd)]

def get_init_inputs():
    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]
    '''
    
    Please return only the JAX code as output.

    ","import math
import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    n_embd: int
    n_head: int
    attn_pdrop: float
    resid_pdrop: float
    max_seqlen: int

    def setup(self):
        assert self.n_embd % self.n_head == 0
        self.c_attn = nn.Dense(3 * self.n_embd)
        self.c_proj = nn.Dense(self.n_embd)
        self.attn_dropout = nn.Dropout(self.attn_pdrop)
        self.resid_dropout = nn.Dropout(self.resid_pdrop)
        # Causal mask: shape (1, 1, max_seqlen, max_seqlen)
        bias = jnp.tril(jnp.ones((self.max_seqlen, self.max_seqlen), dtype=jnp.float32))
        self.bias = bias[None, None, :, :]

    @nn.compact
    def __call__(self, x, *, deterministic: bool = True):
        B, T, C = x.shape
        n_head = self.n_head
        n_embd = self.n_embd
        head_dim = C // n_head

        # Linear projection for q, k, v
        qkv = self.c_attn(x)
        q, k, v = jnp.split(qkv, 3, axis=2)
        # Reshape and transpose to (B, n_head, T, head_dim)
        def split_heads(t):
            t = t.reshape(B, T, n_head, head_dim)
            return jnp.transpose(t, (0, 2, 1, 3))
        q = split_heads(q)
        k = split_heads(k)
        v = split_heads(v)

        # Attention scores
        att = jnp.einsum('bnth,bnsh->bnts', q, k) * (1.0 / math.sqrt(head_dim))
        # Causal mask
        mask = self.bias[:, :, :T, :T]
        att = jnp.where(mask == 0, -1e10, att)
        att = nn.softmax(att, axis=-1)
        att = self.attn_dropout(att, deterministic=deterministic)
        y = jnp.einsum('bnts,bnsh->bnth', att, v)
        # Re-assemble heads: (B, T, C)
        y = jnp.transpose(y, (0, 2, 1, 3)).reshape(B, T, C)
        y = self.c_proj(y)
        y = self.resid_dropout(y, deterministic=deterministic)
        return y

batch_size = 128
max_seqlen = 1024
seq_len = 512
n_embd = 768
n_head = 8
attn_pdrop = 0.0
resid_pdrop = 0.0

def get_inputs():
    return [jnp.random.rand(batch_size, seq_len, n_embd).astype(jnp.float32)]

def get_init_inputs():
    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py

class NewGELU(nn.Module):
    """"""
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """"""
    def __init__(self):
        super(NewGELU, self).__init__()
    
    def forward(self, x):
        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))

class CausalSelfAttention(nn.Module):
    """"""
    A vanilla multi-head masked self-attention layer with a projection at the end.
    It is possible to use torch.nn.MultiheadAttention here but I am including an
    explicit implementation here to show that there is nothing too scary here.
    """"""

    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):
        super().__init__()
        assert n_embd % n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(n_embd, 3 * n_embd)
        # output projection
        self.c_proj = nn.Linear(n_embd, n_embd)
        # regularization
        self.attn_dropout = nn.Dropout(attn_pdrop)
        self.resid_dropout = nn.Dropout(resid_pdrop)
        # causal mask to ensure that attention is only applied to the left in the input sequence
        self.register_buffer(""bias"", torch.tril(torch.ones(max_seqlen, max_seqlen))
                                     .view(1, 1, max_seqlen, max_seqlen))
        self.n_head = n_head
        self.n_embd = n_embd

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y
    
class Model(nn.Module):
    """""" an unassuming Transformer block """"""

    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen):
        super().__init__()
        self.ln_1 = nn.LayerNorm(n_embd)
        self.attn = CausalSelfAttention(n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen)
        self.ln_2 = nn.LayerNorm(n_embd)
        self.mlp = nn.ModuleDict(dict(
            c_fc    = nn.Linear(n_embd, 4 * n_embd),
            c_proj  = nn.Linear(4 * n_embd, n_embd),
            act     = NewGELU(),
            dropout = nn.Dropout(resid_pdrop),
        ))
        m = self.mlp
        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlpf(self.ln_2(x))
        return x

batch_size = 128
max_seqlen = 1024
seq_len = 512
n_embd = 768
n_head = 8
attn_pdrop = 0.0
resid_pdrop = 0.0

def get_inputs():
    return [torch.rand(batch_size, seq_len, n_embd)]

def get_init_inputs():
    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn
import math

class NewGELU(nn.Module):
    """"""
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """"""
    @nn.compact
    def __call__(self, x):
        return 0.5 * x * (1.0 + jnp.tanh(jnp.sqrt(2.0 / jnp.pi) * (x + 0.044715 * jnp.power(x, 3.0))))

class CausalSelfAttention(nn.Module):
    n_embd: int
    n_head: int
    attn_pdrop: float
    resid_pdrop: float
    max_seqlen: int

    def setup(self):
        assert self.n_embd % self.n_head == 0
        self.c_attn = nn.Dense(3 * self.n_embd)
        self.c_proj = nn.Dense(self.n_embd)
        self.attn_dropout = nn.Dropout(self.attn_pdrop)
        self.resid_dropout = nn.Dropout(self.resid_pdrop)
        # Causal mask: (1, 1, max_seqlen, max_seqlen)
        bias = jnp.tril(jnp.ones((self.max_seqlen, self.max_seqlen), dtype=jnp.float32))
        self.bias = bias[None, None, :, :]

    def __call__(self, x, *, deterministic):
        B, T, C = x.shape

        # Linear projection and split into q, k, v
        qkv = self.c_attn(x)  # (B, T, 3 * n_embd)
        q, k, v = jnp.split(qkv, 3, axis=2)
        # Reshape for multi-head: (B, nh, T, hs)
        nh = self.n_head
        hs = C // nh
        k = k.reshape(B, T, nh, hs).transpose(0, 2, 1, 3)
        q = q.reshape(B, T, nh, hs).transpose(0, 2, 1, 3)
        v = v.reshape(B, T, nh, hs).transpose(0, 2, 1, 3)

        # Attention scores
        att = jnp.einsum('bnth,bnsh->bnts', q, k) * (1.0 / math.sqrt(hs))
        # Causal mask
        mask = self.bias[:, :, :T, :T]
        att = jnp.where(mask == 0, -1e10, att)
        att = nn.softmax(att, axis=-1)
        att = self.attn_dropout(att, deterministic=deterministic)
        y = jnp.einsum('bnts,bnsh->bnth', att, v)
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)

        y = self.c_proj(y)
        y = self.resid_dropout(y, deterministic=deterministic)
        return y

class Model(nn.Module):
    n_embd: int
    n_head: int
    attn_pdrop: float
    resid_pdrop: float
    max_seqlen: int

    def setup(self):
        self.ln_1 = nn.LayerNorm()
        self.attn = CausalSelfAttention(
            n_embd=self.n_embd,
            n_head=self.n_head,
            attn_pdrop=self.attn_pdrop,
            resid_pdrop=self.resid_pdrop,
            max_seqlen=self.max_seqlen,
        )
        self.ln_2 = nn.LayerNorm()
        self.c_fc = nn.Dense(4 * self.n_embd)
        self.act = NewGELU()
        self.c_proj = nn.Dense(self.n_embd)
        self.dropout = nn.Dropout(self.resid_pdrop)

    def mlpf(self, x, *, deterministic):
        x = self.c_fc(x)
        x = self.act(x)
        x = self.c_proj(x)
        x = self.dropout(x, deterministic=deterministic)
        return x

    def __call__(self, x, *, deterministic):
        x = x + self.attn(self.ln_1(x), deterministic=deterministic)
        x = x + self.mlpf(self.ln_2(x), deterministic=deterministic)
        return x

batch_size = 128
max_seqlen = 1024
seq_len = 512
n_embd = 768
n_head = 8
attn_pdrop = 0.0
resid_pdrop = 0.0

def get_inputs():
    return [jnp.random.rand(batch_size, seq_len, n_embd).astype(jnp.float32)]

def get_init_inputs():
    return [n_embd, n_head, attn_pdrop, resid_pdrop, max_seqlen]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn

# U-Net Implementation
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.Softmax(dim=-1),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.double_conv(x)

class Model(nn.Module):
    def __init__(self, in_channels, out_channels, features):
        """"""
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param features: Number of base features (will be doubled in each layer)
        """"""
        super(Model, self).__init__()
        self.encoder1 = DoubleConv(in_channels, features)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder2 = DoubleConv(features, features * 2)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder3 = DoubleConv(features * 2, features * 4)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder4 = DoubleConv(features * 4, features * 8)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.bottleneck = DoubleConv(features * 8, features * 16)

        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)
        self.decoder4 = DoubleConv(features * 16, features * 8)
        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)
        self.decoder3 = DoubleConv(features * 8, features * 4)
        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)
        self.decoder2 = DoubleConv(features * 4, features * 2)
        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)
        self.decoder1 = DoubleConv(features * 2, features)

        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)

    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels, height, width)
        """"""
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))
        enc4 = self.encoder4(self.pool3(enc3))

        bottleneck = self.bottleneck(self.pool4(enc4))

        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)

        return self.final_conv(dec1)
    
batch_size = 8
in_channels = 8
out_channels = 4
height = 64
width = 512
features = 64
# Test code for UNet
def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, features]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class DoubleConv(nn.Module):
    in_channels: int
    out_channels: int

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(self.out_channels, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.softmax(x, axis=-1)
        x = nn.Conv(self.out_channels, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=False)(x)
        x = nn.softmax(x, axis=-1)
        return x

class Model(nn.Module):
    in_channels: int
    out_channels: int
    features: int

    @nn.compact
    def __call__(self, x, *, train=True):
        # Encoder
        enc1 = DoubleConv(self.in_channels, self.features)(x)
        enc1_pooled = nn.max_pool(enc1, window_shape=(2, 2), strides=(2, 2), padding='VALID')
        enc2 = DoubleConv(self.features, self.features * 2)(enc1_pooled)
        enc2_pooled = nn.max_pool(enc2, window_shape=(2, 2), strides=(2, 2), padding='VALID')
        enc3 = DoubleConv(self.features * 2, self.features * 4)(enc2_pooled)
        enc3_pooled = nn.max_pool(enc3, window_shape=(2, 2), strides=(2, 2), padding='VALID')
        enc4 = DoubleConv(self.features * 4, self.features * 8)(enc3_pooled)
        enc4_pooled = nn.max_pool(enc4, window_shape=(2, 2), strides=(2, 2), padding='VALID')

        # Bottleneck
        bottleneck = DoubleConv(self.features * 8, self.features * 16)(enc4_pooled)

        # Decoder
        def upconv(x, features):
            # ConvTranspose2d with kernel_size=2, stride=2
            # Use nn.ConvTranspose
            return nn.ConvTranspose(features, kernel_size=(2, 2), strides=(2, 2), padding='SAME')(x)

        dec4 = upconv(bottleneck, self.features * 8)
        # Crop enc4 if needed to match dec4 spatial dims
        dec4 = jnp.concatenate([dec4, crop_to(enc4, dec4)], axis=1)
        dec4 = DoubleConv(self.features * 16, self.features * 8)(dec4)

        dec3 = upconv(dec4, self.features * 4)
        dec3 = jnp.concatenate([dec3, crop_to(enc3, dec3)], axis=1)
        dec3 = DoubleConv(self.features * 8, self.features * 4)(dec3)

        dec2 = upconv(dec3, self.features * 2)
        dec2 = jnp.concatenate([dec2, crop_to(enc2, dec2)], axis=1)
        dec2 = DoubleConv(self.features * 4, self.features * 2)(dec2)

        dec1 = upconv(dec2, self.features)
        dec1 = jnp.concatenate([dec1, crop_to(enc1, dec1)], axis=1)
        dec1 = DoubleConv(self.features * 2, self.features)(dec1)

        out = nn.Conv(self.out_channels, kernel_size=(1, 1))(dec1)
        return out

def crop_to(enc, dec):
    # enc, dec: (batch, channels, h, w)
    # Crop enc spatial dims to match dec
    h_enc, w_enc = enc.shape[2], enc.shape[3]
    h_dec, w_dec = dec.shape[2], dec.shape[3]
    dh = h_enc - h_dec
    dw = w_enc - w_dec
    h_start = dh // 2
    w_start = dw // 2
    return enc[:, :, h_start:h_start + h_dec, w_start:w_start + w_dec]

batch_size = 8
in_channels = 8
out_channels = 4
height = 64
width = 512
features = 64

def get_inputs():
    # Return a list with a single input array
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, in_channels, height, width))
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, features]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class Model(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """"""
        Mamba Structured State Space model implementation for benchmarking.
        
        :param batch_size: Size of the batch
        :param seq_length: Length of the input sequence
        :param n_heads: Number of attention heads
        :param d_head: Dimension of each head
        :param d_state: Dimension of the state space
        :param block_len: Length of each block for chunked computation
        """"""
        super(Model, self).__init__()
        
        assert seq_length % block_len == 0, ""Sequence length must be divisible by block length""
        
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len
        
        # Initialize parameters
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        
    def segsum(self, x):
        """"""Naive segment sum calculation.""""""
        T = x.size(-1)
        x_cumsum = torch.cumsum(x, dim=-1)
        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
        return x_segsum
    
    def forward(self, X, initial_states=None):
        """"""
        Forward pass implementing the SSD operation.
        
        :param X: Input tensor of shape (batch, length, n_heads, d_head)
        :param initial_states: Optional initial states
        :return: Output tensor Y and final state
        """"""
        # Rearrange into blocks/chunks
        X_blocks, A_blocks, B_blocks, C_blocks = [
            rearrange(x, ""b (c l) ... -> b c l ..."", l=self.block_len)
            for x in (X, self.A, self.B, self.C)
        ]
        
        A_blocks = rearrange(A_blocks, ""b c l h -> b h c l"")
        A_cumsum = torch.cumsum(A_blocks, dim=-1)
        
        # 1. Compute diagonal block outputs
        L = torch.exp(self.segsum(A_blocks))
        Y_diag = torch.einsum(""bclhn,bcshn,bhcls,bcshp->bclhp"", 
                             C_blocks, B_blocks, L, X_blocks)
        
        # 2. Compute intra-chunk states
        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
        states = torch.einsum(""bclhn,bhcl,bclhp->bchpn"", 
                            B_blocks, decay_states, X_blocks)
        
        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)
        
        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        new_states = torch.einsum(""bhzc,bchpn->bzhpn"", decay_chunk, states)
        states = new_states[:, :-1]
        
        # 4. Compute state-to-output conversion
        state_decay_out = torch.exp(A_cumsum)
        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', 
                           C_blocks, states, state_decay_out)
        
        # Combine diagonal and off-diagonal terms
        Y = rearrange(Y_diag + Y_off, ""b c l h p -> b (c l) h p"")
        
        
        return Y

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    return [torch.rand(batch_size, seq_length, n_heads, d_head)]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

def rearrange_blocks(x, block_len):
    # x: (b, seq, ...)
    b, seq = x.shape[:2]
    c = seq // block_len
    new_shape = (b, c, block_len) + x.shape[2:]
    return x.reshape(new_shape)

def reverse_rearrange_blocks(x):
    # x: (b, c, l, ...)
    b, c, l = x.shape[:3]
    new_shape = (b, c * l) + x.shape[3:]
    return x.reshape(new_shape)

class Model:
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64, key=None):
        assert seq_length % block_len == 0, ""Sequence length must be divisible by block length""
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len

        if key is None:
            key = jax.random.PRNGKey(0)
        k1, k2, k3 = jax.random.split(key, 3)
        self.A = jax.random.normal(k1, (batch_size, seq_length, n_heads))
        self.B = jax.random.normal(k2, (batch_size, seq_length, n_heads, d_state))
        self.C = jax.random.normal(k3, (batch_size, seq_length, n_heads, d_state))

    def segsum(self, x):
        # x: (..., T)
        T = x.shape[-1]
        x_cumsum = jnp.cumsum(x, axis=-1)
        x_cumsum_exp = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = jnp.tril(jnp.ones((T, T), dtype=bool))
        x_segsum = jnp.where(mask, x_cumsum_exp, -jnp.inf)
        return x_segsum

    def forward(self, X, initial_states=None):
        # X: (batch, length, n_heads, d_head)
        block_len = self.block_len

        # Rearrange into blocks/chunks
        X_blocks = rearrange_blocks(X, block_len)
        A_blocks = rearrange_blocks(self.A, block_len)
        B_blocks = rearrange_blocks(self.B, block_len)
        C_blocks = rearrange_blocks(self.C, block_len)

        # A_blocks: (b, c, l, h) -> (b, h, c, l)
        A_blocks = jnp.transpose(A_blocks, (0, 3, 1, 2))
        A_cumsum = jnp.cumsum(A_blocks, axis=-1)  # (b, h, c, l)

        # 1. Compute diagonal block outputs
        # L = exp(segsum(A_blocks)) -> (b, h, l, l)
        L = jnp.exp(self.segsum(A_blocks))  # (b, h, l, l)
        # C_blocks: (b, c, l, h, d_state)
        # B_blocks: (b, c, l, h, d_state)
        # X_blocks: (b, c, l, h, d_head)
        # L: (b, h, l, l)
        # We need to align dims for einsum
        # Let's expand L to (b, 1, l, l, h, 1)
        L_exp = L[:, None, :, :, :, None]  # (b, 1, h, l, l, 1)
        # C_blocks: (b, c, l, h, d_state)
        # B_blocks: (b, c, l, h, d_state)
        # X_blocks: (b, c, l, h, d_head)
        # We'll use einsum to match PyTorch's:
        # ""bclhn,bcshn,bhcls,bcshp->bclhp""
        # But in JAX, let's do:
        # C_blocks: (b, c, l, h, d_state)
        # B_blocks: (b, c, l, h, d_state)
        # L: (b, h, l, l)
        # X_blocks: (b, c, l, h, d_head)
        # We'll need to sum over s and n (l and d_state)
        # Let's expand dims to align
        # We'll use a for loop for clarity and correctness

        b, c, l, h, p = C_blocks.shape[0], C_blocks.shape[1], C_blocks.shape[2], C_blocks.shape[3], X_blocks.shape[-1]
        d_state = C_blocks.shape[-1]

        # Y_diag: (b, c, l, h, p)
        def compute_Y_diag(bi, ci):
            # C: (l, h, d_state)
            # B: (l, h, d_state)
            # X: (l, h, p)
            # L: (h, l, l)
            C = C_blocks[bi, ci]  # (l, h, d_state)
            B = B_blocks[bi, ci]  # (l, h, d_state)
            X = X_blocks[bi, ci]  # (l, h, p)
            Lh = L[bi]  # (h, l, l)
            # We'll compute for each h
            def compute_h(h_idx):
                # C: (l, d_state)
                # B: (l, d_state)
                # X: (l, p)
                # L: (l, l)
                Ch = C[:, h_idx, :]  # (l, d_state)
                Bh = B[:, h_idx, :]  # (l, d_state)
                Xh = X[:, h_idx, :]  # (l, p)
                Lhh = Lh[h_idx]      # (l, l)
                # einsum over s, n: sum_s sum_n C[l, n] * B[s, n] * L[l, s] * X[s, p]
                # -> for each l, p
                # We'll do:
                # Y[l, p] = sum_{s, n} C[l, n] * B[s, n] * L[l, s] * X[s, p]
                # Let's expand dims:
                # C[l, n] -> (l, 1, n, 1)
                # B[s, n] -> (1, s, n, 1)
                # L[l, s] -> (l, s, 1, 1)
                # X[s, p] -> (1, s, 1, p)
                C_exp = Ch[:, None, :, None]
                B_exp = Bh[None, :, :, None]
                L_exp = Lhh[:, :, None, None]
                X_exp = Xh[None, :, None, :]
                prod = C_exp * B_exp * L_exp * X_exp  # (l, s, n, p)
                Y = jnp.sum(prod, axis=(1,2))  # sum over s, n -> (l, p)
                return Y
            Yh = jax.vmap(compute_h)(jnp.arange(h))
            # Yh: (h, l, p) -> (l, h, p)
            return jnp.transpose(Yh, (1,0,2))
        Y_diag = jax.vmap(lambda bi: jax.vmap(lambda ci: compute_Y_diag(bi, ci))(jnp.arange(c)))(jnp.arange(b))
        # Y_diag: (b, c, l, h, p)

        # 2. Compute intra-chunk states
        # decay_states = exp(A_cumsum[:, :, :, -1:] - A_cumsum)
        # A_cumsum: (b, h, c, l)
        decay_states = jnp.exp(A_cumsum[:, :, :, -1:] - A_cumsum)  # (b, h, c, l)
        # B_blocks: (b, c, l, h, d_state)
        # X_blocks: (b, c, l, h, d_head)
        # We'll sum over l
        # states: (b, c, h, d_head, d_state)
        def compute_states(bi, ci):
            # B: (l, h, d_state)
            # decay: (h, l)
            # X: (l, h, d_head)
            B = B_blocks[bi, ci]  # (l, h, d_state)
            decay = decay_states[bi, :, ci]  # (h, l)
            X = X_blocks[bi, ci]  # (l, h, d_head)
            # For each h, p, n: sum over l: B[l, h, n] * decay[h, l] * X[l, h, p]
            def compute_h(h_idx):
                B_h = B[:, h_idx, :]  # (l, d_state)
                decay_h = decay[h_idx]  # (l,)
                X_h = X[:, h_idx, :]    # (l, d_head)
                # (l, d_state) * (l, 1) * (l, d_head)
                # We'll do outer product over l, then sum over l
                # For each n, p: sum_l B[l, n] * decay[l] * X[l, p]
                prod = B_h * decay_h[:, None]  # (l, d_state)
                # Now, for each p, n: sum_l prod[l, n] * X[l, p]
                # We'll do: (l, d_state)T @ (l, d_head) -> (d_state, d_head)
                # But we want (d_head, d_state)
                res = jnp.dot(X_h.T, prod)  # (d_head, d_state)
                return res
            states_h = jax.vmap(compute_h)(jnp.arange(h))  # (h, d_head, d_state)
            # (h, d_head, d_state) -> (h, d_head, d_state)
            return states_h
        states = jax.vmap(lambda bi: jax.vmap(lambda ci: compute_states(bi, ci))(jnp.arange(c)))(jnp.arange(b))
        # states: (b, c, h, d_head, d_state)

        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = jnp.zeros_like(states[:, :1])  # (b, 1, h, d_head, d_state)
        states_cat = jnp.concatenate([initial_states, states], axis=1)  # (b, c+1, h, d_head, d_state)

        # decay_chunk = exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        # A_cumsum[:, :, :, -1]: (b, h, c)
        A_last = A_cumsum[:, :, :, -1]  # (b, h, c)
        # Pad (1, 0) on last axis
        A_last_padded = jnp.pad(A_last, ((0,0),(0,0),(1,0)), mode='constant')
        decay_chunk = jnp.exp(self.segsum(A_last_padded))  # (b, h, c+1, c+1)

        # new_states = einsum(""bhzc,bchpn->bzhpn"", decay_chunk, states_cat)
        # decay_chunk: (b, h, z, c+1)
        # states_cat: (b, c+1, h, d_head, d_state)
        # We'll do: for z in c+1, sum over c+1: decay_chunk[b, h, z, s] * states_cat[b, s, h, d_head, d_state]
        def compute_new_states(bi):
            # decay_chunk: (h, c+1, c+1)
            # states_cat: (c+1, h, d_head, d_state)
            dc = decay_chunk[bi]  # (h, c+1, c+1)
            sc = states_cat[bi]   # (c+1, h, d_head, d_state)
            def compute_z(z_idx):
                # For each h, sum over s
                def compute_h(h_idx):
                    # dc: (c+1,)
                    # sc: (c+1, d_head, d_state)
                    weights = dc[h_idx, z_idx]  # (c+1,)
                    # weighted sum over s
                    weighted = weights[:, None, None] * sc[:, h_idx]
                    return jnp.sum(weighted, axis=0)  # (d_head, d_state)
                return jax.vmap(compute_h)(jnp.arange(h))  # (h, d_head, d_state)
            return jax.vmap(compute_z)(jnp.arange(c+1))  # (c+1, h, d_head, d_state)
        new_states = jax.vmap(compute_new_states)(jnp.arange(b))  # (b, c+1, h, d_head, d_state)
        # states = new_states[:, :-1]
        states = new_states[:, :-1]  # (b, c, h, d_head, d_state)

        # 4. Compute state-to-output conversion
        # state_decay_out = exp(A_cumsum)
        state_decay_out = jnp.exp(A_cumsum)  # (b, h, c, l)
        # C_blocks: (b, c, l, h, d_state)
        # states: (b, c, h, d_head, d_state)
        # state_decay_out: (b, h, c, l)
        # We'll do: einsum('bclhn,bchpn,bhcl->bclhp', C_blocks, states, state_decay_out)
        # C_blocks: (b, c, l, h, d_state)
        # states: (b, c, h, d_head, d_state)
        # state_decay_out: (b, h, c, l)
        def compute_Y_off(bi, ci):
            # C: (l, h, d_state)
            # states: (h, d_head, d_state)
            # decay: (h, l)
            C = C_blocks[bi, ci]  # (l, h, d_state)
            S = states[bi, ci]    # (h, d_head, d_state)
            decay = state_decay_out[bi, :, ci]  # (h, l)
            # For each l, h, p: sum_n C[l, h, n] * S[h, p, n] * decay[h, l]
            # We'll do: for l in block, h in n_heads, p in d_head
            def compute_l(l_idx):
                def compute_h(h_idx):
                    C_lh = C[l_idx, h_idx]  # (d_state,)
                    S_hp = S[h_idx]         # (d_head, d_state)
                    d = decay[h_idx, l_idx]
                    # sum_n C_lh[n] * S_hp[p, n] * d
                    # For each p: sum_n C_lh[n] * S_hp[p, n]
                    prod = jnp.dot(S_hp, C_lh) * d  # (d_head,)
                    return prod
                return jax.vmap(compute_h)(jnp.arange(h))  # (h, d_head)
            Y = jax.vmap(compute_l)(jnp.arange(l))  # (l, h, d_head)
            # (l, h, d_head) -> (l, h, d_head)
            return Y
        Y_off = jax.vmap(lambda bi: jax.vmap(lambda ci: compute_Y_off(bi, ci))(jnp.arange(c)))(jnp.arange(b))
        # Y_off: (b, c, l, h, d_head)
        # Transpose to (b, c, l, h, d_head)
        # Combine diagonal and off-diagonal terms
        Y = Y_diag + Y_off
        Y = reverse_rearrange_blocks(Y)  # (b, seq_length, h, d_head)
        return Y

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    key = jax.random.PRNGKey(42)
    X = jax.random.uniform(key, (batch_size, seq_length, n_heads, d_head))
    return [X]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class Model(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """"""
        Mamba Structured State Space model implementation for benchmarking.
        
        :param batch_size: Size of the batch
        :param seq_length: Length of the input sequence
        :param n_heads: Number of attention heads
        :param d_head: Dimension of each head
        :param d_state: Dimension of the state space
        :param block_len: Length of each block for chunked computation
        """"""
        super(Model, self).__init__()
        
        assert seq_length % block_len == 0, ""Sequence length must be divisible by block length""
        
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len
        
        # Initialize parameters
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        
    def segsum(self, x):
        """"""Naive segment sum calculation.""""""
        T = x.size(-1)
        x_cumsum = torch.cumsum(x, dim=-1)
        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
        return x_segsum
    
    def forward(self, X, initial_states=None):
        """"""
        Forward pass implementing the SSD operation.
        
        :param X: Input tensor of shape (batch, length, n_heads, d_head)
        :param initial_states: Optional initial states
        :return: Output tensor Y and final state
        """"""
        # Rearrange into blocks/chunks
        X_blocks, A_blocks, B_blocks, C_blocks = [
            rearrange(x, ""b (c l) ... -> b c l ..."", l=self.block_len)
            for x in (X, self.A, self.B, self.C)
        ]
        
        A_blocks = rearrange(A_blocks, ""b c l h -> b h c l"")
        A_cumsum = torch.cumsum(A_blocks, dim=-1)
        
        # 1. Compute diagonal block outputs
        L = torch.exp(self.segsum(A_blocks))
        Y_diag = torch.einsum(""bclhn,bcshn,bhcls,bcshp->bclhp"", 
                             C_blocks, B_blocks, L, X_blocks)
        
        # 2. Compute intra-chunk states
        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
        states = torch.einsum(""bclhn,bhcl,bclhp->bchpn"", 
                            B_blocks, decay_states, X_blocks)
        
        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)
        
        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        new_states = torch.einsum(""bhzc,bchpn->bzhpn"", decay_chunk, states)
        return new_states[:, -1]

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    return [torch.rand(batch_size, seq_length, n_heads, d_head)]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]

    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64, key=None):
        assert seq_length % block_len == 0, ""Sequence length must be divisible by block length""
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len

        if key is None:
            key = jax.random.PRNGKey(0)
        kA, kB, kC = jax.random.split(key, 3)
        self.A = jax.random.normal(kA, (batch_size, seq_length, n_heads))
        self.B = jax.random.normal(kB, (batch_size, seq_length, n_heads, d_state))
        self.C = jax.random.normal(kC, (batch_size, seq_length, n_heads, d_state))

    def rearrange_blocks(self, x, l):
        # x: (b, seq, ...)
        b = x.shape[0]
        seq = x.shape[1]
        c = seq // l
        new_shape = (b, c, l) + x.shape[2:]
        return x.reshape(new_shape)

    def segsum(self, x):
        # x: (..., T)
        T = x.shape[-1]
        x_cumsum = jnp.cumsum(x, axis=-1)
        x_cumsum_exp = jnp.expand_dims(x_cumsum, axis=-2)  # (..., 1, T)
        x_cumsum_exp2 = jnp.expand_dims(x_cumsum, axis=-1) # (..., T, 1)
        x_segsum = x_cumsum_exp - x_cumsum_exp2  # (..., T, T)
        mask = jnp.tril(jnp.ones((T, T), dtype=bool), k=0)
        # Broadcast mask to x_segsum shape
        mask = jnp.broadcast_to(mask, x_segsum.shape)
        x_segsum = jnp.where(mask, x_segsum, -jnp.inf)
        return x_segsum

    def forward(self, X, initial_states=None):
        # X: (batch, length, n_heads, d_head)
        # Rearrange into blocks/chunks
        X_blocks = self.rearrange_blocks(X, self.block_len)
        A_blocks = self.rearrange_blocks(self.A, self.block_len)
        B_blocks = self.rearrange_blocks(self.B, self.block_len)
        C_blocks = self.rearrange_blocks(self.C, self.block_len)

        # A_blocks: (b, c, l, h) -> (b, h, c, l)
        A_blocks = jnp.transpose(A_blocks, (0, 3, 1, 2))
        A_cumsum = jnp.cumsum(A_blocks, axis=-1)  # (b, h, c, l)

        # 1. Compute diagonal block outputs
        L = jnp.exp(self.segsum(A_blocks))  # (b, h, c, l, l)
        # C_blocks: (b, c, l, h, d_state)
        # B_blocks: (b, c, l, h, d_state)
        # X_blocks: (b, c, l, n_heads, d_head)
        # We'll need to align axes for einsum
        # Let's expand dims to match
        # C_blocks: (b, c, l, h, d_state)
        # B_blocks: (b, c, l, h, d_state)
        # L: (b, h, c, l, l)
        # X_blocks: (b, c, l, h, d_head)
        # We'll use: ""b c l h n, b c l h p, b h c l s, b c s h p -> b c l h p""
        # But PyTorch code uses: ""bclhn,bcshn,bhcls,bcshp->bclhp""
        # Let's transpose X_blocks to (b, c, l, h, d_head)
        # We'll need to align axes for einsum
        # Let's use the same notation as PyTorch
        # C_blocks: (b, c, l, h, d_state) -> b c l h n
        # B_blocks: (b, c, s, h, d_state) -> b c s h n
        # L: (b, h, c, l, s) -> b h c l s
        # X_blocks: (b, c, s, h, p) -> b c s h p
        # Output: b c l h p
        Y_diag = jnp.einsum(
            ""bclhn,bcshn,bhcls,bcshp->bclhp"",
            C_blocks, B_blocks, L, X_blocks
        )

        # 2. Compute intra-chunk states
        # decay_states = exp(A_cumsum[:, :, :, -1:] - A_cumsum)
        # A_cumsum: (b, h, c, l)
        last_A = A_cumsum[..., -1:]  # (b, h, c, 1)
        decay_states = jnp.exp(last_A - A_cumsum)  # (b, h, c, l)
        # B_blocks: (b, c, l, h, d_state) -> (b, c, l, h, d_state)
        # decay_states: (b, h, c, l) -> (b, c, l, h)
        decay_states_t = jnp.transpose(decay_states, (0, 2, 3, 1))  # (b, c, l, h)
        # X_blocks: (b, c, l, h, d_head)
        # We'll need to sum over l
        # ""bclhn,bclh,bclhp->bchpn""
        states = jnp.einsum(
            ""bclhn,bclh,bclhp->bchpn"",
            B_blocks, decay_states_t, X_blocks
        )

        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = jnp.zeros_like(states[:, :1])
        states = jnp.concatenate([initial_states, states], axis=1)  # (b, c+1, h, p, n)

        # decay_chunk = exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        # A_cumsum: (b, h, c, l)
        # A_cumsum[:, :, :, -1]: (b, h, c)
        last_A_cumsum = A_cumsum[..., -1]  # (b, h, c)
        # Pad (1, 0) on last axis
        last_A_cumsum_padded = jnp.pad(last_A_cumsum, ((0,0),(0,0),(1,0)), mode='constant')
        # self.segsum: (..., T)
        decay_chunk = jnp.exp(self.segsum(last_A_cumsum_padded))  # (b, h, c+1, c+1)
        # states: (b, c+1, h, p, n)
        # decay_chunk: (b, h, c+1, c+1)
        # ""bhzc,bchpn->bzhpn""
        new_states = jnp.einsum(""bhzc,bchpn->bzhpn"", decay_chunk, states)
        return new_states[:, -1]  # (b, h, p, n)

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    key = jax.random.PRNGKey(42)
    X = jax.random.uniform(key, (batch_size, seq_length, n_heads, d_head))
    return [X]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes):
        """"""
        LeNet-5 architecture implementation in PyTorch.

        :param num_classes: The number of output classes.
        """"""
        super(Model, self).__init__()
        
        # Convolutional layers
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)
        
        # Fully connected layers
        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)
        self.fc2 = nn.Linear(in_features=120, out_features=84)
        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)
    
    def forward(self, x):
        """"""
        Forward pass of the LeNet-5 model.

        :param x: The input tensor, shape (batch_size, 1, 32, 32)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        # First convolutional layer with ReLU activation and max pooling
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, kernel_size=2, stride=2)
        
        # Second convolutional layer with ReLU activation and max pooling
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, kernel_size=2, stride=2)
        
        # Flatten the output for the fully connected layers
        x = x.view(-1, 16*5*5)
        
        # First fully connected layer with ReLU activation
        x = F.relu(self.fc1(x))
        
        # Second fully connected layer with ReLU activation
        x = F.relu(self.fc2(x))
        
        # Final fully connected layer
        x = self.fc3(x)
        
        return x

# Test code for the LeNet-5 model (larger batch & image)
batch_size = 4096
num_classes = 20

def get_inputs():
    return [torch.rand(batch_size, 1, 32, 32)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    num_classes: int

    @nn.compact
    def __call__(self, x):
        # First convolutional layer with ReLU activation and max pooling
        x = nn.Conv(features=6, kernel_size=(5, 5), strides=(1, 1), use_bias=True)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')

        # Second convolutional layer with ReLU activation and max pooling
        x = nn.Conv(features=16, kernel_size=(5, 5), strides=(1, 1), use_bias=True)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')

        # Flatten the output for the fully connected layers
        x = x.reshape((x.shape[0], -1))

        # First fully connected layer with ReLU activation
        x = nn.Dense(features=120)(x)
        x = nn.relu(x)

        # Second fully connected layer with ReLU activation
        x = nn.Dense(features=84)(x)
        x = nn.relu(x)

        # Final fully connected layer
        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code for the LeNet-5 model (larger batch & image)
batch_size = 4096
num_classes = 20

def get_inputs():
    return [jnp.random.rand(batch_size, 1, 32, 32).astype(jnp.float32)]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# From https://github.com/karpathy/minGPT/blob/master/mingpt/model.py

class NewGELU(nn.Module):
    """"""
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """"""
    def __init__(self):
        super(NewGELU, self).__init__()
    
    def forward(self, x):
        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))

class Model(nn.Module):
    """"""
    A multi-head masked self-attention layer with a projection at the end that uses ReLU instead of Softmax.
    It is possible to use torch.nn.MultiheadAttention here but I am including an
    explicit implementation here to show that there is nothing too scary here.
    """"""

    def __init__(self, n_embd, n_head, max_seqlen):
        super().__init__()
        assert n_embd % n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(n_embd, 3 * n_embd)
        # output projection
        self.c_proj = nn.Linear(n_embd, n_embd)
        # causal mask to ensure that attention is only applied to the left in the input sequence
        self.register_buffer(""bias"", torch.tril(torch.ones(max_seqlen, max_seqlen))
                                     .view(1, 1, max_seqlen, max_seqlen))
        self.n_head = n_head
        self.n_embd = n_embd

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.relu(att)

        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        return y

batch_size = 16
max_seqlen = 1024
n_embd = 768  # Hidden dimension, typical for BERT-base size
n_head = 12   # Number of attention heads, typical for BERT-base size

def get_inputs():
    return [torch.rand(batch_size, max_seqlen, n_embd)]

def get_init_inputs():
    return [n_embd, n_head, max_seqlen]
    '''
    
    Please return only the JAX code as output.

    ","import math
import jax
import jax.numpy as jnp
from flax import linen as nn

class NewGELU(nn.Module):
    """"""
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """"""
    @nn.compact
    def __call__(self, x):
        return 0.5 * x * (1.0 + jnp.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * jnp.power(x, 3.0))))

class Model(nn.Module):
    n_embd: int
    n_head: int
    max_seqlen: int

    def setup(self):
        assert self.n_embd % self.n_head == 0
        self.c_attn = nn.Dense(3 * self.n_embd)
        self.c_proj = nn.Dense(self.n_embd)
        # Causal mask: shape (1, 1, max_seqlen, max_seqlen)
        bias = jnp.tril(jnp.ones((self.max_seqlen, self.max_seqlen), dtype=jnp.float32))
        self.bias = bias[None, None, :, :]

    def __call__(self, x):
        # x: (B, T, C)
        B, T, C = x.shape

        # Linear projection and split into q, k, v
        qkv = self.c_attn(x)  # (B, T, 3 * n_embd)
        q, k, v = jnp.split(qkv, 3, axis=2)  # Each (B, T, n_embd)

        # Reshape for multi-head: (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)
        head_dim = C // self.n_head
        k = k.reshape(B, T, self.n_head, head_dim).transpose(0, 2, 1, 3)
        q = q.reshape(B, T, self.n_head, head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(B, T, self.n_head, head_dim).transpose(0, 2, 1, 3)

        # Attention scores: (B, n_head, T, head_dim) x (B, n_head, head_dim, T) -> (B, n_head, T, T)
        att = jnp.einsum('bnth,bnhd->bntd', q, k.transpose(0,1,3,2))
        att = att / math.sqrt(head_dim)

        # Causal mask
        mask = self.bias[:, :, :T, :T]
        att = jnp.where(mask == 0, -jnp.inf, att)

        # ReLU instead of softmax
        att = jnp.maximum(att, 0.0)

        # Weighted sum: (B, n_head, T, T) x (B, n_head, T, head_dim) -> (B, n_head, T, head_dim)
        y = jnp.einsum('bnij,bnjh->bnih', att, v)

        # Re-assemble all head outputs side by side: (B, n_head, T, head_dim) -> (B, T, C)
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)

        return y

batch_size = 16
max_seqlen = 1024
n_embd = 768
n_head = 12

def get_inputs():
    return [jnp.random.rand(batch_size, max_seqlen, n_embd).astype(jnp.float32)]

def get_init_inputs():
    return [n_embd, n_head, max_seqlen]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        :param num_classes: The number of output classes (default is 1000 for ImageNet)
        """"""
        super(Model, self).__init__()
        
        # First convolutional layer
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)
        
        # Second convolutional layer
        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)
        self.relu2 = nn.ReLU(inplace=True)
        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)
        
        # Third convolutional layer
        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU(inplace=True)
        
        # Fourth convolutional layer
        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)
        self.relu4 = nn.ReLU(inplace=True)
        
        # Fifth convolutional layer
        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)
        self.relu5 = nn.ReLU(inplace=True)
        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2)
        
        # Fully connected layers
        self.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096)
        self.relu6 = nn.ReLU(inplace=True)
        self.dropout1 = nn.Dropout(p=0.0)
        
        self.fc2 = nn.Linear(in_features=4096, out_features=4096)
        self.relu7 = nn.ReLU(inplace=True)
        self.dropout2 = nn.Dropout(p=0.0)
        
        self.fc3 = nn.Linear(in_features=4096, out_features=num_classes)
    
    def forward(self, x):
        """"""
        :param x: The input tensor, shape (batch_size, 3, 224, 224)
        :return: The output tensor, shape (batch_size, num_classes)
        """"""
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.maxpool2(x)
        
        x = self.conv3(x)
        x = self.relu3(x)
        
        x = self.conv4(x)
        x = self.relu4(x)
        
        x = self.conv5(x)
        x = self.relu5(x)
        x = self.maxpool3(x)
        
        x = torch.flatten(x, 1)
        
        x = self.fc1(x)
        x = self.relu6(x)
        x = self.dropout1(x)
        
        x = self.fc2(x)
        x = self.relu7(x)
        x = self.dropout2(x)
        
        x = self.fc3(x)
        
        return x

# Test code
batch_size = 1024
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, 3, 224, 224)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, *, train: bool = False):
        # First convolutional layer
        x = nn.Conv(features=96, kernel_size=(11, 11), strides=(4, 4), padding='SAME', use_bias=True)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='VALID')

        # Second convolutional layer
        x = nn.Conv(features=256, kernel_size=(5, 5), strides=(1, 1), padding='SAME', use_bias=True)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='VALID')

        # Third convolutional layer
        x = nn.Conv(features=384, kernel_size=(3, 3), strides=(1, 1), padding='SAME', use_bias=True)(x)
        x = nn.relu(x)

        # Fourth convolutional layer
        x = nn.Conv(features=384, kernel_size=(3, 3), strides=(1, 1), padding='SAME', use_bias=True)(x)
        x = nn.relu(x)

        # Fifth convolutional layer
        x = nn.Conv(features=256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', use_bias=True)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='VALID')

        # Flatten
        x = x.reshape((x.shape[0], -1))

        # Fully connected layers
        x = nn.Dense(features=4096)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.0)(x, deterministic=not train)

        x = nn.Dense(features=4096)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.0)(x, deterministic=not train)

        x = nn.Dense(features=self.num_classes)(x)
        return x

# Test code
batch_size = 1024
num_classes = 1000

def get_inputs():
    # JAX expects NHWC by default, but Flax Conv2D supports NCHW if input is NCHW.
    # We'll keep NCHW to match PyTorch.
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, 3, 224, 224), dtype=jnp.float32)
    return [x]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class InceptionModule(nn.Module):
    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):
        """"""
        :param in_channels: Number of input channels
        :param out_1x1: Number of output channels for the 1x1 convolution
        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution
        :param out_3x3: Number of output channels for the 3x3 convolution
        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution
        :param out_5x5: Number of output channels for the 5x5 convolution
        :param pool_proj: Number of output channels for the pooling projection
        """"""
        super(InceptionModule, self).__init__()
        
        # 1x1 convolution branch
        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)
        
        # 3x3 convolution branch
        self.branch3x3 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),
            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)
        )
        
        # 5x5 convolution branch
        self.branch5x5 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),
            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)
        )
        
        # Max pooling branch
        self.branch_pool = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, pool_proj, kernel_size=1)
        )
    
    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels, height, width)
        """"""
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3(x)
        branch5x5 = self.branch5x5(x)
        branch_pool = self.branch_pool(x)
        
        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]
        return torch.cat(outputs, 1)

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """"""
        :param num_classes: Number of output classes
        """"""
        super(Model, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)
        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)
        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)
        
        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)
        
        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)
        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)
        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)
        
        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.0)
        self.fc = nn.Linear(1024, num_classes)
    
    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, 3, height, width)
        :return: Output tensor, shape (batch_size, num_classes)
        """"""
        x = self.maxpool1(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = self.maxpool2(F.relu(self.conv3(x)))
        
        x = self.inception3a(x)
        x = self.inception3b(x)
        x = self.maxpool3(x)
        
        x = self.inception4a(x)
        x = self.inception4b(x)
        x = self.inception4c(x)
        x = self.inception4d(x)
        x = self.inception4e(x)
        x = self.maxpool4(x)
        
        x = self.inception5a(x)
        x = self.inception5b(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, input_channels, height, width)]

def get_init_inputs():
    return [num_classes]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class InceptionModule(nn.Module):
    in_channels: int
    out_1x1: int
    reduce_3x3: int
    out_3x3: int
    reduce_5x5: int
    out_5x5: int
    pool_proj: int

    @nn.compact
    def __call__(self, x):
        # 1x1 convolution branch
        branch1x1 = nn.Conv(self.out_1x1, kernel_size=(1,1))(x)

        # 3x3 convolution branch
        branch3x3 = nn.Conv(self.reduce_3x3, kernel_size=(1,1))(x)
        branch3x3 = nn.Conv(self.out_3x3, kernel_size=(3,3), padding='SAME')(branch3x3)

        # 5x5 convolution branch
        branch5x5 = nn.Conv(self.reduce_5x5, kernel_size=(1,1))(x)
        branch5x5 = nn.Conv(self.out_5x5, kernel_size=(5,5), padding='SAME')(branch5x5)

        # Max pooling branch
        branch_pool = nn.max_pool(x, window_shape=(3,3), strides=(1,1), padding='SAME')
        branch_pool = nn.Conv(self.pool_proj, kernel_size=(1,1))(branch_pool)

        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]
        return jnp.concatenate(outputs, axis=1)

class Model(nn.Module):
    num_classes: int = 1000

    @nn.compact
    def __call__(self, x, train: bool = False):
        x = nn.Conv(64, kernel_size=(7,7), strides=(2,2), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        x = nn.Conv(64, kernel_size=(1,1))(x)
        x = nn.relu(x)
        x = nn.Conv(192, kernel_size=(3,3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        x = InceptionModule(192, 64, 96, 128, 16, 32, 32)(x)
        x = InceptionModule(256, 128, 128, 192, 32, 96, 64)(x)
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        x = InceptionModule(480, 192, 96, 208, 16, 48, 64)(x)
        x = InceptionModule(512, 160, 112, 224, 24, 64, 64)(x)
        x = InceptionModule(512, 128, 128, 256, 24, 64, 64)(x)
        x = InceptionModule(512, 112, 144, 288, 32, 64, 64)(x)
        x = InceptionModule(528, 256, 160, 320, 32, 128, 128)(x)
        x = nn.max_pool(x, window_shape=(3,3), strides=(2,2), padding='SAME')

        x = InceptionModule(832, 256, 160, 320, 32, 128, 128)(x)
        x = InceptionModule(832, 384, 192, 384, 48, 128, 128)(x)

        # AdaptiveAvgPool2d((1,1)) equivalent: mean over H,W
        x = jnp.mean(x, axis=(2,3), keepdims=True)
        x = x.reshape((x.shape[0], -1))
        x = nn.Dropout(0.0)(x, deterministic=not train)
        x = nn.Dense(self.num_classes)(x)
        return x

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    # JAX expects NHWC by default, but Flax Conv uses NCHW if axis=1 (default)
    # So we keep NCHW for compatibility with the above model
    key = jax.random.PRNGKey(0)
    x = jax.random.uniform(key, (batch_size, input_channels, height, width))
    return [x]

def get_init_inputs():
    return [num_classes]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1):
        """"""
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param stride: Stride for the first convolutional layer
        :param downsample: Downsample layer for the shortcut connection
        """"""
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = nn.Sequential(
            nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),
            nn.BatchNorm2d(out_channels * self.expansion),
        )
        self.stride = stride

    def forward(self, x):
        """"""
        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels, height, width)
        """"""
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out
    
# Test code
in_channels = 3
out_channels = 64
stride = 1
batch_size = 10
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, in_channels, 224, 224)]

def get_init_inputs():
    return [in_channels, out_channels, stride]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp
from flax import linen as nn

class Model(nn.Module):
    in_channels: int
    out_channels: int
    stride: int = 1
    expansion: int = 1

    @nn.compact
    def __call__(self, x):
        # First conv-bn-relu
        out = nn.Conv(
            features=self.out_channels,
            kernel_size=(3, 3),
            strides=(self.stride, self.stride),
            padding='SAME',
            use_bias=False,
            name='conv1'
        )(x)
        out = nn.BatchNorm(use_running_average=False, name='bn1')(out)
        out = nn.relu(out)

        # Second conv-bn
        out = nn.Conv(
            features=self.out_channels,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding='SAME',
            use_bias=False,
            name='conv2'
        )(out)
        out = nn.BatchNorm(use_running_average=False, name='bn2')(out)

        # Downsample path
        identity = x
        identity = nn.Conv(
            features=self.out_channels * self.expansion,
            kernel_size=(1, 1),
            strides=(self.stride, self.stride),
            padding='SAME',
            use_bias=False,
            name='downsample_conv'
        )(identity)
        identity = nn.BatchNorm(use_running_average=False, name='downsample_bn')(identity)

        out = out + identity
        out = nn.relu(out)
        return out

# Test code
in_channels = 3
out_channels = 64
stride = 1
batch_size = 10
num_classes = 1000

def get_inputs():
    return [jnp.random.rand(batch_size, in_channels, 224, 224).astype(jnp.float32)]

def get_init_inputs():
    return [in_channels, out_channels, stride]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""google/bigbird-roberta-base""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In JAX/Flax, you would load or define your model here.
        # For demonstration, we'll just store the config and model_name.

    def __call__(self, x):
        # Placeholder: Replace with actual model logic.
        # For demonstration, return a fake logits tensor of shape (batch, seq, vocab)
        batch_size, seq_len = x.shape
        vocab_size = self.config.vocab_size
        return jnp.zeros((batch_size, seq_len, vocab_size))

class Config:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

model_name = ""google/bigbird-roberta-base""
# Placeholder: In practice, load config from a file or dict.
vocab_size = 50265  # Example vocab size; adjust as needed.
config = Config(vocab_size)
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0),
        (batch_size, sequence_length),
        minval=0,
        maxval=vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""google/electra-small-discriminator""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 511
batch_size = 1

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model integration in JAX.
# In practice, you would use Flax models from transformers (e.g., FlaxAutoModelForCausalLM).
# Here, we provide a dummy model for demonstration.

class DummyCausalLM:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

    def __call__(self, x):
        # Simulate logits output: shape (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        return type('Output', (), {'logits': logits})

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(self.config.vocab_size)

    def __call__(self, x):
        return self.model(x).logits

# Simulate HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

# Simulate loading config
def load_config(model_name):
    # For demonstration, set vocab_size to a typical value
    return DummyConfig(vocab_size=30522)

model_name = ""google/electra-small-discriminator""
config = load_config(model_name)
vocab_size = config.vocab_size
sequence_length = 511
batch_size = 1

def get_inputs():
    # Random integer inputs in [0, vocab_size)
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""google/electra-small-discriminator""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In JAX/Flax, you would load or define your model here.
        # For demonstration, we'll just store the config and model_name.
        # Actual model loading (e.g., with HuggingFace Flax models) is omitted.

    def __call__(self, x):
        # Placeholder: Replace with actual model logic.
        # For demonstration, we'll just return a dummy logits tensor.
        # Shape: (batch_size, sequence_length, vocab_size)
        batch_size, sequence_length = x.shape
        vocab_size = self.config.vocab_size
        return jnp.zeros((batch_size, sequence_length, vocab_size))

# Dummy config class to mimic HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

model_name = ""google/electra-small-discriminator""
# In real use, you would load the config from HuggingFace or similar.
# Here, we just set vocab_size to a typical value for demonstration.
vocab_size = 30522
config = DummyConfig(vocab_size)
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0),
        (batch_size, sequence_length),
        minval=0,
        maxval=vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""google/reformer-enwik8""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a Flax-compatible model.
# In practice, you would use a Flax/transformers model here.
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In real use, load a Flax model here.
        self.vocab_size = config.vocab_size

    def __call__(self, x):
        # Placeholder: Replace with actual model logic.
        # For demonstration, return random logits of shape (batch, seq, vocab)
        batch_size, seq_len = x.shape
        logits = jax.random.normal(jax.random.PRNGKey(0), (batch_size, seq_len, self.vocab_size))
        return logits

# Simulate HuggingFace config loading
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

# Simulate config loading
model_name = ""google/reformer-enwik8""
# In real use, load config with transformers.FlaxAutoConfig.from_pretrained
config = DummyConfig(vocab_size=258)  # Example vocab size; adjust as needed
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    # JAX random integer generation
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""google/electra-small-discriminator""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX/Flax.
# In practice, you would use transformers.FlaxAutoModelForCausalLM, but here we mock the interface.
class DummyCausalLM:
    def __init__(self, config):
        self.config = config
        self.vocab_size = getattr(config, ""vocab_size"", 30522)

    def __call__(self, x):
        # Simulate logits output: (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(config)

    def __call__(self, x):
        return self.model(x).logits

# Simulate HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size=30522):
        self.vocab_size = vocab_size

# In real code, use: from transformers import AutoConfig
def load_config(model_name):
    # Simulate config loading
    return DummyConfig(vocab_size=30522)

model_name = ""google/electra-small-discriminator""
config = load_config(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0), (batch_size, sequence_length), 0, vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""google/reformer-enwik8""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX/Flax.
# In practice, you would use transformers.FlaxAutoModelForCausalLM, but here we mock the interface.
class DummyCausalLM:
    def __init__(self, config):
        self.config = config
        self.vocab_size = config.vocab_size

    def __call__(self, x):
        # Simulate logits output: (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(config)

    def __call__(self, x):
        return self.model(x).logits

# Simulate HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

# Simulate loading config
model_name = ""google/reformer-enwik8""
vocab_size = 258  # Example vocab size; adjust as needed
config = DummyConfig(vocab_size)
sequence_length = 32
batch_size = 1024

def get_inputs():
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""gpt2""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 1023
batch_size = 1

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX.
# In practice, you would use a Flax model from transformers, e.g., FlaxAutoModelForCausalLM.
# Here, we provide a minimal stub to match the interface.

class DummyCausalLM:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

    def __call__(self, x):
        # Simulate logits output: shape (batch, seq_len, vocab_size)
        batch, seq_len = x.shape
        logits = jnp.zeros((batch, seq_len, self.vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(self.config['vocab_size'])

    def __call__(self, x):
        return self.model(x).logits

model_name = ""gpt2""
# Simulate config as a dict with vocab_size
config = {'vocab_size': 50257}
vocab_size = config['vocab_size']
sequence_length = 1023
batch_size = 1

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0),
        (batch_size, sequence_length),
        minval=0,
        maxval=vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""facebook/bart-large""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX/Flax.
# In practice, you would use transformers.FlaxAutoModelForCausalLM, but here we use a stub.
class DummyCausalLM:
    def __init__(self, config):
        self.config = config
        self.vocab_size = getattr(config, ""vocab_size"", 50265)

    def __call__(self, x):
        # Simulate logits output: (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

# Dummy config to mimic HuggingFace config object
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

def from_pretrained(model_name, config=None):
    # In real code, use transformers.FlaxAutoModelForCausalLM.from_pretrained
    return DummyCausalLM(config)

def config_from_pretrained(model_name):
    # In real code, use transformers.AutoConfig.from_pretrained
    return DummyConfig(vocab_size=50265)

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = from_pretrained(self.model_name, config=self.config)

    def __call__(self, x):
        return self.model(x).logits

model_name = ""facebook/bart-large""
config = config_from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = jax.random.randint(jax.random.PRNGKey(0), (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""EleutherAI/gpt-neo-2.7B""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 512

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal language model.
# In practice, you would implement or import a Flax model here.
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In real use, load or define your Flax model here.
        self.vocab_size = config[""vocab_size""]
        self.sequence_length = config.get(""n_positions"", 32)
        self.hidden_size = config.get(""hidden_size"", 768)

    def __call__(self, x):
        # Dummy logits: [batch, seq, vocab_size]
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        return logits

# Dummy config mimicking HuggingFace's AutoConfig
def load_config(model_name):
    # In practice, load from a file or define as needed.
    return {
        ""vocab_size"": 50257,
        ""n_positions"": 32,
        ""hidden_size"": 768,
    }

model_name = ""EleutherAI/gpt-neo-2.7B""
config = load_config(model_name)
vocab_size = config[""vocab_size""]
sequence_length = 32
batch_size = 512

def get_inputs():
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""gpt2""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 1024

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal language model.
# In practice, you would use a Flax model or a pre-trained model from HuggingFace's Flax library.
# Here, we define a minimal stub to match the interface.

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # In a real scenario, load a Flax model here.
        # For demonstration, we just store vocab_size.
        self.vocab_size = config['vocab_size']

    def __call__(self, x):
        # Dummy logits: shape (batch, seq, vocab_size)
        batch_size, seq_len = x.shape
        # For demonstration, return zeros.
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        return logits

# Dummy config mimicking HuggingFace's config object
def load_config(model_name):
    # For demonstration, hardcode GPT-2's vocab size.
    return {'vocab_size': 50257}

model_name = ""gpt2""
config = load_config(model_name)
vocab_size = config['vocab_size']
sequence_length = 32
batch_size = 1024

def get_inputs():
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""EleutherAI/gpt-neo-2.7B""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal LM model.
# In practice, you would use a Flax/transformers model here.
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In real use, load a Flax model here.
        self.vocab_size = config['vocab_size']

    def __call__(self, x):
        # Dummy logits: shape (batch, seq, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        return logits

model_name = ""EleutherAI/gpt-neo-2.7B""
# Simulate config as a dict for this example
config = {'vocab_size': 50257}
vocab_size = config['vocab_size']
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0),
        (batch_size, sequence_length),
        minval=0,
        maxval=vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""facebook/bart-large""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX/Flax.
# In practice, you would use transformers.FlaxAutoModelForCausalLM, but here we mock the interface.
class DummyFlaxCausalLM:
    def __init__(self, config):
        self.config = config

    def __call__(self, x):
        # Simulate logits output: (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        vocab_size = self.config.vocab_size
        logits = jnp.zeros((batch_size, seq_len, vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

# Dummy config to mimic HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

# Model definition
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyFlaxCausalLM(self.config)

    def __call__(self, x):
        return self.model(x).logits

model_name = ""facebook/bart-large""
config = DummyConfig(vocab_size=50265)  # Example vocab size for BART
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0),
        (batch_size, sequence_length),
        minval=0,
        maxval=vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""facebook/opt-1.3b""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 2047
batch_size = 1

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX.
# In practice, you would use FlaxAutoModelForCausalLM from transformers.
# Here, we define a dummy model for demonstration.

class DummyCausalLM:
    def __init__(self, config):
        self.config = config
        self.vocab_size = config.vocab_size

    def __call__(self, x):
        # Simulate logits output: (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(self.config)

    def __call__(self, x):
        return self.model(x).logits

# Simulate HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

    @classmethod
    def from_pretrained(cls, model_name):
        # For demonstration, set vocab_size to a typical value
        return cls(vocab_size=50272)

model_name = ""facebook/opt-1.3b""
config = DummyConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 2047
batch_size = 1

def get_inputs():
    inputs = jax.random.randint(
        jax.random.PRNGKey(0),
        (batch_size, sequence_length),
        minval=0,
        maxval=vocab_size
    )
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""EleutherAI/gpt-neo-2.7B""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 2047
batch_size = 1

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for HuggingFace model loading in JAX/Flax.
# In practice, you would use transformers.FlaxAutoModelForCausalLM, but here we use a stub.
class DummyCausalLM:
    def __init__(self, config):
        self.config = config
        self.vocab_size = config.vocab_size

    def __call__(self, x):
        # Simulate logits output: shape (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        class Output:
            def __init__(self, logits):
                self.logits = logits
        return Output(logits)

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(self.config)

    def __call__(self, x):
        return self.model(x).logits

# Simulate HuggingFace config
class DummyConfig:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

    @classmethod
    def from_pretrained(cls, model_name):
        # For demo, set vocab_size to a typical GPT-Neo value
        return cls(vocab_size=50257)

model_name = ""EleutherAI/gpt-neo-2.7B""
config = DummyConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 2047
batch_size = 1

def get_inputs():
    # Random integer input in [0, vocab_size)
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""facebook/opt-1.3b""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal language model.
# In practice, you would use a Flax model from HuggingFace or implement your own.
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In real use, load a Flax model here.
        self.vocab_size = config[""vocab_size""]
        self.hidden_size = config.get(""hidden_size"", 768)

    def __call__(self, x):
        # Dummy logits: [batch, seq, vocab_size]
        batch_size, seq_len = x.shape
        # For demonstration, just return zeros
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        return logits

# Dummy config mimicking HuggingFace AutoConfig
def load_config(model_name):
    # In real use, load from HuggingFace or your own config
    return {
        ""vocab_size"": 50272,
        ""hidden_size"": 2048,
    }

model_name = ""facebook/opt-1.3b""
config = load_config(model_name)
vocab_size = config[""vocab_size""]
sequence_length = 256
batch_size = 32

def get_inputs():
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""facebook/bart-large""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 1023
batch_size = 1

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal language model.
# In practice, you would implement or import a Flax model here.
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In real use, load or define your Flax model here.
        self.vocab_size = config['vocab_size']

    def __call__(self, x):
        # Placeholder: Replace with actual model logic.
        # For demonstration, return random logits of shape (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        key = jax.random.PRNGKey(0)
        logits = jax.random.normal(key, (batch_size, seq_len, self.vocab_size))
        return logits

# Simulate HuggingFace config
def load_config(model_name):
    # In practice, load config from file or define as needed.
    # Here, we use a minimal dict for demonstration.
    return {'vocab_size': 50265}

model_name = ""facebook/bart-large""
config = load_config(model_name)
vocab_size = config['vocab_size']
sequence_length = 1023
batch_size = 1

def get_inputs():
    key = jax.random.PRNGKey(42)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""gpt2""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 256
batch_size = 32

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal language model.
# In practice, you would use a Flax model or implement your own.
class DummyCausalLM:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size

    def __call__(self, x):
        # Simulate logits output: (batch, seq_len, vocab_size)
        batch_size, seq_len = x.shape
        # For demonstration, return zeros
        logits = jnp.zeros((batch_size, seq_len, self.vocab_size))
        return type('Output', (), {'logits': logits})()

class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.model = DummyCausalLM(self.config['vocab_size'])

    def __call__(self, x):
        return self.model(x).logits

model_name = ""gpt2""
# Simulate config as a dict
config = {
    'vocab_size': 50257  # GPT-2 default vocab size
}
vocab_size = config['vocab_size']
sequence_length = 256
batch_size = 32

def get_inputs():
    # Random integer inputs in [0, vocab_size)
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
"You are an expert in Pytorch to JAX translation, please translate the given code snippet in Pytorch into JAX:
    
    '''
    
import torch
from transformers import AutoModelForCausalLM, AutoConfig

class Model(torch.nn.Module):
    def __init__(self, model_name, config):
        super().__init__()
        self.model_name = model_name
        self.config = config
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, config=self.config)

    def forward(self, x):
        return self.model(x).logits

model_name = ""facebook/opt-1.3b""
config = AutoConfig.from_pretrained(model_name)
vocab_size = config.vocab_size
sequence_length = 32
batch_size = 512

def get_inputs():
    inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
    return [inputs]

def get_init_inputs():
    return [model_name, config]
    '''
    
    Please return only the JAX code as output.

    ","import jax
import jax.numpy as jnp

# Placeholder for a causal language model.
# In practice, you would implement or import a Flax/JAX model here.
class Model:
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        # Placeholder: In real use, load or define your Flax model here.
        self.vocab_size = config['vocab_size']
        self.sequence_length = config.get('sequence_length', 32)

    def __call__(self, x):
        # Placeholder: Simulate logits output with random values.
        # Shape: (batch_size, sequence_length, vocab_size)
        batch_size, sequence_length = x.shape
        logits = jnp.zeros((batch_size, sequence_length, self.vocab_size))
        return logits

# Simulate config loading (since transformers is not used in JAX/Flax).
model_name = ""facebook/opt-1.3b""
config = {
    'vocab_size': 50272,  # Example vocab size for OPT-1.3b
    'sequence_length': 32
}
vocab_size = config['vocab_size']
sequence_length = 32
batch_size = 512

def get_inputs():
    # Random integer inputs in [0, vocab_size)
    key = jax.random.PRNGKey(0)
    inputs = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)
    return [inputs]

def get_init_inputs():
    return [model_name, config]"
